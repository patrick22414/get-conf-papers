<div role="tabpanel" class="tab-pane fade   active in" id="accept-spotlight">

  <ul class="list-unstyled submissions-list">
    <li class="note " data-id="SJgwNerKvB" data-number="2252">
      <h4>
        <a href="/forum?id=SJgwNerKvB">
          Continual learning with hypernetworks
        </a>


        <a href="/pdf?id=SJgwNerKvB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=voswaldj%40ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="voswaldj@ethz.ch">Johannes von Oswald</a>, <a href="/profile?email=henningc%40ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="henningc@ethz.ch">Christian Henning</a>, <a href="/profile?email=sacramento%40ini.ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="sacramento@ini.ethz.ch">João Sacramento</a>, <a href="/profile?email=bgrewe%40ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="bgrewe@ethz.ch">Benjamin F. Grewe</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#SJgwNerKvB-details-149" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SJgwNerKvB-details-149">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Continual learning (CL) is less difficult for this class of models thanks to a simple key feature: instead of recalling the input-output relations of all previously seen data, task-conditioned hypernetworks only require rehearsing task-specific weight realizations, which can be maintained in memory using a simple regularizer. Besides achieving state-of-the-art performance on standard CL benchmarks, additional experiments on long task sequences reveal that task-conditioned hypernetworks display a very large capacity to retain previous memories. Notably, such long memory lifetimes are achieved in a compressive regime, when the number of trainable hypernetwork weights is comparable or smaller than target network size. We provide insight into the structure of low-dimensional task embedding spaces (the input space of the hypernetwork) and show that task-conditioned hypernetworks demonstrate transfer learning. Finally, forward information transfer is further supported by empirical results on a challenging CL benchmark based on the CIFAR-10/100 image datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Continual Learning, Catastrophic Forgetting, Meta Model, Hypernetwork</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/chrhenning/hypercl</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SJgwNerKvB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BkxUvnEYDH" data-number="5">
      <h4>
        <a href="/forum?id=BkxUvnEYDH">
          Program Guided Agent
        </a>


        <a href="/pdf?id=BkxUvnEYDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=shaohuas%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="shaohuas@usc.edu">Shao-Hua Sun</a>, <a href="/profile?email=telinwu%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="telinwu@usc.edu">Te-Lin Wu</a>, <a href="/profile?email=limjj%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="limjj@usc.edu">Joseph J. Lim</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>21 Replies</span>


      </div>

      <a href="#BkxUvnEYDH-details-264" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BkxUvnEYDH-details-264">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a modular framework that can accomplish tasks specified by programs and achieve zero-shot generalization to more complex tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Developing agents that can learn to follow natural language instructions has been an emerging research direction. While being accessible and flexible, natural language instructions can sometimes be ambiguous even to humans. To address this, we propose to utilize programs, structured in a formal language, as a precise and expressive way to specify tasks. We then devise a modular framework that learns to perform a task specified by a program – as different circumstances give rise to diverse ways to accomplish the task, our framework can perceive which circumstance it is currently under, and instruct a multitask policy accordingly to fulfill each subtask of the overall task. Experimental results on a 2D Minecraft environment not only demonstrate that the proposed framework learns to reliably accomplish program instructions and achieves zero-shot generalization to more complex instructions but also verify the efficiency of the proposed modulation mechanism for learning the multitask policy. We also conduct an analysis comparing various models which learn from programs and natural language instructions in an end-to-end fashion.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Program Execution, Program Executor, Program Understanding, Program Guided Agent, Learning to Execute, Deep Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BkxUvnEYDH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BygPO2VKPH" data-number="44">
      <h4>
        <a href="/forum?id=BygPO2VKPH">
          Sparse Coding with Gated Learned ISTA
        </a>


        <a href="/pdf?id=BygPO2VKPH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=wukl14%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="wukl14@mails.tsinghua.edu.cn">Kailun Wu</a>, <a href="/profile?email=guoyiwen.ai%40bytedance.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="guoyiwen.ai@bytedance.com">Yiwen Guo</a>, <a href="/profile?email=liza19%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="liza19@mails.tsinghua.edu.cn">Ziang Li</a>, <a href="/profile?email=zcs%40mail.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="zcs@mail.tsinghua.edu.cn">Changshui Zhang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#BygPO2VKPH-details-881" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BygPO2VKPH-details-881">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose gated mechanisms to enhance learned ISTA for sparse coding, with theoretical guarantees on the superiority of the method. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we study the learned iterative shrinkage thresholding algorithm (LISTA) for solving sparse coding problems. Following assumptions made by prior works, we first discover that the code components in its estimations may be lower than expected, i.e., require gains, and to address this problem, a gated mechanism amenable to theoretical analysis is then introduced. Specific design of the gates is inspired by convergence analyses of the mechanism and hence its effectiveness can be formally guaranteed. In addition to the gain gates, we further introduce overshoot gates for compensating insufficient step size in LISTA. Extensive empirical results confirm our theoretical findings and verify the effectiveness of our method.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Sparse coding, deep learning, learned ISTA, convergence analysis</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/wukailun/GLISTA</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BygPO2VKPH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1ldO2EFPr" data-number="47">
      <h4>
        <a href="/forum?id=S1ldO2EFPr">
          Graph Neural Networks Exponentially Lose Expressive Power for Node Classification
        </a>


        <a href="/pdf?id=S1ldO2EFPr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=kenta_oono%40mist.i.u-tokyo.ac.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="kenta_oono@mist.i.u-tokyo.ac.jp">Kenta Oono</a>, <a href="/profile?email=taiji%40mist.i.u-tokyo.ac.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="taiji@mist.i.u-tokyo.ac.jp">Taiji Suzuki</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 20 Oct 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>12 Replies</span>


      </div>

      <a href="#S1ldO2EFPr-details-972" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1ldO2EFPr-details-972">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We relate the asymptotic behavior of graph neural networks to the graph spectra of underlying graphs and gives principled guidelines for normalizing weights.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity.
                Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes.
                Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erd\H{o}s -- R\'{e}nyi graph.
                We show that when the Erd\H{o}s -- R\'{e}nyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ``information loss" in the limit of infinite layers with high probability.
                Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at https://github.com/delta2323/gnn-asymptotics.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/delta2323/gnn-asymptotics</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Network, Deep Learning, Expressive Power</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1ldO2EFPr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rJljdh4KDH" data-number="54">
      <h4>
        <a href="/forum?id=rJljdh4KDH">
          Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells
        </a>


        <a href="/pdf?id=rJljdh4KDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=gengchen_mai%40geog.ucsb.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="gengchen_mai@geog.ucsb.edu">Gengchen Mai</a>, <a href="/profile?email=janowicz%40ucsb.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="janowicz@ucsb.edu">Krzysztof Janowicz</a>, <a href="/profile?email=boyan1%40linkedin.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="boyan1@linkedin.com">Bo Yan</a>, <a href="/profile?email=ruizhu%40geog.ucsb.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ruizhu@geog.ucsb.edu">Rui Zhu</a>, <a href="/profile?email=lingcai%40ucsb.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="lingcai@ucsb.edu">Ling Cai</a>, <a href="/profile?email=noon99%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="noon99@gmail.com">Ni Lao</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>13 Replies</span>


      </div>

      <a href="#rJljdh4KDH-details-687" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rJljdh4KDH-details-687">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Grid cell, space encoding, spatially explicit model, multi-scale periodic representation, unsupervised learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value "> We propose a representation learning model called Space2vec to encode the absolute positions and spatial relationships of places.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Unsupervised text encoding models have recently fueled substantial progress in NLP. The key idea is to use neural networks to convert words in texts to vector space representations (embeddings) based on word positions in a sentence and their contexts, which are suitable for end-to-end training of downstream tasks. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as POIs into models. A general-purpose representation model for space is valuable for a multitude of tasks. However, no such general model exists to date beyond simply applying discretization or feed-forward&nbsp;nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for location encoding and is critical for recognizing places and for path-integration. Therefore, we propose a representation learning model called Space2Vec to encode the absolute positions and spatial relationships of places. We conduct experiments on two real-world geographic data for two different tasks: 1) predicting types of POIs given their positions and context, 2) image classification leveraging their geo-locations. Results show that because of its multi-scale representations, Space2Vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed-forward nets, and tile embedding approaches for location modeling and image classification tasks. Detailed analysis shows that all baselines can at most well handle distribution at one scale but show poor performances in other scales. In contrast, Space2Vec ’s multi-scale representation can handle distributions at different scales.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/gengchenmai/space2vec</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rJljdh4KDH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="r1lfF2NYvH" data-number="71">
      <h4>
        <a href="/forum?id=r1lfF2NYvH">
          InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization
        </a>


        <a href="/pdf?id=r1lfF2NYvH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=sunfanyun%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sunfanyun@gmail.com">Fan-Yun Sun</a>, <a href="/profile?email=jhoffmann%40g.harvard.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jhoffmann@g.harvard.edu">Jordan Hoffman</a>, <a href="/profile?email=vikasverma.iitm%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="vikasverma.iitm@gmail.com">Vikas Verma</a>, <a href="/profile?email=jian.tang%40hec.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="jian.tang@hec.ca">Jian Tang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#r1lfF2NYvH-details-143" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="r1lfF2NYvH-details-143">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semisupervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">graph-level representation learning, mutual information maximization</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/fanyun-sun/InfoGraph</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=r1lfF2NYvH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="B1e9Y2NYvS" data-number="88">
      <h4>
        <a href="/forum?id=B1e9Y2NYvS">
          On Robustness of Neural Ordinary Differential Equations
        </a>


        <a href="/pdf?id=B1e9Y2NYvS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=hanshu.yan%40u.nus.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="hanshu.yan@u.nus.edu">Hanshu YAN</a>, <a href="/profile?email=dujiawei%40u.nus.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dujiawei@u.nus.edu">Jiawei DU</a>, <a href="/profile?email=vtan%40nus.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="top" title="vtan@nus.edu.sg">Vincent TAN</a>, <a href="/profile?email=elefjia%40nus.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="top" title="elefjia@nus.edu.sg">Jiashi FENG</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>21 Replies</span>


      </div>

      <a href="#B1e9Y2NYvS-details-103" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="B1e9Y2NYvS-details-103">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> Neural ordinary differential equations (ODEs) have been attracting increasing attention in various research domains recently. There have been some works studying optimization issues and approximation capabilities of neural ODEs, but their robustness is still yet unclear. In this work, we fill this important gap by exploring robustness properties of neural ODEs both empirically and theoretically. We first present an empirical study on the robustness of the neural ODE-based networks (ODENets) by exposing them to inputs with various types of perturbations and subsequently investigating the changes of the corresponding outputs. In contrast to conventional convolutional neural networks (CNNs), we find that the ODENets are more robust against both random Gaussian perturbations and adversarial attack examples. We then provide an insightful understanding of this phenomenon by exploiting a certain desirable property of the flow of a continuous-time ODE, namely that integral curves are non-intersecting. Our work suggests that, due to their intrinsic robustness, it is promising to use neural ODEs as a basic block for building robust deep network models. To further enhance the robustness of vanilla neural ODEs, we propose the time-invariant steady neural ODE (TisODE), which regularizes the flow on perturbed data via the time-invariant property and the imposition of a steady-state constraint. We show that the TisODE method outperforms vanilla neural ODEs and also can work in conjunction with other state-of-the-art architectural methods to build more robust deep networks.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural ODE</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=B1e9Y2NYvS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="H1xscnEKDr" data-number="128">
      <h4>
        <a href="/forum?id=H1xscnEKDr">
          Defending Against Physically Realizable Attacks on Image Classification
        </a>


        <a href="/pdf?id=H1xscnEKDr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=tongwu%40wustl.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tongwu@wustl.edu">Tong Wu</a>, <a href="/profile?email=liangtong%40wustl.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="liangtong@wustl.edu">Liang Tong</a>, <a href="/profile?email=yvorobeychik%40wustl.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yvorobeychik@wustl.edu">Yevgeniy Vorobeychik</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>13 Replies</span>


      </div>

      <a href="#H1xscnEKDr-details-294" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="H1xscnEKDr-details-294">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Defending Against Physically Realizable Attacks on Image Classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the problem of defending deep neural network approaches for image classification from physically realizable attacks. First, we demonstrate that the two most scalable and effective methods for learning robust models, adversarial training with PGD attacks and randomized smoothing, exhibit very limited effectiveness against three of the highest profile physical attacks. Next, we propose a new abstract adversarial model, rectangular occlusion attacks, in which an adversary places a small adversarially crafted rectangle in an image, and develop two approaches for efficiently computing the resulting adversarial examples. Finally, we demonstrate that adversarial training using our new attack yields image classification models that exhibit high robustness against the physically realizable attacks we study, offering the first effective generic defense against such attacks.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">defense against physical attacks, adversarial machine learning</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/tongwu2020/phattacks</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=H1xscnEKDr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rklEj2EFvB" data-number="149">
      <h4>
        <a href="/forum?id=rklEj2EFvB">
          Estimating Gradients for Discrete Random Variables by Sampling without Replacement
        </a>


        <a href="/pdf?id=rklEj2EFvB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=w.w.m.kool%40uva.nl" class="profile-link" data-toggle="tooltip" data-placement="top" title="w.w.m.kool@uva.nl">Wouter Kool</a>, <a href="/profile?email=h.c.vanhoof%40uva.nl" class="profile-link" data-toggle="tooltip" data-placement="top" title="h.c.vanhoof@uva.nl">Herke van Hoof</a>, <a href="/profile?email=m.welling%40uva.nl" class="profile-link" data-toggle="tooltip" data-placement="top" title="m.welling@uva.nl">Max Welling</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 22 Oct 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#rklEj2EFvB-details-486" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rklEj2EFvB-details-486">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">gradient, estimator, discrete, categorical, sampling, without replacement, reinforce, baseline, variance, gumbel, vae, structured prediction</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We derive a low-variance, unbiased gradient estimator for expectations over discrete random variables based on sampling without replacement</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We derive an unbiased estimator for expectations over discrete random variables based on sampling without replacement, which reduces variance as it avoids duplicate samples. We show that our estimator can be derived as the Rao-Blackwellization of three different estimators. Combining our estimator with REINFORCE, we obtain a policy gradient estimator and we reduce its variance using a built-in control variate which is obtained without additional model evaluations. The resulting estimator is closely related to other gradient estimators. Experiments with a toy problem, a categorical Variational Auto-Encoder and a structured prediction problem show that our estimator is the only estimator that is consistently among the best estimators in both high and low entropy settings.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/wouterkool/estimating-gradients-without-replacement</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rklEj2EFvB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HyeSin4FPB" data-number="151">
      <h4>
        <a href="/forum?id=HyeSin4FPB">
          Learning to Control PDEs with Differentiable Physics
        </a>


        <a href="/pdf?id=HyeSin4FPB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=philipp.holl%40tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="philipp.holl@tum.de">Philipp Holl</a>, <a href="/profile?email=nils.thuerey%40tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="nils.thuerey@tum.de">Nils Thuerey</a>, <a href="/profile?email=vkoltun%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="vkoltun@gmail.com">Vladlen Koltun</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#HyeSin4FPB-details-637" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HyeSin4FPB-details-637">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Differentiable physics, Optimal control, Deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We train a combination of neural networks to predict optimal trajectories for complex physical systems.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Predicting outcomes and planning interactions with the physical world are long-standing goals for machine learning. A variety of such tasks involves continuous physical systems, which can be described by partial differential equations (PDEs) with many degrees of freedom. Existing methods that aim to control the dynamics of such systems are typically limited to relatively short time frames or a small number of interaction parameters. We present a novel hierarchical predictor-corrector scheme which enables neural networks to learn to understand and control complex nonlinear physical systems over long time frames. We propose to split the problem into two distinct tasks: planning and control. To this end, we introduce a predictor network that plans optimal trajectories and a control network that infers the corresponding control parameters. Both stages are trained end-to-end using a differentiable PDE solver. We demonstrate that our method successfully develops an understanding of complex physical systems and learns to control them for tasks involving PDEs such as the incompressible Navier-Stokes equations.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HyeSin4FPB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HygOjhEYDH" data-number="158">
      <h4>
        <a href="/forum?id=HygOjhEYDH">
          Intensity-Free Learning of Temporal Point Processes
        </a>


        <a href="/pdf?id=HygOjhEYDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=shchur%40in.tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="shchur@in.tum.de">Oleksandr Shchur</a>, <a href="/profile?email=bilos%40in.tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="bilos@in.tum.de">Marin Biloš</a>, <a href="/profile?email=guennemann%40in.tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="guennemann@in.tum.de">Stephan Günnemann</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>16 Replies</span>


      </div>

      <a href="#HygOjhEYDH-details-804" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HygOjhEYDH-details-804">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Learn in temporal point processes by modeling the conditional density, not the conditional intensity.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Temporal point processes are the dominant paradigm for modeling sequences of events happening at irregular intervals. The standard way of learning in such models is by estimating the conditional intensity function. However, parameterizing the intensity function usually incurs several trade-offs. We show how to overcome the limitations of intensity-based approaches by directly modeling the conditional distribution of inter-event times. We draw on the literature on normalizing flows to design models that are flexible and efficient. We additionally propose a simple mixture model that matches the flexibility of flow-based models, but also permits sampling and computing moments in closed form. The proposed models achieve state-of-the-art performance in standard prediction tasks and are suitable for novel applications, such as learning sequence embeddings and imputing missing data.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/shchur/ifl-tpp</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Temporal point process, neural density estimation</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HygOjhEYDH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HJeTo2VFwH" data-number="168">
      <h4>
        <a href="/forum?id=HJeTo2VFwH">
          A Signal Propagation Perspective for Pruning Neural Networks at Initialization
        </a>


        <a href="/pdf?id=HJeTo2VFwH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=namhoon%40robots.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="namhoon@robots.ox.ac.uk">Namhoon Lee</a>, <a href="/profile?email=thalaiyasingam.ajanthan%40anu.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="thalaiyasingam.ajanthan@anu.edu.au">Thalaiyasingam Ajanthan</a>, <a href="/profile?email=stephen.gould%40anu.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="stephen.gould@anu.edu.au">Stephen Gould</a>, <a href="/profile?email=phst%40robots.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="phst@robots.ox.ac.uk">Philip H. S. Torr</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#HJeTo2VFwH-details-291" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HJeTo2VFwH-details-291">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We formally characterize the initialization conditions for effective pruning at initialization and analyze the signal propagation properties of the resulting pruned networks which leads to a method to enhance their trainability and pruning results.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Network pruning is a promising avenue for compressing deep neural networks. A typical approach to pruning starts by training a model and then removing redundant parameters while minimizing the impact on what is learned. Alternatively, a recent approach shows that pruning can be done at initialization prior to training, based on a saliency criterion called connection sensitivity. However, it remains unclear exactly why pruning an untrained, randomly initialized neural network is effective. In this work, by noting connection sensitivity as a form of gradient, we formally characterize initialization conditions to ensure reliable connection sensitivity measurements, which in turn yields effective pruning results. Moreover, we analyze the signal propagation properties of the resulting pruned networks and introduce a simple, data-free method to improve their trainability. Our modifications to the existing pruning at initialization method lead to improved results on all tested network models for image classification tasks. Furthermore, we empirically study the effect of supervision for pruning and demonstrate that our signal propagation perspective, combined with unsupervised pruning, can be useful in various scenarios where pruning is applied to non-standard arbitrarily-designed architectures.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural network pruning, signal propagation perspective, sparse neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HJeTo2VFwH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BJlRs34Fvr" data-number="171">
      <h4>
        <a href="/forum?id=BJlRs34Fvr">
          Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets
        </a>


        <a href="/pdf?id=BJlRs34Fvr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=wu-dx16%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="wu-dx16@mails.tsinghua.edu.cn">Dongxian Wu</a>, <a href="/profile?email=eewangyisen%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="eewangyisen@gmail.com">Yisen Wang</a>, <a href="/profile?email=xiast%40sz.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="xiast@sz.tsinghua.edu.cn">Shu-Tao Xia</a>, <a href="/profile?email=baileyj%40unimelb.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="baileyj@unimelb.edu.au">James Bailey</a>, <a href="/profile?email=xingjun.ma%40unimelb.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="xingjun.ma@unimelb.edu.au">Xingjun Ma</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#BJlRs34Fvr-details-774" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BJlRs34Fvr-details-774">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We identify the security weakness of skip connections in ResNet-like neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Skip connections are an essential component of current state-of-the-art deep neural networks (DNNs) such as ResNet, WideResNet, DenseNet, and ResNeXt. Despite their huge success in building deeper and more powerful DNNs, we identify a surprising \emph{security weakness} of skip connections in this paper. Use of skip connections \textit{allows easier generation of highly transferable adversarial examples}. Specifically, in ResNet-like (with skip connections) neural networks, gradients can backpropagate through either skip connections or residual modules. We find that using more gradients from the skip connections rather than the residual modules according to a decay factor, allows one to craft adversarial examples with high transferability. Our method is termed \emph{Skip Gradient Method} (SGM). We conduct comprehensive transfer attacks against state-of-the-art DNNs including ResNets, DenseNets, Inceptions, Inception-ResNet, Squeeze-and-Excitation Network (SENet) and robustly trained DNNs. We show that employing SGM on the gradient flow can greatly improve the transferability of crafted attacks in almost all cases. Furthermore, SGM can be easily combined with existing black-box attack techniques, and obtain high improvements over state-of-the-art transferability methods. Our findings not only motivate new research into the architectural vulnerability of DNNs, but also open up further challenges for the design of secure DNN architectures.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Adversarial Example, Transferability, Skip Connection, Neural Network</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BJlRs34Fvr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="H1ebhnEYDH" data-number="178">
      <h4>
        <a href="/forum?id=H1ebhnEYDH">
          White Noise Analysis of Neural Networks
        </a>


        <a href="/pdf?id=H1ebhnEYDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=aliborji%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="aliborji@gmail.com">Ali Borji</a>, <a href="/profile?email=sikun%40ucsb.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sikun@ucsb.edu">Sikun Lin</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>12 Replies</span>


      </div>

      <a href="#H1ebhnEYDH-details-133" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="H1ebhnEYDH-details-133">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A white noise analysis of modern deep neural networks is presented to unveil
                their biases at the whole network level or the single neuron level. Our analysis is
                based on two popular and related methods in psychophysics and neurophysiology
                namely classification images and spike triggered analysis. These methods have
                been widely used to understand the underlying mechanisms of sensory systems
                in humans and monkeys. We leverage them to investigate the inherent biases of
                deep neural networks and to obtain a first-order approximation of their functionality.
                We emphasize on CNNs since they are currently the state of the art methods
                in computer vision and are a decent model of human visual processing. In
                addition, we study multi-layer perceptrons, logistic regression, and recurrent neural
                networks. Experiments over four classic datasets, MNIST, Fashion-MNIST,
                CIFAR-10, and ImageNet, show that the computed bias maps resemble the target
                classes and when used for classification lead to an over two-fold performance than
                the chance level. Further, we show that classification images can be used to attack
                a black-box classifier and to detect adversarial patch attacks. Finally, we utilize
                spike triggered averaging to derive the filters of CNNs and explore how the behavior
                of a network changes when neurons in different layers are modulated. Our
                effort illustrates a successful example of borrowing from neurosciences to study
                ANNs and highlights the importance of cross-fertilization and synergy across machine
                learning, deep learning, and computational neuroscience.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Classification images, spike triggered analysis, deep learning, network visualization, adversarial attack, adversarial defense, microstimulation, computational neuroscience</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/aliborji/WhiteNoiseAnalysis.git</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=H1ebhnEYDH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Byl8hhNYPS" data-number="189">
      <h4>
        <a href="/forum?id=Byl8hhNYPS">
          Neural Machine Translation with Universal Visual Representation
        </a>


        <a href="/pdf?id=Byl8hhNYPS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=zhangzs%40sjtu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhangzs@sjtu.edu.cn">Zhuosheng Zhang</a>, <a href="/profile?email=khchen%40nict.go.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="khchen@nict.go.jp">Kehai Chen</a>, <a href="/profile?email=wangrui%40nict.go.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangrui@nict.go.jp">Rui Wang</a>, <a href="/profile?email=mutiyama%40nict.go.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="mutiyama@nict.go.jp">Masao Utiyama</a>, <a href="/profile?email=eiichiro.sumita%40nict.go.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="eiichiro.sumita@nict.go.jp">Eiichiro Sumita</a>, <a href="/profile?email=charlee%40sjtu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="charlee@sjtu.edu.cn">Zuchao Li</a>, <a href="/profile?email=zhaohai%40cs.sjtu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhaohai@cs.sjtu.edu.cn">Hai Zhao</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 30 Apr 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#Byl8hhNYPS-details-208" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Byl8hhNYPS-details-208">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence, extending image applicability in NMT.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Machine Translation, Visual Representation, Multimodal Machine Translation, Language Representation</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/cooelf/UVR-NMT</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Byl8hhNYPS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BJeKh3VYDH" data-number="198">
      <h4>
        <a href="/forum?id=BJeKh3VYDH">
          Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds
        </a>


        <a href="/pdf?id=BJeKh3VYDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=lukas.prantl%40tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="lukas.prantl@tum.de">Lukas Prantl</a>, <a href="/profile?email=nuttapong26%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="nuttapong26@gmail.com">Nuttapong Chentanez</a>, <a href="/profile?email=jeschke%40stefan-jeschke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jeschke@stefan-jeschke.com">Stefan Jeschke</a>, <a href="/profile?email=nils.thuerey%40tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="nils.thuerey@tum.de">Nils Thuerey</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#BJeKh3VYDH-details-500" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BJeKh3VYDH-details-500">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">point clouds, spatio-temporal representations, Lagrangian data, temporal coherence, super-resolution, denoising</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a generative neural network approach for temporally coherent point clouds.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Point clouds, as a form of Lagrangian representation, allow for powerful and flexible applications in a large number of computational disciplines. We propose a novel deep-learning method to learn stable and temporally coherent feature spaces for points clouds that change over time. We identify a set of inherent problems with these approaches: without knowledge of the time dimension, the inferred solutions can exhibit strong flickering, and easy solutions to suppress this flickering can result in undesirable local minima that manifest themselves as halo structures. We propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the aforementioned halos. We combine these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. We show that our method works for large, deforming point sets from different sources to demonstrate the flexibility of our approach.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BJeKh3VYDH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BJlS634tPr" data-number="226">
      <h4>
        <a href="/forum?id=BJlS634tPr">
          PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search
        </a>


        <a href="/pdf?id=BJlS634tPr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=yuhuixu%40sjtu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="yuhuixu@sjtu.edu.cn">Yuhui Xu</a>, <a href="/profile?email=198808xc%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="198808xc@gmail.com">Lingxi Xie</a>, <a href="/profile?email=zxphistory%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zxphistory@gmail.com">Xiaopeng Zhang</a>, <a href="/profile?email=1410452%40tongji.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="1410452@tongji.edu.cn">Xin Chen</a>, <a href="/profile?email=guojunq%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="guojunq@gmail.com">Guo-Jun Qi</a>, <a href="/profile?email=tian.qi1%40huawei.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tian.qi1@huawei.com">Qi Tian</a>, <a href="/profile?email=xionghongkai%40sjtu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="xionghongkai@sjtu.edu.cn">Hongkai Xiong</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#BJlS634tPr-details-154" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BJlS634tPr-details-154">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Allowing partial channel connection in super-networks to regularize and accelerate differentiable architecture search</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Differentiable architecture search (DARTS) provided a fast solution in finding effective network architectures, but suffered from large memory and computing overheads in jointly training a super-net and searching for an optimal architecture. In this paper, we present a novel approach, namely Partially-Connected DARTS, by sampling a small part of super-net to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, we perform operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges of super-net caused by sampling different channels. We solve it by introducing edge normalization, which adds a new set of edge-level hyper-parameters to reduce uncertainty in search. Thanks to the reduced memory cost, PC-DARTS can be trained with a larger batch size and, consequently, enjoy both faster speed and higher training stability. Experiment results demonstrate the effectiveness of the proposed method. Specifically, we achieve an error rate of 2.57% on CIFAR10 within merely 0.1 GPU-days for architecture search, and a state-of-the-art top-1 error rate of 24.2% on ImageNet (under the mobile setting) within 3.8 GPU-days for search. Our code has been made available at https://www.dropbox.com/sh/on9lg3rpx1r6dkf/AABG5mt0sMHjnEJyoRnLEYW4a?dl=0.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://www.dropbox.com/sh/on9lg3rpx1r6dkf/AABG5mt0sMHjnEJyoRnLEYW4a?dl=0</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Architecture Search, DARTS, Regularization, Normalization</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BJlS634tPr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rkxZyaNtwB" data-number="290">
      <h4>
        <a href="/forum?id=rkxZyaNtwB">
          Online and stochastic optimization beyond Lipschitz continuity: A Riemannian approach
        </a>


        <a href="/pdf?id=rkxZyaNtwB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=kimon.antonakopoulos%40inria.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="kimon.antonakopoulos@inria.fr">Kimon Antonakopoulos</a>, <a href="/profile?email=veronica.belmega%40ensea.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="veronica.belmega@ensea.fr">E. Veronica Belmega</a>, <a href="/profile?email=panayotis.mertikopoulos%40imag.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="panayotis.mertikopoulos@imag.fr">Panayotis Mertikopoulos</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 25 May 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#rkxZyaNtwB-details-566" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rkxZyaNtwB-details-566">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Online optimization, stochastic optimization, Poisson inverse problems</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a novel version of Lipschitz objective continuity that allows stochastic mirror descent methodologies to achieve optimal convergence rates in problems with singularities.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Motivated by applications to machine learning and imaging science, we study a class of online and stochastic optimization problems with loss functions that are not Lipschitz continuous; in particular, the loss functions encountered by the optimizer could exhibit gradient singularities or be singular themselves. Drawing on tools and techniques from Riemannian geometry, we examine a Riemann–Lipschitz (RL) continuity condition which is tailored to the singularity landscape of the problem’s loss functions. In this way, we are able to tackle cases beyond the Lipschitz framework provided by a global norm, and we derive optimal regret bounds and last iterate convergence results through the use of regularized learning methods (such as online mirror descent). These results are subsequently validated in a class of stochastic Poisson inverse problems that arise in imaging science.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rkxZyaNtwB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Skgvy64tvr" data-number="305">
      <h4>
        <a href="/forum?id=Skgvy64tvr">
          Enhancing Adversarial Defense by k-Winners-Take-All
        </a>


        <a href="/pdf?id=Skgvy64tvr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=chang%40cs.columbia.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="chang@cs.columbia.edu">Chang Xiao</a>, <a href="/profile?email=pz2225%40columbia.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="pz2225@columbia.edu">Peilin Zhong</a>, <a href="/profile?email=cxz%40cs.columbia.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="cxz@cs.columbia.edu">Changxi Zheng</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#Skgvy64tvr-details-886" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Skgvy64tvr-details-886">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">adversarial defense, activation function, winner takes all</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a simple change to existing neural network structures for better defending against gradient-based adversarial attacks, using the k-winners-take-all activation function.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a simple change to existing neural network structures for better defending against gradient-based adversarial attacks. Instead of using popular activation functions (such as ReLU), we advocate the use of k-Winners-Take-All (k-WTA) activation, a C0 discontinuous function that purposely invalidates the neural network model’s gradient at densely distributed input data points. The proposed k-WTA activation can be readily used in nearly all existing networks and training methods with no significant overhead. Our proposal is theoretically rationalized. We analyze why the discontinuities in k-WTA networks can largely prevent gradient-based search of adversarial examples and why they at the same time remain innocuous to the network training. This understanding is also empirically backed. We test k-WTA activation on various network structures optimized by a training method, be it adversarial training or not. In all cases, the robustness of k-WTA networks outperforms that of traditional networks under white-box attacks.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/a554b554/kWTA-Activation</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Skgvy64tvr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Hke-WTVtwr" data-number="364">
      <h4>
        <a href="/forum?id=Hke-WTVtwr">
          Encoding word order in complex embeddings
        </a>


        <a href="/pdf?id=Hke-WTVtwr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=wang%40dei.unipd.it" class="profile-link" data-toggle="tooltip" data-placement="top" title="wang@dei.unipd.it">Benyou Wang</a>, <a href="/profile?email=zhaodh%40tju.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhaodh@tju.edu.cn">Donghao Zhao</a>, <a href="/profile?email=chrh%40di.ku.dk" class="profile-link" data-toggle="tooltip" data-placement="top" title="chrh@di.ku.dk">Christina Lioma</a>, <a href="/profile?email=qiuchili%40dei.unipd.it" class="profile-link" data-toggle="tooltip" data-placement="top" title="qiuchili@dei.unipd.it">Qiuchi Li</a>, <a href="/profile?email=pzhang%40tju.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="pzhang@tju.edu.cn">Peng Zhang</a>, <a href="/profile?email=simonsen%40di.ku.dk" class="profile-link" data-toggle="tooltip" data-placement="top" title="simonsen@di.ku.dk">Jakob Grue Simonsen</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>14 Replies</span>


      </div>

      <a href="#Hke-WTVtwr-details-546" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Hke-WTVtwr-details-546">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Sequential word order is important when processing text. Currently, neural networks (NNs) address this by modeling word position using position embeddings. The problem is that position embeddings capture the position of individual words, but not the ordered relationship (e.g., adjacency or precedence) between individual word positions. We present a novel and principled solution for modeling both the global absolute positions of words and their order relationships. Our solution generalizes word embeddings, previously defined as independent vectors, to continuous word functions over a variable (position). The benefit of continuous functions over variable positions is that word representations shift smoothly with increasing positions. Hence, word representations in different positions can correlate with each other in a continuous function. The general solution of these functions can be extended to complex-valued variants. We extend CNN, RNN and Transformer NNs to complex-valued versions to incorporate our complex embedding (we make all code available). Experiments on text classification, machine translation and language modeling show gains over both classical word embeddings and position-enriched word embeddings. To our knowledge, this is the first work in NLP to link imaginary numbers in complex-valued representations to concrete meanings (i.e., word order).</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/iclr-complex-order/complex-order</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">word embedding, complex-valued neural network, position embedding</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Hke-WTVtwr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="B1x1ma4tDr" data-number="435">
      <h4>
        <a href="/forum?id=B1x1ma4tDr">
          DDSP: Differentiable Digital Signal Processing
        </a>


        <a href="/pdf?id=B1x1ma4tDr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=jesseengel%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jesseengel@google.com">Jesse Engel</a>, <a href="/profile?email=hanoih%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hanoih@google.com">Lamtharn (Hanoi) Hantrakul</a>, <a href="/profile?email=gcj%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gcj@google.com">Chenjie Gu</a>, <a href="/profile?email=adarob%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="adarob@google.com">Adam Roberts</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 27 Aug 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>20 Replies</span>


      </div>

      <a href="#B1x1ma4tDr-details-364" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="B1x1ma4tDr-details-364">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Better audio synthesis by combining interpretable DSP with end-to-end learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will is available at https://github.com/magenta/ddsp and we encourage further contributions from the community and domain experts.
              </span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dsp, audio, music, nsynth, wavenet, wavernn, vocoder, synthesizer, sound, signal, processing, tensorflow, autoencoder, disentanglement</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/magenta/ddsp</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=B1x1ma4tDr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SJl5Np4tPr" data-number="496">
      <h4>
        <a href="/forum?id=SJl5Np4tPr">
          Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation
        </a>


        <a href="/pdf?id=SJl5Np4tPr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=htseng6%40ucmerced.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="htseng6@ucmerced.edu">Hung-Yu Tseng</a>, <a href="/profile?email=hlee246%40ucmerced.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="hlee246@ucmerced.edu">Hsin-Ying Lee</a>, <a href="/profile?email=jbhuang%40vt.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jbhuang@vt.edu">Jia-Bin Huang</a>, <a href="/profile?email=mhyang%40ucmerced.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mhyang@ucmerced.edu">Ming-Hsuan Yang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#SJl5Np4tPr-details-501" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SJl5Np4tPr-details-501">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Few-shot classification aims to recognize novel categories with only few labeled images in each class. Existing metric-based few-shot classification algorithms predict categories by comparing the feature embeddings of query images with those from a few labeled images (support examples) using a learned metric function. While promising performance has been demonstrated, these methods often fail to generalize to unseen domains due to large discrepancy of the feature distribution across domains. In this work, we address the problem of few-shot classification under domain shifts for metric-based methods. Our core idea is to use feature-wise transformation layers for augmenting the image features using affine transforms to simulate various feature distributions under different domains in the training stage. To capture variations of the feature distributions under different domains, we further apply a learning-to-learn approach to search for the hyper-parameters of the feature-wise transformation layers. We conduct extensive experiments and ablation studies under the domain generalization setting using five few-shot classification datasets: mini-ImageNet, CUB, Cars, Places, and Plantae. Experimental results demonstrate that the proposed feature-wise transformation layer is applicable to various metric-based models, and provides consistent improvements on the few-shot classification performance under domain shift.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SJl5Np4tPr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HklRwaEKwB" data-number="615">
      <h4>
        <a href="/forum?id=HklRwaEKwB">
          Ridge Regression: Structure, Cross-Validation, and Sketching
        </a>


        <a href="/pdf?id=HklRwaEKwB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=sfliu%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sfliu@stanford.edu">Sifan Liu</a>, <a href="/profile?email=dobribanedgar%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dobribanedgar@gmail.com">Edgar Dobriban</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>5 Replies</span>


      </div>

      <a href="#HklRwaEKwB-details-613" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HklRwaEKwB-details-613">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">ridge regression, sketching, random matrix theory, cross-validation, high-dimensional asymptotics</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We study the structure of ridge regression in a high-dimensional asymptotic framework, and get insights about cross-validation and sketching.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise.
                We study the bias of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="172">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D43E TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>K</mi>
                    </math></mjx-assistive-mml>
                </mjx-container>-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/liusf15/RidgeRegression</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HklRwaEKwB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SJgndT4KwB" data-number="646">
      <h4>
        <a href="/forum?id=SJgndT4KwB">
          Finite Depth and Width Corrections to the Neural Tangent Kernel
        </a>


        <a href="/pdf?id=SJgndT4KwB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=bhanin%40math.tamu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="bhanin@math.tamu.edu">Boris Hanin</a>, <a href="/profile?email=mnica%40math.utoronto.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="mnica@math.utoronto.ca">Mihai Nica</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#SJgndT4KwB-details-298" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SJgndT4KwB-details-298">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Tangent Kernel, Finite Width Corrections, Random ReLU Net, Wide Networks, Deep Networks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">The neural tangent kernel in a randomly initialized ReLU net is non-trivial fluctuations as long as the depth and width are comparable. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We prove the precise scaling, at finite depth and width, for the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network. The standard deviation is exponential in the ratio of network depth to width. Thus, even in the limit of infinite overparameterization, the NTK is not deterministic if depth and width simultaneously tend to infinity. Moreover, we prove that for such deep and wide networks, the NTK has a non-trivial evolution during training by showing that the mean of its first SGD update is also exponential in the ratio of network depth to width. This is sharp contrast to the regime where depth is fixed and network width is very large. Our results suggest that, unlike relatively shallow and wide networks, deep and wide ReLU networks are capable of learning data-dependent features even in the so-called lazy training regime. </span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SJgndT4KwB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BklEFpEYwS" data-number="665">
      <h4>
        <a href="/forum?id=BklEFpEYwS">
          Meta-Learning without Memorization
        </a>


        <a href="/pdf?id=BklEFpEYwS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=mzyin%40utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mzyin@utexas.edu">Mingzhang Yin</a>, <a href="/profile?email=gjt%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gjt@google.com">George Tucker</a>, <a href="/profile?email=mingyuan.zhou%40mccombs.utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mingyuan.zhou@mccombs.utexas.edu">Mingyuan Zhou</a>, <a href="/profile?email=svlevine%40eecs.berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="svlevine@eecs.berkeley.edu">Sergey Levine</a>, <a href="/profile?email=cbfinn%40cs.stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="cbfinn@cs.stanford.edu">Chelsea Finn</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 28 Apr 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#BklEFpEYwS-details-363" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BklEFpEYwS-details-363">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">meta-learning, memorization, regularization, overfitting, mutually-exclusive</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We identify and formalize the memorization problem in meta-learning and solve this problem with novel meta-regularization method, which greatly expand the domain that meta-learning can be applicable to and effective on.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.&nbsp;</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/google-research/google-research/tree/master/meta_learning_without_memorization</span>
            </li>
            <li>
              <strong class="note-content-field">Poster:</strong>
              <span class="note-content-value "><a href="/attachment?id=BklEFpEYwS&amp;name=poster" class="attachment-download-link" title="Download Poster" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Full Presentation Video:</strong>
              <span class="note-content-value ">https://slideslive.com/38922670/invited-talk-the-big-problem-with-metalearning-and-how-bayesians-can-fix-it</span>
            </li>
            <li>
              <strong class="note-content-field">Slides:</strong>
              <span class="note-content-value ">https://mingzhang-yin.github.io/assets/pdfs/iclr2020_slides.pdf</span>
            </li>
            <li>
              <strong class="note-content-field">Spotlight Video:</strong>
              <span class="note-content-value ">https://www.youtube.com/watch?v=emUvd3WqHMs&amp;feature=youtu.be</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BklEFpEYwS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BJgy96EYvr" data-number="691">
      <h4>
        <a href="/forum?id=BJgy96EYvr">
          Influence-Based Multi-Agent Exploration
        </a>


        <a href="/pdf?id=BJgy96EYvr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=tonghanwang1996%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tonghanwang1996@gmail.com">Tonghan Wang*</a>, <a href="/profile?email=1040594377%40qq.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="1040594377@qq.com">Jianhao Wang*</a>, <a href="/profile?email=jxwuyi%40openai.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jxwuyi@openai.com">Yi Wu</a>, <a href="/profile?email=chongjie%40tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="chongjie@tsinghua.edu.cn">Chongjie Zhang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>13 Replies</span>


      </div>

      <a href="#BJgy96EYvr-details-312" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BJgy96EYvr-details-312">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-agent reinforcement learning, Exploration</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Intrinsically motivated reinforcement learning aims to address the exploration challenge for sparse-reward tasks. However, the study of exploration methods in transition-dependent multi-agent settings is largely absent from the literature. We aim to take a step towards solving this problem. We present two exploration methods: exploration via information-theoretic influence (EITI) and exploration via decision-theoretic influence (EDTI), by exploiting the role of interaction in coordinated behaviors of agents. EITI uses mutual information to capture the interdependence between the transition dynamics of agents. EDTI uses a novel intrinsic reward, called Value of Interaction (VoI), to characterize and quantify the influence of one agent's behavior on expected returns of other agents. By optimizing EITI or EDTI objective as a regularizer, agents are encouraged to coordinate their exploration and learn policies to optimize the team performance. We show how to optimize these regularizers so that they can be easily integrated with policy gradient reinforcement learning. The resulting update rule draws a connection between coordinated exploration and intrinsic reward distribution. Finally, we empirically demonstrate the significant strength of our methods in a variety of multi-agent scenarios.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/TonghanWang/EITI-EDTI</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BJgy96EYvr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SJeqs6EFvB" data-number="754">
      <h4>
        <a href="/forum?id=SJeqs6EFvB">
          HOPPITY: LEARNING GRAPH TRANSFORMATIONS TO DETECT AND FIX BUGS IN PROGRAMS
        </a>


        <a href="/pdf?id=SJeqs6EFvB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=edinella%40seas.upenn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="edinella@seas.upenn.edu">Elizabeth Dinella</a>, <a href="/profile?email=hadai%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hadai@google.com">Hanjun Dai</a>, <a href="/profile?email=liby99%40seas.upenn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="liby99@seas.upenn.edu">Ziyang Li</a>, <a href="/profile?email=mhnaik%40cis.upenn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mhnaik@cis.upenn.edu">Mayur Naik</a>, <a href="/profile?email=lsong%40cc.gatech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="lsong@cc.gatech.edu">Le Song</a>, <a href="/profile?email=kewang%40visa.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kewang@visa.com">Ke Wang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 23 May 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#SJeqs6EFvB-details-545" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SJeqs6EFvB-details-545">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">An learning-based approach for detecting and fixing bugs in Javascript</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a learning-based approach to detect and fix a broad range of bugs in Javascript programs. We frame the problem in terms of learning a sequence of graph transformations: given a buggy program modeled by a graph structure, our model makes a sequence of predictions including the position of bug nodes and corresponding graph edits to produce a fix. Unlike previous works that use deep neural networks, our approach targets bugs that are more complex and semantic in nature (i.e.~bugs that require adding or deleting statements to fix). We have realized our approach in a tool called HOPPITY. By training on 290,715 Javascript code change commits on Github, HOPPITY correctly detects and fixes bugs in 9,490 out of 36,361 programs in an end-to-end fashion. Given the bug location and type of the fix, HOPPITY also outperforms the baseline approach by a wide margin.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Bug Detection, Program Repair, Graph Neural Network, Graph Transformation</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/AI-nstein/hoppity</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SJeqs6EFvB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BJge3TNKwH" data-number="769">
      <h4>
        <a href="/forum?id=BJge3TNKwH">
          Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations
        </a>


        <a href="/pdf?id=BJge3TNKwH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=skolouri%40hrl.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="skolouri@hrl.com">Soheil Kolouri</a>, <a href="/profile?email=naketz%40hrl.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="naketz@hrl.com">Nicholas A. Ketz</a>, <a href="/profile?email=a.soltoggio%40lboro.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="a.soltoggio@lboro.ac.uk">Andrea Soltoggio</a>, <a href="/profile?email=pkpilly%40hrl.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="pkpilly@hrl.com">Praveen K. Pilly</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>5 Replies</span>


      </div>

      <a href="#BJge3TNKwH-details-364" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BJge3TNKwH-details-364">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">selective plasticity, catastrophic forgetting, intransigence</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">"A novel framework for overcoming catastrophic forgetting by preserving the distribution of the network's output at an arbitrary layer."</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training. Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods like Elastic Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS). We then propose a fundamentally different class of preservation methods that aim at preserving the distribution of internal neural representations for previous tasks while learning a new one. We propose the sliced Cram\'{e}r distance as a suitable choice for such preservation and evaluate our Sliced Cramer Preservation (SCP) algorithm through extensive empirical investigations on various network architectures in both supervised and unsupervised learning settings. We show that SCP consistently utilizes the learning capacity of the network better than online-EWC and MAS methods on various incremental learning tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BJge3TNKwH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rJeB36NKvB" data-number="781">
      <h4>
        <a href="/forum?id=rJeB36NKvB">
          How much Position Information Do Convolutional Neural Networks Encode?
        </a>


        <a href="/pdf?id=rJeB36NKvB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=amirul%40scs.ryerson.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="amirul@scs.ryerson.ca">Md Amirul Islam*</a>, <a href="/profile?email=sen.jia%40ryerson.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="sen.jia@ryerson.ca">Sen Jia*</a>, <a href="/profile?email=bruce%40ryerson.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="bruce@ryerson.ca">Neil D. B. Bruce</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>17 Replies</span>


      </div>

      <a href="#rJeB36NKvB-details-135" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rJeB36NKvB-details-135">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">network understanding, absolute position information</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rJeB36NKvB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HJenn6VFvB" data-number="797">
      <h4>
        <a href="/forum?id=HJenn6VFvB">
          Hamiltonian Generative Networks
        </a>


        <a href="/pdf?id=HJenn6VFvB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=petertoth%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="petertoth@google.com">Peter Toth</a>, <a href="/profile?email=danilor%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="danilor@google.com">Danilo J. Rezende</a>, <a href="/profile?email=drewjaegle%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="drewjaegle@google.com">Andrew Jaegle</a>, <a href="/profile?email=sracaniere%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sracaniere@google.com">Sébastien Racanière</a>, <a href="/profile?email=botev%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="botev@google.com">Aleksandar Botev</a>, <a href="/profile?email=irinah%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="irinah@google.com">Irina Higgins</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 23 Apr 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>13 Replies</span>


      </div>

      <a href="#HJenn6VFvB-details-14" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HJenn6VFvB-details-14">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a class of generative models that reliably learn Hamiltonian dynamics from high-dimensional observations. The learnt Hamiltonian can be applied to sequence modeling or as a normalising flow.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time, and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses Hamiltonian dynamics to model expressive densities. Hence, we hope that our work serves as a first practical demonstration of the value that the Hamiltonian formalism can bring to machine learning. More results and video evaluations are available at: http://tiny.cc/hgn</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hamiltonian dynamics, normalising flows, generative model, physics</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HJenn6VFvB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SkeyppEFvS" data-number="804">
      <h4>
        <a href="/forum?id=SkeyppEFvS">
          CoPhy: Counterfactual Learning of Physical Dynamics
        </a>


        <a href="/pdf?id=SkeyppEFvS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=fabien.baradel%40insa-lyon.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="fabien.baradel@insa-lyon.fr">Fabien Baradel</a>, <a href="/profile?email=nneverova%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="nneverova@fb.com">Natalia Neverova</a>, <a href="/profile?email=julien.mille%40insa-cvl.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="julien.mille@insa-cvl.fr">Julien Mille</a>, <a href="/profile?email=mori%40cs.sfu.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="mori@cs.sfu.ca">Greg Mori</a>, <a href="/profile?email=christian.wolf%40insa-lyon.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="christian.wolf@insa-lyon.fr">Christian Wolf</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#SkeyppEFvS-details-93" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SkeyppEFvS-details-93">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input. We develop the CoPhy benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower of blocks, a set of bouncing balls or colliding objects, we learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene. The alternative future is predicted given the altered past and a latent representation of the confounders learned by the model in an end-to-end fashion with no supervision. We compare against feedforward video prediction baselines and show how observing alternative experiences allows the network to capture latent physical properties of the environment, which results in significantly more accurate predictions at the level of super human performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">intuitive physics, visual reasoning</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/fabienbaradel/cophy</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SkeyppEFvS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BJg866NFvB" data-number="820">
      <h4>
        <a href="/forum?id=BJg866NFvB">
          Estimating counterfactual treatment outcomes over time through adversarially balanced representations
        </a>


        <a href="/pdf?id=BJg866NFvB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=ioana.bica%40eng.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="ioana.bica@eng.ox.ac.uk">Ioana Bica</a>, <a href="/profile?email=a7med3laa%40hotmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="a7med3laa@hotmail.com">Ahmed M Alaa</a>, <a href="/profile?email=james.jordon%40wolfson.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="james.jordon@wolfson.ox.ac.uk">James Jordon</a>, <a href="/profile?email=mschaar%40turing.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="mschaar@turing.ac.uk">Mihaela van der Schaar</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#BJg866NFvB-details-867" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BJg866NFvB-details-867">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">treatment effects over time, causal inference, counterfactual estimation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Identifying when to give treatments to patients and how to select among multiple treatments over time are important medical problems with a few existing solutions. In this paper, we introduce the Counterfactual Recurrent Network (CRN), a novel sequence-to-sequence model that leverages the increasingly available patient observational data to estimate treatment effects over time and answer such medical questions. To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the observational data, CRN uses domain adversarial training to build balancing representations of the patient history. At each timestep, CRN constructs a treatment invariant representation which removes the association between patient history and treatment assignments and thus can be reliably used for making counterfactual predictions. On a simulated model of tumour growth, with varying degree of time-dependent confounding, we show how our model achieves lower error in estimating counterfactuals and in choosing the correct treatment and timing of treatment than current state-of-the-art methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BJg866NFvB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Skep6TVYDB" data-number="836">
      <h4>
        <a href="/forum?id=Skep6TVYDB">
          Gradientless Descent: High-Dimensional Zeroth-Order Optimization
        </a>


        <a href="/pdf?id=Skep6TVYDB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=dgg%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dgg@google.com">Daniel Golovin</a>, <a href="/profile?email=karro%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="karro@google.com">John Karro</a>, <a href="/profile?email=gpk%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gpk@google.com">Greg Kochanski</a>, <a href="/profile?email=chansoo%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="chansoo@google.com">Chansoo Lee</a>, <a href="/profile?email=xingyousong%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="xingyousong@google.com">Xingyou Song</a>, <a href="/profile?email=qiuyiz%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="qiuyiz@google.com">Qiuyi Zhang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#Skep6TVYDB-details-890" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Skep6TVYDB-details-890">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Zeroth Order Optimization</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Zeroth-order optimization is the process of minimizing an objective <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="173">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D453 TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-mo class="mjx-n">
                      <mjx-c class="mjx-c28"></mjx-c>
                    </mjx-mo>
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D465 TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-mo class="mjx-n">
                      <mjx-c class="mjx-c29"></mjx-c>
                    </mjx-mo>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>f</mi>
                      <mo stretchy="false">(</mo>
                      <mi>x</mi>
                      <mo stretchy="false">)</mo>
                    </math></mjx-assistive-mml>
                </mjx-container>, given oracle access to evaluations at adaptively chosen inputs <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="174">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D465 TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>x</mi>
                    </math></mjx-assistive-mml>
                </mjx-container>. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\it any monotone transform} of a smooth and strongly convex objective with latent dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="175">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D458 TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-mo class="mjx-n" space="4">
                      <mjx-c class="mjx-c2265"></mjx-c>
                    </mjx-mo>
                    <mjx-mi class="mjx-i" space="4">
                      <mjx-c class="mjx-c1D45B TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>k</mi>
                      <mo>≥</mo>
                      <mi>n</mi>
                    </math></mjx-assistive-mml>
                </mjx-container>, we present a novel analysis that shows convergence within an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="176">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D716 TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>ϵ</mi>
                    </math></mjx-assistive-mml>
                </mjx-container>-ball of the optimum in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="177">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D442 TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-mo class="mjx-n">
                      <mjx-c class="mjx-c28"></mjx-c>
                    </mjx-mo>
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D458 TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D444 TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-mi class="mjx-n" space="2">
                      <mjx-c class="mjx-c6C"></mjx-c>
                      <mjx-c class="mjx-c6F"></mjx-c>
                      <mjx-c class="mjx-c67"></mjx-c>
                    </mjx-mi>
                    <mjx-mo class="mjx-n">
                      <mjx-c class="mjx-c2061"></mjx-c>
                    </mjx-mo>
                    <mjx-mo class="mjx-n">
                      <mjx-c class="mjx-c28"></mjx-c>
                    </mjx-mo>
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D45B TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-mo class="mjx-n">
                      <mjx-c class="mjx-c29"></mjx-c>
                    </mjx-mo>
                    <mjx-mi class="mjx-n" space="2">
                      <mjx-c class="mjx-c6C"></mjx-c>
                      <mjx-c class="mjx-c6F"></mjx-c>
                      <mjx-c class="mjx-c67"></mjx-c>
                    </mjx-mi>
                    <mjx-mo class="mjx-n">
                      <mjx-c class="mjx-c2061"></mjx-c>
                    </mjx-mo>
                    <mjx-mo class="mjx-n">
                      <mjx-c class="mjx-c28"></mjx-c>
                    </mjx-mo>
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D445 TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-texatom texclass="ORD">
                      <mjx-mo class="mjx-n">
                        <mjx-c class="mjx-c2F"></mjx-c>
                      </mjx-mo>
                    </mjx-texatom>
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D716 TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-mo class="mjx-n">
                      <mjx-c class="mjx-c29"></mjx-c>
                    </mjx-mo>
                    <mjx-mo class="mjx-n">
                      <mjx-c class="mjx-c29"></mjx-c>
                    </mjx-mo>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>O</mi>
                      <mo stretchy="false">(</mo>
                      <mi>k</mi>
                      <mi>Q</mi>
                      <mi>log</mi>
                      <mo data-mjx-texclass="NONE">⁡</mo>
                      <mo stretchy="false">(</mo>
                      <mi>n</mi>
                      <mo stretchy="false">)</mo>
                      <mi>log</mi>
                      <mo data-mjx-texclass="NONE">⁡</mo>
                      <mo stretchy="false">(</mo>
                      <mi>R</mi>
                      <mrow>
                        <mo>/</mo>
                      </mrow>
                      <mi>ϵ</mi>
                      <mo stretchy="false">)</mo>
                      <mo stretchy="false">)</mo>
                    </math></mjx-assistive-mml>
                </mjx-container> evaluations, where the input dimension is <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="178">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D45B TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>n</mi>
                    </math></mjx-assistive-mml>
                </mjx-container>, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="179">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D445 TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>R</mi>
                    </math></mjx-assistive-mml>
                </mjx-container> is the diameter of the input space and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="180">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D444 TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>Q</mi>
                    </math></mjx-assistive-mml>
                </mjx-container> is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.
              </span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Skep6TVYDB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Hkekl0NFPr" data-number="913">
      <h4>
        <a href="/forum?id=Hkekl0NFPr">
          Conditional Learning of Fair Representations
        </a>


        <a href="/pdf?id=Hkekl0NFPr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=han.zhao%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="han.zhao@cs.cmu.edu">Han Zhao</a>, <a href="/profile?email=acoston%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="acoston@cs.cmu.edu">Amanda Coston</a>, <a href="/profile?email=tah47%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="tah47@cam.ac.uk">Tameem Adel</a>, <a href="/profile?email=ggordon%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ggordon@cs.cmu.edu">Geoffrey J. Gordon</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#Hkekl0NFPr-details-248" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Hkekl0NFPr-details-248">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">algorithmic fairness, representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a novel algorithm for learning fair representations that can simultaneously mitigate two notions of disparity among different demographic subgroups.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a novel algorithm for learning fair representations that can simultaneously mitigate two notions of disparity among different demographic subgroups in the classification setting. Two key components underpinning the design of our algorithm are balanced error rate and conditional alignment of representations. We show how these two components contribute to ensuring accuracy parity and equalized false-positive and false-negative rates across groups without impacting demographic parity. Furthermore, we also demonstrate both in theory and on two real-world experiments that the proposed algorithm leads to a better utility-fairness trade-off on balanced datasets compared with existing algorithms on learning fair representations for classification.
              </span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Hkekl0NFPr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="ByxxgCEYDS" data-number="917">
      <h4>
        <a href="/forum?id=ByxxgCEYDS">
          Inductive Matrix Completion Based on Graph Neural Networks
        </a>


        <a href="/pdf?id=ByxxgCEYDS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=muhan%40wustl.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="muhan@wustl.edu">Muhan Zhang</a>, <a href="/profile?email=chen%40cse.wustl.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="chen@cse.wustl.edu">Yixin Chen</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#ByxxgCEYDS-details-664" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="ByxxgCEYDS-details-664">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">matrix completion, graph neural network</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user's age or movie's genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem. IGMC trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive -- it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using side information while achieving similar or better performances than state-of-the-art transductive methods; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) Long-range dependencies might not be necessary for modeling recommender systems.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/muhanzhang/IGMC</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=ByxxgCEYDS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Hkx7xRVYDr" data-number="924">
      <h4>
        <a href="/forum?id=Hkx7xRVYDr">
          Duration-of-Stay Storage Assignment under Uncertainty
        </a>


        <a href="/pdf?id=Hkx7xRVYDr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=mlli%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mlli@mit.edu">Michael Lingzhi Li</a>, <a href="/profile?email=ewolf%40lineagelogistics.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ewolf@lineagelogistics.com">Elliott Wolf</a>, <a href="/profile?email=dwintz%40lineagelogistics.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dwintz@lineagelogistics.com">Daniel Wintz</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>20 Replies</span>


      </div>

      <a href="#Hkx7xRVYDr-details-135" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Hkx7xRVYDr-details-135">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Storage Assignment, Deep Learning, Duration-of-Stay, Application, Natural Language Processing, Parallel Network</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We develop a new storage assignment framework with a novel neural network that enables large efficiency gains in the warehouse.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Storage assignment, the act of choosing what goods are placed in what locations in a warehouse, is a central problem of supply chain logistics. Past literature has shown that the optimal method to assign pallets is to arrange them in increasing duration of stay in the warehouse (the Duration-of-Stay, or DoS, method), but the methodology requires perfect prior knowledge of DoS for each pallet, which is unknown and uncertain under realistic conditions. Attempts to predict DoS have largely been unfruitful due to the multi-valuedness nature (every shipment contains multiple identical pallets with different DoS) and data sparsity induced by lack of matching historical conditions. In this paper, we introduce a new framework for storage assignment that provides a solution to the DoS prediction problem through a distributional reformulation and a novel neural network, ParallelNet. Through collaboration with a world-leading cold storage company, we show that the system is able to predict DoS with a MAPE of 29%, a decrease of ~30% compared to a CNN-LSTM model, and suffers less performance decay into the future. The framework is then integrated into a first-of-its-kind Storage Assignment system, which is being deployed in warehouses across United States, with initial results showing up to 21% in labor savings. We also release the first publicly available set of warehousing records to facilitate research into this central problem.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://anonymous.4open.science/r/8de2111c-d496-423e-86f3-b5e31792bead/</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Hkx7xRVYDr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HklSeREtPB" data-number="926">
      <h4>
        <a href="/forum?id=HklSeREtPB">
          Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks
        </a>


        <a href="/pdf?id=HklSeREtPB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=ccueva%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ccueva@gmail.com">Christopher J. Cueva</a>, <a href="/profile?email=peterwang724%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="peterwang724@gmail.com">Peter Y. Wang</a>, <a href="/profile?email=mattchin35%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mattchin35@gmail.com">Matthew Chin</a>, <a href="/profile?email=weixxpku%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="weixxpku@gmail.com">Xue-Xin Wei</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#HklSeREtPB-details-190" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HklSeREtPB-details-190">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Artificial neural networks trained with gradient descent are capable of recapitulating both realistic neural activity and the anatomical organization of a biological circuit.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent work suggests goal-driven training of neural networks can be used to model neural activity in the brain. While response properties of neurons in artificial neural networks bear similarities to those in the brain, the network architectures are often constrained to be different. Here we ask if a neural network can recover both neural representations and, if the architecture is unconstrained and optimized, also the anatomical properties of neural circuits. We demonstrate this in a system where the connectivity and the functional organization have been characterized, namely, the head direction circuit of the rodent and fruit fly. We trained recurrent neural networks (RNNs) to estimate head direction through integration of angular velocity. We found that the two distinct classes of neurons observed in the head direction system, the Compass neurons and the Shifter neurons, emerged naturally in artificial neural networks as a result of training. Furthermore, connectivity analysis and in-silico neurophysiology revealed structural and mechanistic similarities between artificial networks and the head direction system. Overall, our results show that optimization of RNNs in a goal-driven task can recapitulate the structure and function of biological circuits, suggesting that artificial neural networks can be used to study the brain at the level of both neural activity and anatomical organization.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">recurrent network, head direction system, neural circuits, neural coding</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HklSeREtPB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SyxrxR4KPS" data-number="928">
      <h4>
        <a href="/forum?id=SyxrxR4KPS">
          Deep neuroethology of a virtual rodent
        </a>


        <a href="/pdf?id=SyxrxR4KPS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=jsmerel%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jsmerel@google.com">Josh Merel</a>, <a href="/profile?email=diegoaldarondo%40g.harvard.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="diegoaldarondo@g.harvard.edu">Diego Aldarondo</a>, <a href="/profile?email=jesse_d_marshall%40fas.harvard.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jesse_d_marshall@fas.harvard.edu">Jesse Marshall</a>, <a href="/profile?email=tassa%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tassa@google.com">Yuval Tassa</a>, <a href="/profile?email=gregwayne%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gregwayne@google.com">Greg Wayne</a>, <a href="/profile?email=olveczky%40fas.harvard.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="olveczky@fas.harvard.edu">Bence Olveczky</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#SyxrxR4KPS-details-481" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SyxrxR4KPS-details-481">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">computational neuroscience, motor control, deep RL</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We built a physical simulation of a rodent, trained it to solve a set of tasks, and analyzed the resulting networks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing our understanding of real and artificial neural networks in sensory and cognitive systems. However, this interaction between fields is less developed in the study of motor control. In this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. We then use this platform to study motor activity across contexts by training a model to solve four complex tasks. Using methods familiar to neuroscientists, we describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize motor activity relative to the rodent's behavior and goals. We find that the model uses two classes of representations which respectively encode the task-specific behavioral strategies and task-invariant behavioral kinematics. These representations are reflected in the sequential activity and population dynamics of neural subpopulations. Overall, the virtual rodent facilitates grounded collaborations between deep reinforcement learning and motor neuroscience.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SyxrxR4KPS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1glGANtDr" data-number="990">
      <h4>
        <a href="/forum?id=S1glGANtDr">
          Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation
        </a>


        <a href="/pdf?id=S1glGANtDr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=ztang%40cs.utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ztang@cs.utexas.edu">Ziyang Tang*</a>, <a href="/profile?email=yihao%40cs.utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yihao@cs.utexas.edu">Yihao Feng*</a>, <a href="/profile?email=lihongli.cs%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lihongli.cs@gmail.com">Lihong Li</a>, <a href="/profile?email=dennyzhou%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dennyzhou@google.com">Dengyong Zhou</a>, <a href="/profile?email=lqiang%40cs.utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="lqiang@cs.utexas.edu">Qiang Liu</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>14 Replies</span>


      </div>

      <a href="#S1glGANtDr-details-668" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1glGANtDr-details-668">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We develop a new doubly robust estimator based on the infinite horizon density ratio and off policy value estimation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect. In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">off-policy evaluation, infinite horizon, doubly robust, reinforcement learning</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1glGANtDr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="H1ldzA4tPr" data-number="1005">
      <h4>
        <a href="/forum?id=H1ldzA4tPr">
          Learning Compositional Koopman Operators for Model-Based Control
        </a>


        <a href="/pdf?id=H1ldzA4tPr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=liyunzhu%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="liyunzhu@mit.edu">Yunzhu Li</a>, <a href="/profile?email=haohe%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="haohe@mit.edu">Hao He</a>, <a href="/profile?email=jiajunwu.cs%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jiajunwu.cs@gmail.com">Jiajun Wu</a>, <a href="/profile?email=dina%40csail.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dina@csail.mit.edu">Dina Katabi</a>, <a href="/profile?email=torralba%40csail.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="torralba@csail.mit.edu">Antonio Torralba</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 28 Apr 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>12 Replies</span>


      </div>

      <a href="#H1ldzA4tPr-details-165" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="H1ldzA4tPr-details-165">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Koopman operators, graph neural networks, compositionality</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Learning compositional Koopman operators for efficient system identification and model-based control.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Finding an embedding space for a linear approximation of a nonlinear dynamical system enables efficient system identification and control synthesis. The Koopman operator theory lays the foundation for identifying the nonlinear-to-linear coordinate transformations with data-driven methods. Recently, researchers have proposed to use deep neural networks as a more expressive class of basis functions for calculating the Koopman operators. These approaches, however, assume a fixed dimensional state space; they are therefore not applicable to scenarios with a variable number of objects. In this paper, we propose to learn compositional Koopman operators, using graph neural networks to encode the state into object-centric embeddings and using a block-wise linear transition matrix to regularize the shared structure across objects. The learned dynamics can quickly adapt to new environments of unknown physical parameters and produce control signals to achieve a specified goal. Our experiments on manipulating ropes and controlling soft robots show that the proposed method has better efficiency and generalization ability than existing baselines.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=H1ldzA4tPr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HkxYzANYDB" data-number="1008">
      <h4>
        <a href="/forum?id=HkxYzANYDB">
          CLEVRER: Collision Events for Video Representation and Reasoning
        </a>


        <a href="/pdf?id=HkxYzANYDB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=kyi%40g.harvard.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kyi@g.harvard.edu">Kexin Yi*</a>, <a href="/profile?email=ganchuang1990%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ganchuang1990@gmail.com">Chuang Gan*</a>, <a href="/profile?email=liyunzhu%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="liyunzhu@mit.edu">Yunzhu Li</a>, <a href="/profile?email=pushmeet%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="pushmeet@google.com">Pushmeet Kohli</a>, <a href="/profile?email=jiajunwu%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jiajunwu@mit.edu">Jiajun Wu</a>, <a href="/profile?email=torralba%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="torralba@mit.edu">Antonio Torralba</a>, <a href="/profile?email=jbt%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jbt@mit.edu">Joshua B. Tenenbaum</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#HkxYzANYDB-details-441" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HkxYzANYDB-details-441">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks. Motivated by the theory of human casual judgment, CLEVRER includes four types of question: descriptive (e.g., ‘what color’), explanatory (‘what’s responsible for’), predictive (‘what will happen next’), and counterfactual (‘what if’). We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations. </span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neuro-symbolic, Reasoning</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">http://clevrer.csail.mit.edu/</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HkxYzANYDB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="r1lZ7AEKvB" data-number="1028">
      <h4>
        <a href="/forum?id=r1lZ7AEKvB">
          The Logical Expressiveness of Graph Neural Networks
        </a>


        <a href="/pdf?id=r1lZ7AEKvB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=pbarcelo%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="pbarcelo@gmail.com">Pablo Barceló</a>, <a href="/profile?email=egor.kostylev%40cs.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="egor.kostylev@cs.ox.ac.uk">Egor V. Kostylev</a>, <a href="/profile?email=mikael.monet%40imfd.cl" class="profile-link" data-toggle="tooltip" data-placement="top" title="mikael.monet@imfd.cl">Mikael Monet</a>, <a href="/profile?email=jorge.perez.rojas%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jorge.perez.rojas@gmail.com">Jorge Pérez</a>, <a href="/profile?email=juan.reutter%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="juan.reutter@gmail.com">Juan Reutter</a>, <a href="/profile?email=jpsilvapena%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jpsilvapena@gmail.com">Juan Pablo Silva</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#r1lZ7AEKvB-details-806" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="r1lZ7AEKvB-details-806">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We characterize the expressive power of GNNs in terms of classical logical languages, separating different GNNs and showing connections with standard notions in Knowledge Representation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The ability of graph neural networks (GNNs) for distinguishing nodes in graphs has been recently characterized in terms of the Weisfeiler-Lehman (WL) test for checking graph isomorphism. This characterization, however, does not settle the issue of which Boolean node classifiers (i.e., functions classifying nodes in graphs as true or false) can be expressed by GNNs. We tackle this problem by focusing on Boolean classifiers expressible as formulas in the logic FOC2, a well-studied fragment of first order logic. FOC2 is tightly related to the WL test, and hence to GNNs. We start by studying a popular class of GNNs, which we call AC-GNNs, in which the features of each node in the graph are updated, in successive layers, only in terms of the features of its neighbors. We show that this class of GNNs is too weak to capture all FOC2 classifiers, and provide a syntactic characterization of the largest subclass of FOC2 classifiers that can be captured by AC-GNNs. This subclass coincides with a logic heavily used by the knowledge representation community. We then look at what needs to be added to AC-GNNs for capturing all FOC2 classifiers. We show that it suffices to add readout functions, which allow to update the features of a node not only in terms of its neighbors, but also in terms of a global attribute vector. We call GNNs of this kind ACR-GNNs. We experimentally validate our findings showing that, on synthetic data conforming to FOC2 formulas, AC-GNNs struggle to fit the training data while ACR-GNNs can generalize even to graphs of sizes not seen during training.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://anonymous.4open.science/r/787222e2-ad5e-4810-a788-e80f0fe7eff0/</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, First Order Logic, Expressiveness</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=r1lZ7AEKvB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="r1g87C4KwB" data-number="1039">
      <h4>
        <a href="/forum?id=r1g87C4KwB">
          The Break-Even Point on Optimization Trajectories of Deep Neural Networks
        </a>


        <a href="/pdf?id=r1g87C4KwB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=staszek.jastrzebski%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="staszek.jastrzebski@gmail.com">Stanislaw Jastrzebski</a>, <a href="/profile?email=msz93%40o2.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="msz93@o2.pl">Maciej Szymczak</a>, <a href="/profile?email=stanislav.fort%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="stanislav.fort@gmail.com">Stanislav Fort</a>, <a href="/profile?email=devansharpit%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="devansharpit@gmail.com">Devansh Arpit</a>, <a href="/profile?email=jcktbr%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jcktbr@gmail.com">Jacek Tabor</a>, <a href="/profile?email=kyunghyun.cho%40nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kyunghyun.cho@nyu.edu">Kyunghyun Cho*</a>, <a href="/profile?email=k.j.geras%40nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="k.j.geras@nyu.edu">Krzysztof Geras*</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 28 Apr 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>12 Replies</span>


      </div>

      <a href="#r1g87C4KwB-details-264" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="r1g87C4KwB-details-264">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In the early phase of training of deep neural networks there exists a "break-even point" which determines properties of the entire optimization trajectory.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The early phase of training of deep neural networks is critical for their final performance. In this work, we study how the hyperparameters of stochastic gradient descent (SGD) used in the early phase of training affect the rest of the optimization trajectory. We argue for the existence of the "``break-even" point on this trajectory, beyond which the curvature of the loss surface and noise in the gradient are implicitly regularized by SGD. In particular, we demonstrate on multiple classification tasks that using a large learning rate in the initial phase of training reduces the variance of the gradient, and improves the conditioning of the covariance of gradients. These effects are beneficial from the optimization perspective and become visible after the break-even point. Complementing prior work, we also show that using a low learning rate results in bad conditioning of the loss surface even for a neural network with batch normalization layers. In short, our work shows that key properties of the loss surface are strongly influenced by SGD in the early phase of training. We argue that studying the impact of the identified effects on generalization is a promising future direction.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generalization, sgd, learning rate, batch size, hessian, curvature, trajectory, optimization</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=r1g87C4KwB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="H1eA7AEtvS" data-number="1057">
      <h4>
        <a href="/forum?id=H1eA7AEtvS">
          ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
        </a>


        <a href="/pdf?id=H1eA7AEtvS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=lanzhzh%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lanzhzh@google.com">Zhenzhong Lan</a>, <a href="/profile?email=mchen%40ttic.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mchen@ttic.edu">Mingda Chen</a>, <a href="/profile?email=seabass%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="seabass@google.com">Sebastian Goodman</a>, <a href="/profile?email=kgimpel%40ttic.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kgimpel@ttic.edu">Kevin Gimpel</a>, <a href="/profile?email=piyushsharma%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="piyushsharma@google.com">Piyush Sharma</a>, <a href="/profile?email=rsoricut%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="rsoricut@google.com">Radu Soricut</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>32 Replies</span>


      </div>

      <a href="#H1eA7AEtvS-details-393" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="H1eA7AEtvS-details-393">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Natural Language Processing, BERT, Representation Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/google-research/ALBERT</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=H1eA7AEtvS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HJxrVA4FDS" data-number="1073">
      <h4>
        <a href="/forum?id=HJxrVA4FDS">
          Disentangling neural mechanisms for perceptual grouping
        </a>


        <a href="/pdf?id=HJxrVA4FDS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=junkyung_kim%40brown.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="junkyung_kim@brown.edu">Junkyung Kim*</a>, <a href="/profile?email=drew_linsley%40brown.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="drew_linsley@brown.edu">Drew Linsley*</a>, <a href="/profile?email=kalpit_thakkar%40brown.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kalpit_thakkar@brown.edu">Kalpit Thakkar</a>, <a href="/profile?email=thomas_serre%40brown.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="thomas_serre@brown.edu">Thomas Serre</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#HJxrVA4FDS-details-911" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HJxrVA4FDS-details-911">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Perceptual grouping, visual cortex, recurrent feedback, horizontal connections, top-down connections</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Horizontal and top-down feedback connections are responsible for complementary perceptual grouping strategies in biological and recurrent vision systems.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level "Gestalt" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://bit.ly/2wdQYGd</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HJxrVA4FDS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rJgJDAVKvB" data-number="1161">
      <h4>
        <a href="/forum?id=rJgJDAVKvB">
          Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees
        </a>


        <a href="/pdf?id=rJgJDAVKvB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=binghong%40gatech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="binghong@gatech.edu">Binghong Chen</a>, <a href="/profile?email=bodai%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="bodai@google.com">Bo Dai</a>, <a href="/profile?email=qinjielin2018%40u.northwestern.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="qinjielin2018@u.northwestern.edu">Qinjie Lin</a>, <a href="/profile?email=guoye2018%40u.northwestern.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="guoye2018@u.northwestern.edu">Guo Ye</a>, <a href="/profile?email=hanliu%40northwestern.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="hanliu@northwestern.edu">Han Liu</a>, <a href="/profile?email=lsong%40cc.gatech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="lsong@cc.gatech.edu">Le Song</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#rJgJDAVKvB-details-26" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rJgJDAVKvB-details-26">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">learning to plan, representation learning, learning to design algorithm, reinforcement learning, meta learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a meta path planning algorithm which exploits a novel attention-based neural module that can learn generalizable structures from prior experiences to drastically reduce the sample requirement for solving new path planning problems.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a meta path planning algorithm named \emph{Neural Exploration-Exploitation Trees~(NEXT)} for learning from prior experience for solving new path planning problems in high dimensional continuous state and action spaces. Compared to more classical sampling-based methods like RRT, our approach achieves much better sample efficiency in high-dimensions and can benefit from prior experience of planning in similar environments. More specifically, NEXT exploits a novel neural architecture which can learn promising search directions from problem structures. The learned prior is then integrated into a UCB-type algorithm to achieve an online balance between \emph{exploration} and \emph{exploitation} when solving a new problem. We conduct thorough experiments to show that NEXT accomplishes new planning problems with more compact search trees and significantly outperforms state-of-the-art methods on several benchmarks.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/NeurEXT/NEXT-learning-to-plan/blob/master/main.ipynb</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rJgJDAVKvB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BkgYPREtPr" data-number="1182">
      <h4>
        <a href="/forum?id=BkgYPREtPr">
          Symplectic Recurrent Neural Networks
        </a>


        <a href="/pdf?id=BkgYPREtPr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=zc1216%40nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="zc1216@nyu.edu">Zhengdao Chen</a>, <a href="/profile?email=edzhang%40tju.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="edzhang@tju.edu.cn">Jianyu Zhang</a>, <a href="/profile?email=martinarjovsky%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="martinarjovsky@gmail.com">Martin Arjovsky</a>, <a href="/profile?email=leonb%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="leonb@fb.com">Léon Bottou</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#BkgYPREtPr-details-817" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BkgYPREtPr-details-817">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hamiltonian systems, learning physical laws, symplectic integrators, recurrent neural networks, inverse problems</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. SRNNs model the Hamiltonian function of the system by a neural networks, and leverage symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show SRNNs succeed reliably on complex and noisy Hamiltonian systems. Finally, we show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BkgYPREtPr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1gFvANKDS" data-number="1183">
      <h4>
        <a href="/forum?id=S1gFvANKDS">
          Asymptotics of Wide Networks from Feynman Diagrams
        </a>


        <a href="/pdf?id=S1gFvANKDS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=edyer%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="edyer@google.com">Ethan Dyer</a>, <a href="/profile?email=guyga%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="guyga@google.com">Guy Gur-Ari</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#S1gFvANKDS-details-450" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1gFvANKDS-details-450">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A general method for computing the asymptotic behavior of wide networks using Feynman diagrams</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding the asymptotic behavior of wide networks is of considerable interest. In this work, we present a general method for analyzing this large width behavior. The method is an adaptation of Feynman diagrams, a standard tool for computing multivariate Gaussian integrals. We apply our method to study training dynamics, improving existing bounds and deriving new results on wide network evolution during stochastic gradient descent. Going beyond the strict large width limit, we present closed-form expressions for higher-order terms governing wide network training, and test these predictions empirically.
              </span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1gFvANKDS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Sklgs0NFvr" data-number="1305">
      <h4>
        <a href="/forum?id=Sklgs0NFvr">
          Learning The Difference That Makes A Difference With Counterfactually-Augmented Data
        </a>


        <a href="/pdf?id=Sklgs0NFvr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=dkaushik%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dkaushik@cs.cmu.edu">Divyansh Kaushik</a>, <a href="/profile?email=hovy%40cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="hovy@cmu.edu">Eduard Hovy</a>, <a href="/profile?email=zlipton%40cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="zlipton@cmu.edu">Zachary Lipton</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#Sklgs0NFvr-details-364" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Sklgs0NFvr-details-364">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Humans in the loop revise documents to accord with counterfactual labels, resulting resource helps to reduce reliance on spurious associations.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">humans in the loop, annotation artifacts, text classification, sentiment analysis, natural language inference</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/dkaushik96/counterfactually-augmented-data</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Sklgs0NFvr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="r1genAVKPB" data-number="1344">
      <h4>
        <a href="/forum?id=r1genAVKPB">
          Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?
        </a>


        <a href="/pdf?id=r1genAVKPB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=ssdu%40ias.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ssdu@ias.edu">Simon S. Du</a>, <a href="/profile?email=sham%40cs.washington.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sham@cs.washington.edu">Sham M. Kakade</a>, <a href="/profile?email=ruosongw%40andrew.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ruosongw@andrew.cmu.edu">Ruosong Wang</a>, <a href="/profile?email=linyang%40ee.ucla.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="linyang@ee.ucla.edu">Lin F. Yang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#r1genAVKPB-details-463" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="r1genAVKPB-details-463">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, function approximation, lower bound, representation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Exponential lower bounds for value-based and policy-based reinforcement learning with function approximation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which \emph{permit} sample efficient reinforcement learning with little understanding of what are \emph{necessary} conditions for efficient reinforcement learning.
                This work shows that, from the statistical viewpoint, the situation is far subtler than suggested by the more traditional approximation viewpoint, where the requirements on the representation that suffice for sample efficient RL are even more stringent. Our main results&nbsp;provide sharp thresholds for reinforcement learning methods, showing that there are hard limitations on what constitutes good function approximation (in terms of the dimensionality of the representation), where we focus on natural representational conditions relevant to value-based, model-based, and policy-based learning. These lower bounds highlight that having a good (value-based, model-based, or policy-based) representation in and of itself is insufficient for efficient reinforcement learning, unless the quality of this approximation passes certain hard thresholds. Furthermore, our lower bounds also imply exponential separations on the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning. </span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=r1genAVKPB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="B1xm3RVtwB" data-number="1350">
      <h4>
        <a href="/forum?id=B1xm3RVtwB">
          Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning
        </a>


        <a href="/pdf?id=B1xm3RVtwB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=hengyuan%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hengyuan@fb.com">Hengyuan Hu</a>, <a href="/profile?email=jakobfoerster%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jakobfoerster@gmail.com">Jakob N Foerster</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>13 Replies</span>


      </div>

      <a href="#B1xm3RVtwB-details-46" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="B1xm3RVtwB-details-46">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://bit.ly/2mBJLyk</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multi-agent RL, theory of mind</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=B1xm3RVtwB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rkeu30EtvS" data-number="1363">
      <h4>
        <a href="/forum?id=rkeu30EtvS">
          Network Deconvolution
        </a>


        <a href="/pdf?id=rkeu30EtvS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=yechengxi%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yechengxi@gmail.com">Chengxi Ye</a>, <a href="/profile?email=mevanusa%40umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mevanusa@umd.edu">Matthew Evanusa</a>, <a href="/profile?email=huah%40umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="huah@umd.edu">Hua He</a>, <a href="/profile?email=amitrokh%40umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="amitrokh@umd.edu">Anton Mitrokhin</a>, <a href="/profile?email=tomg%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tomg@cs.umd.edu">Tom Goldstein</a>, <a href="/profile?email=yorke%40umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yorke@umd.edu">James A. Yorke</a>, <a href="/profile?email=fer%40umiacs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="fer@umiacs.umd.edu">Cornelia Fermuller</a>, <a href="/profile?email=yiannis%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yiannis@cs.umd.edu">Yiannis Aloimonos</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#rkeu30EtvS-details-317" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rkeu30EtvS-details-317">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a method called network deconvolution that resembles animal vision system to train convolution networks better.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Convolution is a central operation in Convolutional Neural Networks (CNNs), which applies a kernel to overlapping regions shifted across the image. However, because of the strong correlations in real-world image data, convolutional kernels are in effect re-learning redundant data. In this work, we show that this redundancy has made neural network training challenging, and propose network deconvolution, a procedure which optimally removes pixel-wise and channel-wise correlations before the data is fed into each layer. Network deconvolution can be efficiently calculated at a fraction of the computational cost of a convolution layer. We also show that the deconvolution filters in the first layer of the network resemble the center-surround structure found in biological neurons in the visual regions of the brain. Filtering with such kernels results in a sparse representation, a desired property that has been missing in the training of neural networks. Learning from the sparse representation promotes faster convergence and superior results without the use of batch normalization. We apply our network deconvolution operation to 10 modern neural network models by replacing batch normalization within each. Extensive experiments show that the network deconvolution operation is able to deliver performance improvement in all cases on the CIFAR-10, CIFAR-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/yechengxi/deconvolution</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">convolutional networks, network deconvolution, whitening</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rkeu30EtvS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="ryxjnREFwH" data-number="1369">
      <h4>
        <a href="/forum?id=ryxjnREFwH">
          Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension
        </a>


        <a href="/pdf?id=ryxjnREFwH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=xinyun.chen%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="xinyun.chen@berkeley.edu">Xinyun Chen</a>, <a href="/profile?email=crazydonkey%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="crazydonkey@google.com">Chen Liang</a>, <a href="/profile?email=adamsyuwei%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="adamsyuwei@google.com">Adams Wei Yu</a>, <a href="/profile?email=dennyzhou%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dennyzhou@google.com">Denny Zhou</a>, <a href="/profile?email=dawnsong.travel%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dawnsong.travel@gmail.com">Dawn Song</a>, <a href="/profile?email=qvl%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="qvl@google.com">Quoc V. Le</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>13 Replies</span>


      </div>

      <a href="#ryxjnREFwH-details-142" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="ryxjnREFwH-details-142">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Integrating distributed representations with symbolic operations is essential for reading comprehension requiring complex reasoning, such as counting, sorting and arithmetics, but most existing approaches are hard to scale to more domains or more complex reasoning. In this work, we propose the Neural Symbolic Reader (NeRd), which includes a reader, e.g., BERT, to encode the passage and question, and a programmer, e.g., LSTM, to generate a program that is executed to produce the answer. Compared to previous works, NeRd is more scalable in two aspects: (1) domain-agnostic, i.e., the same neural architecture works for different domains; (2) compositional, i.e., when needed, complex programs can be generated by recursively applying the predefined operators, which become executable and interpretable representations for more complex reasoning. Furthermore, to overcome the challenge of training NeRd with weak supervision, we apply data augmentation techniques and hard Expectation-Maximization (EM) with thresholding. On DROP, a challenging reading comprehension dataset that requires discrete reasoning, NeRd achieves 1.37%/1.18% absolute improvement over the state-of-the-art on EM/F1 metrics. With the same architecture, NeRd significantly outperforms the baselines on MathQA, a math problem benchmark that requires multiple steps of reasoning, by 25.5% absolute increment on accuracy when trained on all the annotated programs. More importantly, NeRd still beats the baselines even when only 20% of the program annotations are given.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural symbolic, reading comprehension, question answering</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=ryxjnREFwH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="B1lPaCNtPB" data-number="1397">
      <h4>
        <a href="/forum?id=B1lPaCNtPB">
          Real or Not Real, that is the Question
        </a>


        <a href="/pdf?id=B1lPaCNtPB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=xy019%40ie.cuhk.edu.hk" class="profile-link" data-toggle="tooltip" data-placement="top" title="xy019@ie.cuhk.edu.hk">Yuanbo Xiangli*</a>, <a href="/profile?email=danny.s.deng.ds%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="danny.s.deng.ds@gmail.com">Yubin Deng*</a>, <a href="/profile?email=doubledaibo%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="doubledaibo@gmail.com">Bo Dai*</a>, <a href="/profile?email=ccloy%40ntu.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="top" title="ccloy@ntu.edu.sg">Chen Change Loy</a>, <a href="/profile?email=dhlin%40ie.cuhk.edu.hk" class="profile-link" data-toggle="tooltip" data-placement="top" title="dhlin@ie.cuhk.edu.hk">Dahua Lin</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>13 Replies</span>


      </div>

      <a href="#B1lPaCNtPB-details-719" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="B1lPaCNtPB-details-719">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">GAN, generalization, realness, loss function</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While generative adversarial networks (GAN) have been widely adopted in various topics, in this paper we generalize the standard GAN to a new perspective by treating realness as a random variable that can be estimated from multiple angles. In this generalized framework, referred to as RealnessGAN, the discriminator outputs a distribution as the measure of realness. While RealnessGAN shares similar theoretical guarantees with the standard GAN, it provides more insights on adversarial learning. More importantly, compared to multiple baselines, RealnessGAN provides stronger guidance for the generator, achieving improvements on both synthetic and real-world datasets. Moreover, it enables the basic DCGAN architecture to generate realistic images at 1024*1024 resolution when trained from scratch.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/kam1107/RealnessGAN</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=B1lPaCNtPB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1lOTC4tDS" data-number="1400">
      <h4>
        <a href="/forum?id=S1lOTC4tDS">
          Dream to Control: Learning Behaviors by Latent Imagination
        </a>


        <a href="/pdf?id=S1lOTC4tDS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=mail%40danijar.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mail@danijar.com">Danijar Hafner</a>, <a href="/profile?email=countzero%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="countzero@google.com">Timothy Lillicrap</a>, <a href="/profile?email=jba%40cs.toronto.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jba@cs.toronto.edu">Jimmy Ba</a>, <a href="/profile?email=mnorouzi%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mnorouzi@google.com">Mohammad Norouzi</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>15 Replies</span>


      </div>

      <a href="#S1lOTC4tDS-details-990" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1lOTC4tDS-details-990">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present Dreamer, an agent that learns long-horizon behaviors purely by latent imagination using analytic value gradients.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">world model, latent dynamics, imagination, planning by backprop, policy optimization, planning, reinforcement learning, control, representations, latent variable model, visual control, value function</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://danijar.com/dreamer</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1lOTC4tDS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HJlA0C4tPS" data-number="1451">
      <h4>
        <a href="/forum?id=HJlA0C4tPS">
          A Probabilistic Formulation of Unsupervised Text Style Transfer
        </a>


        <a href="/pdf?id=HJlA0C4tPS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=junxianh%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="junxianh@cs.cmu.edu">Junxian He</a>, <a href="/profile?email=xinyiw1%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="xinyiw1@cs.cmu.edu">Xinyi Wang</a>, <a href="/profile?email=gneubig%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="gneubig@cs.cmu.edu">Graham Neubig</a>, <a href="/profile?email=tberg%40eng.ucsd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tberg@eng.ucsd.edu">Taylor Berg-Kirkpatrick</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#HJlA0C4tPS-details-244" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HJlA0C4tPS-details-244">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We formulate a probabilistic latent sequence model to tackle unsupervised text style transfer, and show its effectiveness across a suite of unsupervised text style transfer tasks. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (e.g. the HMM), our model makes few assumptions about the data it generates: it uses a recurrent language model as a prior and an encoder-decoder as a transduction distribution. While computation of marginal data likelihood is intractable in this model class, we show that amortized variational inference admits a practical surrogate. Further, by drawing connections between our variational objective and other recent unsupervised style transfer and machine translation techniques, we show how our probabilistic view can unify some known non-generative objectives such as backtranslation and adversarial loss. Finally, we demonstrate the effectiveness of our method on a wide range of unsupervised style transfer tasks, including sentiment transfer, formality transfer, word decipherment, author imitation, and related language translation. Across all style transfer tasks, our approach yields substantial gains over state-of-the-art non-generative baselines, including the state-of-the-art unsupervised machine translation techniques that our approach generalizes. Further, we conduct experiments on a standard unsupervised machine translation task and find that our unified approach matches the current state-of-the-art.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">unsupervised text style transfer, deep latent sequence model</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HJlA0C4tPS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SkxpxJBKwS" data-number="1523">
      <h4>
        <a href="/forum?id=SkxpxJBKwS">
          Emergent Tool Use From Multi-Agent Autocurricula
        </a>


        <a href="/pdf?id=SkxpxJBKwS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=bowen%40openai.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="bowen@openai.com">Bowen Baker</a>, <a href="/profile?email=ingmar%40openai.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ingmar@openai.com">Ingmar Kanitscheider</a>, <a href="/profile?email=todor%40openai.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="todor@openai.com">Todor Markov</a>, <a href="/profile?email=jxwuyi%40openai.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jxwuyi@openai.com">Yi Wu</a>, <a href="/profile?email=glenn%40openai.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="glenn@openai.com">Glenn Powell</a>, <a href="/profile?email=bmcgrew%40openai.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="bmcgrew@openai.com">Bob McGrew</a>, <a href="/profile?email=imordatch%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="imordatch@google.com">Igor Mordatch</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>13 Replies</span>


      </div>

      <a href="#SkxpxJBKwS-details-640" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SkxpxJBKwS-details-640">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/openai/multi-agent-emergence-environments</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SkxpxJBKwS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HJxyZkBKDr" data-number="1527">
      <h4>
        <a href="/forum?id=HJxyZkBKDr">
          NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search
        </a>


        <a href="/pdf?id=HJxyZkBKDr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=xuanyi.dxy%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="xuanyi.dxy@gmail.com">Xuanyi Dong</a>, <a href="/profile?email=yi.yang%40uts.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="yi.yang@uts.edu.au">Yi Yang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>22 Replies</span>


      </div>

      <a href="#HJxyZkBKDr-details-22" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HJxyZkBKDr-details-22">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A NAS benchmark applicable to almost any NAS algorithms.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural architecture search (NAS) has achieved breakthrough success in a great number of applications in the past few years.
                It could be time to take a step back and analyze the good and bad aspects in the field of NAS. A variety of algorithms search architectures under different search space. These searched architectures are trained using different setups, e.g., hyper-parameters, data augmentation, regularization. This raises a comparability problem when comparing the performance of various NAS algorithms. NAS-Bench-101 has shown success to alleviate this problem. In this work, we propose an extension to NAS-Bench-101: NAS-Bench-201 with a different search space, results on multiple datasets, and more diagnostic information. NAS-Bench-201 has a fixed search space and provides a unified benchmark for almost any up-to-date NAS algorithms. The design of our search space is inspired by the one used in the most popular cell-based searching algorithms, where a cell is represented as a directed acyclic graph. Each edge here is associated with an operation selected from a predefined operation set. For it to be applicable for all NAS algorithms, the search space defined in NAS-Bench-201 includes all possible architectures generated by 4 nodes and 5 associated operation options, which results in 15,625 neural cell candidates in total. The training log using the same setup and the performance for each architecture candidate are provided for three datasets. This allows researchers to avoid unnecessary repetitive training for selected architecture and focus solely on the search algorithm itself. The training time saved for every architecture also largely improves the efficiency of most NAS algorithms and presents a more computational cost friendly NAS community for a broader range of researchers. We provide additional diagnostic information such as fine-grained loss and accuracy, which can give inspirations to new designs of NAS algorithms. In further support of the proposed NAS-Bench-102, we have analyzed it from many aspects and benchmarked 10 recent NAS algorithms, which verify its applicability.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Architecture Search, AutoML, Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/D-X-Y/NAS-Bench-201</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HJxyZkBKDr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HJlWWJSFDH" data-number="1533">
      <h4>
        <a href="/forum?id=HJlWWJSFDH">
          Strategies for Pre-training Graph Neural Networks
        </a>


        <a href="/pdf?id=HJlWWJSFDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=weihuahu%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="weihuahu@stanford.edu">Weihua Hu*</a>, <a href="/profile?email=liubowen%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="liubowen@stanford.edu">Bowen Liu*</a>, <a href="/profile?email=joegomes%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="joegomes@stanford.edu">Joseph Gomes</a>, <a href="/profile?email=marinka%40cs.stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="marinka@cs.stanford.edu">Marinka Zitnik</a>, <a href="/profile?email=pliang%40cs.stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="pliang@cs.stanford.edu">Percy Liang</a>, <a href="/profile?email=pande%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="pande@stanford.edu">Vijay Pande</a>, <a href="/profile?email=jure%40cs.stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jure@cs.stanford.edu">Jure Leskovec</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>11 Replies</span>


      </div>

      <a href="#HJlWWJSFDH-details-781" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HJlWWJSFDH-details-781">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naïve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Pre-training, Transfer learning, Graph Neural Networks</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/snap-stanford/pretrain-gnns/</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HJlWWJSFDH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rygf-kSYwH" data-number="1534">
      <h4>
        <a href="/forum?id=rygf-kSYwH">
          Behaviour Suite for Reinforcement Learning
        </a>


        <a href="/pdf?id=rygf-kSYwH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=ian.osband%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ian.osband@gmail.com">Ian Osband</a>, <a href="/profile?email=ydoron%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ydoron@google.com">Yotam Doron</a>, <a href="/profile?email=mtthss%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mtthss@google.com">Matteo Hessel</a>, <a href="/profile?email=jaslanides%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jaslanides@google.com">John Aslanides</a>, <a href="/profile?email=esezener%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="esezener@google.com">Eren Sezener</a>, <a href="/profile?email=andresnds%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="andresnds@google.com">Andre Saraiva</a>, <a href="/profile?email=mckinneyk%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mckinneyk@google.com">Katrina McKinney</a>, <a href="/profile?email=lattimore%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lattimore@google.com">Tor Lattimore</a>, <a href="/profile?email=szepi%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="szepi@google.com">Csaba Szepesvari</a>, <a href="/profile?email=baveja%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="baveja@google.com">Satinder Singh</a>, <a href="/profile?email=benvanroy%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="benvanroy@google.com">Benjamin Van Roy</a>, <a href="/profile?email=suttonr%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="suttonr@google.com">Richard Sutton</a>, <a href="/profile?email=davidsilver%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="davidsilver@google.com">David Silver</a>, <a href="/profile?email=hado%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hado@google.com">Hado Van Hasselt</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>15 Replies</span>


      </div>

      <a href="#rygf-kSYwH-details-431" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rygf-kSYwH-details-431">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, benchmark, core issues, scalability, reproducibility</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Bsuite is a collection of carefully-designed experiments that investigate the core capabilities of RL agents.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source this http URL, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/deepmind/bsuite</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rygf-kSYwH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BygzbyHFvB" data-number="1535">
      <h4>
        <a href="/forum?id=BygzbyHFvB">
          FreeLB: Enhanced Adversarial Training for Natural Language Understanding
        </a>


        <a href="/pdf?id=BygzbyHFvB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=chenzhu%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="chenzhu@cs.umd.edu">Chen Zhu</a>, <a href="/profile?email=yu.cheng%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yu.cheng@microsoft.com">Yu Cheng</a>, <a href="/profile?email=zhe.gan%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhe.gan@microsoft.com">Zhe Gan</a>, <a href="/profile?email=siqi.sun%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="siqi.sun@microsoft.com">Siqi Sun</a>, <a href="/profile?email=tomg%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tomg@cs.umd.edu">Tom Goldstein</a>, <a href="/profile?email=jingjl%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jingjl@microsoft.com">Jingjing Liu</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>6 Replies</span>


      </div>

      <a href="#BygzbyHFvB-details-483" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BygzbyHFvB-details-483">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44% and 67.75% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/zhuchen03/FreeLB</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BygzbyHFvB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Hklz71rYvS" data-number="1609">
      <h4>
        <a href="/forum?id=Hklz71rYvS">
          Kernelized Wasserstein Natural Gradient
        </a>


        <a href="/pdf?id=Hklz71rYvS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=michael.n.arbel%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="michael.n.arbel@gmail.com">M Arbel</a>, <a href="/profile?email=arthur.gretton%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="arthur.gretton@gmail.com">A Gretton</a>, <a href="/profile?email=wcli%40math.ucla.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="wcli@math.ucla.edu">W Li</a>, <a href="/profile?email=guidomontufar%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="guidomontufar@gmail.com">G Montufar</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>14 Replies</span>


      </div>

      <a href="#Hklz71rYvS-details-156" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Hklz71rYvS-details-156">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Estimator for the Wasserstein natural gradient</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many machine learning problems can be expressed as the optimization of some cost functional over a parametric family of probability distributions. It is often beneficial to solve such optimization problems using natural gradient methods. These methods are invariant to the parametrization of the family, and thus can yield more effective optimization. Unfortunately, computing the natural gradient is challenging as it requires inverting a high dimensional matrix at each iteration. We propose a general framework to approximate the natural gradient for the Wasserstein metric, by leveraging a dual formulation of the metric restricted to a Reproducing Kernel Hilbert Space. Our approach leads to an estimator for gradient direction that can trade-off accuracy and computational cost, with theoretical guarantees. We verify its accuracy on simple examples, and show the advantage of using such an estimator in classification tasks on \texttt{Cifar10} and \texttt{Cifar100} empirically. </span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">kernel methods, natural gradient, information geometry, Wasserstein metric</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Hklz71rYvS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rJehVyrKwH" data-number="1668">
      <h4>
        <a href="/forum?id=rJehVyrKwH">
          And the Bit Goes Down: Revisiting the Quantization of Neural Networks
        </a>


        <a href="/pdf?id=rJehVyrKwH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=pstock%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="pstock@fb.com">Pierre Stock</a>, <a href="/profile?email=ajoulin%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ajoulin@fb.com">Armand Joulin</a>, <a href="/profile?email=remi.gribonval%40inria.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="remi.gribonval@inria.fr">Rémi Gribonval</a>, <a href="/profile?email=benjamingraham%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="benjamingraham@fb.com">Benjamin Graham</a>, <a href="/profile?email=rvj%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="rvj@fb.com">Hervé Jégou</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>17 Replies</span>


      </div>

      <a href="#rJehVyrKwH-details-107" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rJehVyrKwH-details-107">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">compression, quantization</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Using a structured quantization technique aiming at better in-domain reconstruction to compress convolutional neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we address the problem of reducing the memory footprint of convolutional network architectures. We introduce a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The principle of our approach is that it minimizes the loss reconstruction error for in-domain inputs. Our method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using byte-aligned codebooks to store the compressed weights. We validate our approach by quantizing a high performing ResNet-50 model to a memory size of 5MB (20x compression factor) while preserving a top-1 accuracy of 76.1% on ImageNet object classification and by compressing a Mask R-CNN with a 26x factor.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://drive.google.com/file/d/12QK7onizf2ArpEBK706ly8bNfiM9cPzp/view?usp=sharing</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rJehVyrKwH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BJxSI1SKDH" data-number="1727">
      <h4>
        <a href="/forum?id=BJxSI1SKDH">
          A Latent Morphology Model for Open-Vocabulary Neural Machine Translation
        </a>


        <a href="/pdf?id=BJxSI1SKDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=duyguataman%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="duyguataman@gmail.com">Duygu Ataman</a>, <a href="/profile?email=will.aziz%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="will.aziz@gmail.com">Wilker Aziz</a>, <a href="/profile?email=a.birch%40ed.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="a.birch@ed.ac.uk">Alexandra Birch</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>5 Replies</span>


      </div>

      <a href="#BJxSI1SKDH-details-64" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BJxSI1SKDH-details-64">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">neural machine translation, low-resource languages, latent-variable models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Translation into morphologically-rich languages challenges neural machine translation (NMT) models with extremely sparse vocabularies where atomic treatment of surface forms is unrealistic. This problem is typically addressed by either pre-processing words into subword units or performing translation directly at the level of characters. The former is based on word segmentation algorithms optimized using corpus-level statistics with no regard to the translation task. The latter learns directly from translation data but requires rather deep architectures. In this paper, we propose to translate words by modeling word formation through a hierarchical latent variable model which mimics the process of morphological inflection. Our model generates words one character at a time by composing two latent representations: a continuous one, aimed at capturing the lexical semantics, and a set of (approximately) discrete features, aimed at capturing the morphosyntactic function, which are shared among different surface forms. Our model achieves better accuracy in translation into three morphologically-rich languages than conventional open-vocabulary NMT methods, while also demonstrating a better generalization capacity under low to mid-resource settings.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BJxSI1SKDH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HyevIJStwH" data-number="1732">
      <h4>
        <a href="/forum?id=HyevIJStwH">
          Understanding Why Neural Networks Generalize Well Through GSNR of Parameters
        </a>


        <a href="/pdf?id=HyevIJStwH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=ljlwykqh%40126.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ljlwykqh@126.com">Jinlong Liu</a>, <a href="/profile?email=yunzhi.bai%40outlook.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="yunzhi.bai@outlook.fr">Yunzhi Bai</a>, <a href="/profile?email=jianggq%40pku.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="jianggq@pku.edu.cn">Guoqing Jiang</a>, <a href="/profile?email=roushi0322%40sina.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="roushi0322@sina.cn">Ting Chen</a>, <a href="/profile?email=wanghuayan%40kuaishou.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wanghuayan@kuaishou.com">Huayan Wang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#HyevIJStwH-details-376" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HyevIJStwH-details-376">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs’ remarkable generalization ability.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">DNN, generalization, GSNR, gradient descent</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HyevIJStwH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1xCPJHtDB" data-number="1787">
      <h4>
        <a href="/forum?id=S1xCPJHtDB">
          Model Based Reinforcement Learning for Atari
        </a>


        <a href="/pdf?id=S1xCPJHtDB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=lukaszkaiser%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lukaszkaiser@google.com">Łukasz Kaiser</a>, <a href="/profile?email=mbz%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mbz@google.com">Mohammad Babaeizadeh</a>, <a href="/profile?email=pmilos%40mimuw.edu.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="pmilos@mimuw.edu.pl">Piotr Miłos</a>, <a href="/profile?email=blazej.osinski%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="blazej.osinski@gmail.com">Błażej Osiński</a>, <a href="/profile?email=rhc%40illinois.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="rhc@illinois.edu">Roy H Campbell</a>, <a href="/profile?email=konrad.czechowski%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="konrad.czechowski@gmail.com">Konrad Czechowski</a>, <a href="/profile?email=dumitru%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dumitru@google.com">Dumitru Erhan</a>, <a href="/profile?email=chelseaf%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="chelseaf@google.com">Chelsea Finn</a>, <a href="/profile?email=kozak000%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kozak000@gmail.com">Piotr Kozakowski</a>, <a href="/profile?email=slevine%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="slevine@google.com">Sergey Levine</a>, <a href="/profile?email=afrozm%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="afrozm@google.com">Afroz Mohiuddin</a>, <a href="/profile?email=rsepassi%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="rsepassi@google.com">Ryan Sepassi</a>, <a href="/profile?email=gjt%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gjt@google.com">George Tucker</a>, <a href="/profile?email=henrykmichalewski%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="henrykmichalewski@gmail.com">Henryk Michalewski</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#S1xCPJHtDB-details-162" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1xCPJHtDB-details-162">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We use video prediction models, a model-based reinforcement learning algorithm and 2h of gameplay per game to train agents for 26 Atari games.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">http://bit.ly/2wjgn1a</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, model based rl, video prediction model, atari</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1xCPJHtDB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rkgbYyHtwB" data-number="1831">
      <h4>
        <a href="/forum?id=rkgbYyHtwB">
          Disagreement-Regularized Imitation Learning
        </a>


        <a href="/pdf?id=rkgbYyHtwB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=kdbrant%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kdbrant@cs.umd.edu">Kiante Brantley</a>, <a href="/profile?email=wen.sun%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wen.sun@microsoft.com">Wen Sun</a>, <a href="/profile?email=mihenaff%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mihenaff@microsoft.com">Mikael Henaff</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>12 Replies</span>


      </div>

      <a href="#rkgbYyHtwB-details-613" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rkgbYyHtwB-details-613">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">imitation learning, reinforcement learning, uncertainty</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Method for addressing covariate shift in imitation learning using ensemble uncertainty</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rkgbYyHtwB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="H1enKkrFDB" data-number="1855">
      <h4>
        <a href="/forum?id=H1enKkrFDB">
          Stable Rank Normalization for Improved Generalization in Neural Networks and GANs
        </a>


        <a href="/pdf?id=H1enKkrFDB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=amartya.sanyal%40cs.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="amartya.sanyal@cs.ox.ac.uk">Amartya Sanyal</a>, <a href="/profile?email=philip.torr%40eng.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="philip.torr@eng.ox.ac.uk">Philip H. Torr</a>, <a href="/profile?email=puneet%40robots.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="puneet@robots.ox.ac.uk">Puneet K. Dokania</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>16 Replies</span>


      </div>

      <a href="#H1enKkrFDB-details-401" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="H1enKkrFDB-details-401">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose Stable Rank Normalisation, a new regularisor based on recent generelization bounds and show how to optimize it with extensive experiments.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Exciting new work on generalization bounds for neural networks (NN) given by Bartlett et al. (2017); Neyshabur et al. (2018) closely depend on two parameter- dependant quantities: the Lipschitz constant upper bound and the stable rank (a softer version of rank). Even though these bounds typically have minimal practical utility, they facilitate questions on whether controlling such quantities together could improve the generalization behaviour of NNs in practice. To this end, we propose stable rank normalization (SRN), a novel, provably optimal, and computationally efficient weight-normalization scheme which minimizes the stable rank of a linear operator. Surprisingly we find that SRN, despite being non-convex, can be shown to have a unique optimal solution. We provide extensive analyses across a wide variety of NNs (DenseNet, WideResNet, ResNet, Alexnet, VGG), where applying SRN to their linear layers leads to improved classification accuracy, while simultaneously showing improvements in genealization, evaluated empirically using—(a) shattering experiments (Zhang et al., 2016); and (b) three measures of sample complexity by Bartlett et al. (2017), Neyshabur et al. (2018), &amp; Wei &amp; Ma. Additionally, we show that, when applied to the discriminator of GANs, it improves Inception, FID, and Neural divergence scores, while learning mappings with low empirical Lipschitz constant.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Generelization, regularization, empirical lipschitz</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=H1enKkrFDB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SJlpYJBKvH" data-number="1857">
      <h4>
        <a href="/forum?id=SJlpYJBKvH">
          Measuring the Reliability of Reinforcement Learning Algorithms
        </a>


        <a href="/pdf?id=SJlpYJBKvH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=scychan%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="scychan@google.com">Stephanie C.Y. Chan</a>, <a href="/profile?email=sfishman%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sfishman@google.com">Samuel Fishman</a>, <a href="/profile?email=kbanoop%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kbanoop@google.com">Anoop Korattikara</a>, <a href="/profile?email=canny%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="canny@google.com">John Canny</a>, <a href="/profile?email=sguada%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sguada@google.com">Sergio Guadarrama</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#SJlpYJBKvH-details-303" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SJlpYJBKvH-details-303">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A novel set of metrics for measuring reliability of reinforcement learning algorithms (+ accompanying statistical tests)</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Lack of reliability is a well-known issue for reinforcement learning (RL) algorithms. This problem has gained increasing attention in recent years, and efforts to improve it have grown substantially. To aid RL researchers and production users with the evaluation and improvement of reliability, we propose a set of metrics that quantitatively measure different aspects of reliability. In this work, we focus on variability and risk, both during training and after learning (on a fixed policy). We designed these metrics to be general-purpose, and we also designed complementary statistical tests to enable rigorous comparisons on these metrics. In this paper, we first describe the desired properties of the metrics and their design, the aspects of reliability that they measure, and their applicability to different scenarios. We then describe the statistical tests and make additional practical recommendations for reporting results. The metrics and accompanying statistical tools have been made available as an open-source library. We apply our metrics to a set of common RL algorithms and environments, compare them, and analyze the results.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, metrics, statistics, reliability</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/google-research/rl-reliability-metrics</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SJlpYJBKvH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Hke0K1HKwr" data-number="1859">
      <h4>
        <a href="/forum?id=Hke0K1HKwr">
          Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue
        </a>


        <a href="/pdf?id=Hke0K1HKwr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=byeongchang.kim%40vision.snu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="byeongchang.kim@vision.snu.ac.kr">Byeongchang Kim</a>, <a href="/profile?email=jaewoo.ahn%40vision.snu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="jaewoo.ahn@vision.snu.ac.kr">Jaewoo Ahn</a>, <a href="/profile?email=gunhee%40snu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="gunhee@snu.ac.kr">Gunhee Kim</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#Hke0K1HKwr-details-63" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Hke0K1HKwr-details-63">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dialogue, knowledge, language, conversation</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/bckim92/sequential-knowledge-transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Hke0K1HKwr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SklD9yrFPS" data-number="1882">
      <h4>
        <a href="/forum?id=SklD9yrFPS">
          Neural Tangents: Fast and Easy Infinite Neural Networks in Python
        </a>


        <a href="/pdf?id=SklD9yrFPS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=romann%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="romann@google.com">Roman Novak</a>, <a href="/profile?email=xlc%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="xlc@google.com">Lechao Xiao</a>, <a href="/profile?email=jh2084%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="jh2084@cam.ac.uk">Jiri Hron</a>, <a href="/profile?email=jaehlee%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jaehlee@google.com">Jaehoon Lee</a>, <a href="/profile?email=alemi%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="alemi@google.com">Alexander A. Alemi</a>, <a href="/profile?email=jaschasd%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jaschasd@google.com">Jascha Sohl-Dickstein</a>, <a href="/profile?email=schsam%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="schsam@google.com">Samuel S. Schoenholz</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>16 Replies</span>


      </div>

      <a href="#SklD9yrFPS-details-706" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SklD9yrFPS-details-706">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Keras for infinite neural networks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.

                The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices.

                In addition to the repository below, we provide an accompanying interactive Colab notebook at
                https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb
              </span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://www.github.com/google/neural-tangents</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Infinite Neural Networks, Gaussian Processes, Neural Tangent Kernel, NNGP, NTK, Software Library, Python, JAX</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SklD9yrFPS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Hyx-jyBFPr" data-number="1904">
      <h4>
        <a href="/forum?id=Hyx-jyBFPr">
          Self-labelling via simultaneous clustering and representation learning
        </a>


        <a href="/pdf?id=Hyx-jyBFPr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=yuki%40robots.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="yuki@robots.ox.ac.uk">Asano YM.</a>, <a href="/profile?email=chrisr%40robots.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="chrisr@robots.ox.ac.uk">Rupprecht C.</a>, <a href="/profile?email=vedaldi%40robots.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="vedaldi@robots.ox.ac.uk">Vedaldi A.</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>18 Replies</span>


      </div>

      <a href="#Hyx-jyBFPr-details-997" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Hyx-jyBFPr-details-997">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a self-supervised learning formulation that simultaneously learns feature representations and useful dataset labels by optimizing the common cross-entropy loss for features _and_ labels, while maximizing information.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions.
                In this paper, we propose a novel and principled learning formulation that addresses these issues.
                The method is obtained by maximizing the information between labels and input data indices.
                We show that this criterion extends standard cross-entropy minimization to an optimal transport problem, which we solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm.
                The resulting method is able to self-label visual data so as to train highly competitive image representations without manual labels. Our method achieves state of the art representation learning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and ImageNet and yields the first self-supervised AlexNet that outperforms the supervised Pascal VOC detection baseline. </span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">self-supervision, feature representation learning, clustering</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Hyx-jyBFPr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1e4jkSKvB" data-number="1911">
      <h4>
        <a href="/forum?id=S1e4jkSKvB">
          The intriguing role of module criticality in the generalization of deep networks
        </a>


        <a href="/pdf?id=S1e4jkSKvB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=niladri.chatterji%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="niladri.chatterji@berkeley.edu">Niladri Chatterji</a>, <a href="/profile?email=neyshabur%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="neyshabur@google.com">Behnam Neyshabur</a>, <a href="/profile?email=hsedghi%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hsedghi@google.com">Hanie Sedghi</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#S1e4jkSKvB-details-819" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1e4jkSKvB-details-819">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Module Criticality Phenomenon, Complexity Measure, Deep Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1e4jkSKvB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rkl8sJBYvH" data-number="1915">
      <h4>
        <a href="/forum?id=rkl8sJBYvH">
          Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks
        </a>


        <a href="/pdf?id=rkl8sJBYvH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=arora%40cs.princeton.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="arora@cs.princeton.edu">Sanjeev Arora</a>, <a href="/profile?email=ssdu%40ias.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ssdu@ias.edu">Simon S. Du</a>, <a href="/profile?email=zhiyuanli%40cs.princeton.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhiyuanli@cs.princeton.edu">Zhiyuan Li</a>, <a href="/profile?email=rsalakhu%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="rsalakhu@cs.cmu.edu">Ruslan Salakhutdinov</a>, <a href="/profile?email=ruosongw%40andrew.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ruosongw@andrew.cmu.edu">Ruosong Wang</a>, <a href="/profile?email=dingliy%40cs.princeton.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dingliy@cs.princeton.edu">Dingli Yu</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#rkl8sJBYvH-details-966" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rkl8sJBYvH-details-966">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks.
                1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets.
                2. On CIFAR-10 with 10 – 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%.
                3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance.
                4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK’s efficacy may trace to lower variance of output.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">small data, neural tangent kernel, UCI database, few-shot learning, kernel SVMs, deep learning theory, kernel design</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/LeoYu/neural-tangent-kernel-UCI; https://drive.google.com/open?id=1SdgWmhEcnm4qyaM9xrkN01VF9tj40WZS</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rkl8sJBYvH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BkevoJSYPB" data-number="1919">
      <h4>
        <a href="/forum?id=BkevoJSYPB">
          Differentiation of Blackbox Combinatorial Solvers
        </a>


        <a href="/pdf?id=BkevoJSYPB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=marin.vlastelica%40tue.mpg.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="marin.vlastelica@tue.mpg.de">Marin Vlastelica Pogančić</a>, <a href="/profile?email=anselm.paulus%40tuebingen.mpg.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="anselm.paulus@tuebingen.mpg.de">Anselm Paulus</a>, <a href="/profile?email=vejtek%40atrey.karlin.mff.cuni.cz" class="profile-link" data-toggle="tooltip" data-placement="top" title="vejtek@atrey.karlin.mff.cuni.cz">Vit Musil</a>, <a href="/profile?email=georg.martius%40tuebingen.mpg.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="georg.martius@tuebingen.mpg.de">Georg Martius</a>, <a href="/profile?email=michal.rolinek%40tuebingen.mpg.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="michal.rolinek@tuebingen.mpg.de">Michal Rolinek</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#BkevoJSYPB-details-842" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BkevoJSYPB-details-842">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value "> In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi MIP solver, Blossom V algorithm, and Dijkstra's algorithm into architectures that extract suitable features from raw inputs for the traveling salesman problem, the min-cost perfect matching problem and the shortest path problem.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://sites.google.com/view/combinatorialgradients/home</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">combinatorial algorithms, deep learning, representation learning, optimization</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BkevoJSYPB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rJgsskrFwH" data-number="1926">
      <h4>
        <a href="/forum?id=rJgsskrFwH">
          Scaling Autoregressive Video Models
        </a>


        <a href="/pdf?id=rJgsskrFwH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=diwe%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="diwe@google.com">Dirk Weissenborn</a>, <a href="/profile?email=oscar.tackstrom%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="oscar.tackstrom@gmail.com">Oscar Täckström</a>, <a href="/profile?email=usz%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="usz@google.com">Jakob Uszkoreit</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#rJgsskrFwH-details-370" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rJgsskrFwH-details-370">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">autoregressive models, video prediction, generative models, video generation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a novel autoregressive video generation that achieves strong results on popular datasets and produces encouraging continuations of real world videos.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Due to the statistical complexity of video, the high degree of inherent stochasticity, and the sheer amount of data, generating natural video remains a challenging task. State-of-the-art video generation models attempt to address these issues by combining sometimes complex, often video-specific neural network architectures, latent variable models, adversarial training and a range of other methods. Despite their often high complexity, these approaches still fall short of generating high quality video continuations outside of narrow domains and often struggle with fidelity. In contrast, we show that conceptually simple, autoregressive video generation models based on a three-dimensional self-attention mechanism achieve highly competitive results across multiple metrics on popular benchmark datasets for which they produce continuations of high fidelity and realism. Furthermore, we find that our models are capable of producing diverse and surprisingly realistic continuations on a subset of videos from Kinetics, a large scale action recognition dataset comprised of YouTube videos exhibiting phenomena such as camera movement, complex object interactions and diverse human movement. To our knowledge, this is the first promising application of video-generation models to videos of this complexity.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rJgsskrFwH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rJe2syrtvS" data-number="1930">
      <h4>
        <a href="/forum?id=rJe2syrtvS">
          The Ingredients of Real World Robotic Reinforcement Learning
        </a>


        <a href="/pdf?id=rJe2syrtvS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=henryzhu%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="henryzhu@berkeley.edu">Henry Zhu</a>, <a href="/profile?email=justinvyu%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="justinvyu@berkeley.edu">Justin Yu</a>, <a href="/profile?email=abhigupta%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="abhigupta@berkeley.edu">Abhishek Gupta</a>, <a href="/profile?email=shah%40eecs.berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="shah@eecs.berkeley.edu">Dhruv Shah</a>, <a href="/profile?email=kristian.hartikainen%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kristian.hartikainen@gmail.com">Kristian Hartikainen</a>, <a href="/profile?email=avisingh%40cs.berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="avisingh@cs.berkeley.edu">Avi Singh</a>, <a href="/profile?email=vikashplus%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="vikashplus@gmail.com">Vikash Kumar</a>, <a href="/profile?email=svlevine%40eecs.berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="svlevine@eecs.berkeley.edu">Sergey Levine</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 27 Apr 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#rJe2syrtvS-details-333" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rJe2syrtvS-details-333">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">System to learn robotic tasks in the real world with reinforcement learning without instrumentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, Robotics</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rJe2syrtvS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="ryeYpJSKwr" data-number="1996">
      <h4>
        <a href="/forum?id=ryeYpJSKwr">
          Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization
        </a>


        <a href="/pdf?id=ryeYpJSKwr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=mvolpp89%40googlemail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mvolpp89@googlemail.com">Michael Volpp</a>, <a href="/profile?email=lukas.froehlich%40de.bosch.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lukas.froehlich@de.bosch.com">Lukas P. Fröhlich</a>, <a href="/profile?email=k.fischer-lotte%40online.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="k.fischer-lotte@online.de">Kirsten Fischer</a>, <a href="/profile?email=andreas.doerr3%40de.bosch.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="andreas.doerr3@de.bosch.com">Andreas Doerr</a>, <a href="/profile?email=stefan.falkner%40de.bosch.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="stefan.falkner@de.bosch.com">Stefan Falkner</a>, <a href="/profile?email=fh%40cs.uni-freiburg.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="fh@cs.uni-freiburg.de">Frank Hutter</a>, <a href="/profile?email=christian.daniel%40de.bosch.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="christian.daniel@de.bosch.com">Christian Daniel</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#ryeYpJSKwr-details-16" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="ryeYpJSKwr-details-16">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We perform efficient and flexible transfer learning in the framework of Bayesian optimization through meta-learned neural acquisition functions.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present experiments on a simulation-to-real transfer task as well as on several synthetic functions and on two hyperparameter search problems. The results show that our algorithm (1) automatically identifies structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general AFs if no particular structure is present.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/metabo-iclr2020/MetaBO</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Transfer Learning, Meta Learning, Bayesian Optimization, Reinforcement Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=ryeYpJSKwr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BJliakStvH" data-number="2000">
      <h4>
        <a href="/forum?id=BJliakStvH">
          Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning
        </a>


        <a href="/pdf?id=BJliakStvH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=dscobee%40eecs.berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dscobee@eecs.berkeley.edu">Dexter R.R. Scobee</a>, <a href="/profile?email=sastry%40eecs.berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sastry@eecs.berkeley.edu">S. Shankar Sastry</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#BJliakStvH-details-481" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BJliakStvH-details-481">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Our method infers constraints on task execution by leveraging the principle of maximum entropy to quantify how demonstrations differ from expected, un-constrained behavior.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While most approaches to the problem of Inverse Reinforcement Learning (IRL) focus on estimating a reward function that best explains an expert agent’s policy or demonstrated behavior on a control task, it is often the case that such behavior is more succinctly represented by a simple reward combined with a set of hard constraints. In this setting, the agent is attempting to maximize cumulative rewards subject to these given constraints on their behavior. We reformulate the problem of IRL on Markov Decision Processes (MDPs) such that, given a nominal model of the environment and a nominal reward function, we seek to estimate state, action, and feature constraints in the environment that motivate an agent’s behavior. Our approach is based on the Maximum Entropy IRL framework, which allows us to reason about the likelihood of an expert agent’s demonstrations given our knowledge of an MDP. Using our method, we can infer which constraints can be added to the MDP to most increase the likelihood of observing these demonstrations. We present an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and we evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://drive.google.com/drive/folders/1h2J7o4w4J0_dpldTRpFu_jWQR8CkBbXw</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">learning from demonstration, inverse reinforcement learning, constraint inference</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BJliakStvH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="H1l_0JBYwS" data-number="2031">
      <h4>
        <a href="/forum?id=H1l_0JBYwS">
          Spectral Embedding of Regularized Block Models
        </a>


        <a href="/pdf?id=H1l_0JBYwS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=ndelara%40enst.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="ndelara@enst.fr">Nathan De Lara</a>, <a href="/profile?email=bonald%40enst.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="bonald@enst.fr">Thomas Bonald</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>4 Replies</span>


      </div>

      <a href="#H1l_0JBYwS-details-783" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="H1l_0JBYwS-details-783">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Graph regularization forces spectral embedding to focus on the largest clusters, making the representation less sensitive to noise. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both on both synthetic and real data, showing how regularization improves standard clustering scores. </span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/nathandelara/Spectral-Embedding-of-Regularized-Block-Models/</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Spectral embedding, regularization, block models, clustering</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=H1l_0JBYwS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BkxRRkSKwr" data-number="2043">
      <h4>
        <a href="/forum?id=BkxRRkSKwr">
          Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models
        </a>


        <a href="/pdf?id=BkxRRkSKwr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=xisenjin%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="xisenjin@usc.edu">Xisen Jin</a>, <a href="/profile?email=zywei%40fudan.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="zywei@fudan.edu.cn">Zhongyu Wei</a>, <a href="/profile?email=junyidu%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="junyidu@usc.edu">Junyi Du</a>, <a href="/profile?email=xyxue%40fudan.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="xyxue@fudan.edu.cn">Xiangyang Xue</a>, <a href="/profile?email=xiangren%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="xiangren@usc.edu">Xiang Ren</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>11 Replies</span>


      </div>

      <a href="#BkxRRkSKwr-details-468" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BkxRRkSKwr-details-468">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose measurement of phrase importance and algorithms for hierarchical explanation of neural sequence model predictions</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The impressive performance of neural networks on natural language processing tasks attributes to their ability to model complicated word and phrase compositions. To explain how the model handles semantic compositions, we study hierarchical explanation of neural network predictions. We identify non-additivity and context independent importance attributions within hierarchies as two desirable properties for highlighting word and phrase compositions. We show some prior efforts on hierarchical explanations, e.g. contextual decomposition, do not satisfy the desired properties mathematically, leading to inconsistent explanation quality in different models. In this paper, we start by proposing a formal and general way to quantify the importance of each word and phrase. Following the formulation, we propose Sampling and Contextual Decomposition (SCD) algorithm and Sampling and Occlusion (SOC) algorithm. Human and metrics evaluation on both LSTM models and BERT Transformer models on multiple datasets show that our algorithms outperform prior hierarchical explanation algorithms. Our algorithms help to visualize semantic composition captured by models, extract classification rules and improve human trust of models.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">natural language processing, interpretability</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BkxRRkSKwr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HkxARkrFwB" data-number="2045">
      <h4>
        <a href="/forum?id=HkxARkrFwB">
          word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement
        </a>


        <a href="/pdf?id=HkxARkrFwB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=panahia%40vcu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="panahia@vcu.edu">Aliakbar Panahi</a>, <a href="/profile?email=saeedis%40vcu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="saeedis@vcu.edu">Seyran Saeedi</a>, <a href="/profile?email=tarodz%40vcu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tarodz@vcu.edu">Tom Arodz</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 25 Apr 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#HkxARkrFwB-details-924" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HkxARkrFwB-details-924">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We use ideas from quantum computing to propose word embeddings that utilize much fewer trainable parameters.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning natural language processing models often use vector word embeddings, such as word2vec or GloVe, to represent words. A discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a sequence of continuous vectors. Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. However, storing and accessing embedding vectors for all words in a dictionary requires large amount of space, and may stain systems with limited GPU memory. Here, we used approaches inspired by quantum computing to propose two related methods, word2ket and word2ketXS, for storing word embedding matrix during training and inference in a highly efficient way. Our approach achieves a hundred-fold or more reduction in the space required to store the embeddings with almost no relative drop in accuracy in practical natural language processing tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">word embeddings, natural language processing, model reduction</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/panaali/word2ket</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HkxARkrFwB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rJxbJeHFPS" data-number="2051">
      <h4>
        <a href="/forum?id=rJxbJeHFPS">
          What Can Neural Networks Reason About?
        </a>


        <a href="/pdf?id=rJxbJeHFPS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=keyulu%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="keyulu@mit.edu">Keyulu Xu</a>, <a href="/profile?email=jingling%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jingling@cs.umd.edu">Jingling Li</a>, <a href="/profile?email=mozhi%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mozhi@cs.umd.edu">Mozhi Zhang</a>, <a href="/profile?email=ssdu%40ias.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ssdu@ias.edu">Simon S. Du</a>, <a href="/profile?email=k_keniti%40nii.ac.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="k_keniti@nii.ac.jp">Ken-ichi Kawarabayashi</a>, <a href="/profile?email=stefje%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="stefje@mit.edu">Stefanie Jegelka</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 28 Apr 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>16 Replies</span>


      </div>

      <a href="#rJxbJeHFPS-details-727" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rJxbJeHFPS-details-727">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We develop a theoretical framework to characterize what a neural network can learn to reason about.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reasoning, deep learning theory, algorithmic alignment, graph neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/NNReasoning/What-Can-Neural-Networks-Reason-About </span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rJxbJeHFPS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="B1gdkxHFDH" data-number="2068">
      <h4>
        <a href="/forum?id=B1gdkxHFDH">
          Training individually fair ML models with sensitive subspace robustness
        </a>


        <a href="/pdf?id=B1gdkxHFDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=mikhail.yurochkin%40ibm.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mikhail.yurochkin@ibm.com">Mikhail Yurochkin</a>, <a href="/profile?email=amandarg%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="amandarg@umich.edu">Amanda Bower</a>, <a href="/profile?email=yuekai%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yuekai@umich.edu">Yuekai Sun</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#B1gdkxHFDH-details-569" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="B1gdkxHFDH-details-569">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fairness, adversarial robustness</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Algorithm for training individually fair classifier using adversarial robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases. </span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/IBM/sensitive-subspace-robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=B1gdkxHFDH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SkeuexBtDr" data-number="2105">
      <h4>
        <a href="/forum?id=SkeuexBtDr">
          Learning from Rules Generalizing Labeled Exemplars
        </a>


        <a href="/pdf?id=SkeuexBtDr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=awasthi%40cse.iitb.ac.in" class="profile-link" data-toggle="tooltip" data-placement="top" title="awasthi@cse.iitb.ac.in">Abhijeet Awasthi</a>, <a href="/profile?email=sghosh%40cse.iitb.ac.in" class="profile-link" data-toggle="tooltip" data-placement="top" title="sghosh@cse.iitb.ac.in">Sabyasachi Ghosh</a>, <a href="/profile?email=rasna.goyal66%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="rasna.goyal66@gmail.com">Rasna Goyal</a>, <a href="/profile?email=sunita%40iitb.ac.in" class="profile-link" data-toggle="tooltip" data-placement="top" title="sunita@iitb.ac.in">Sunita Sarawagi</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 16 May 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#SkeuexBtDr-details-166" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SkeuexBtDr-details-166">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Learning from Rules, Learning from limited labeled data, Weakly Supervised Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/awasthiabhijeet/Learning-From-Rules</span>
            </li>
            <li>
              <strong class="note-content-field">Spotlight Video:</strong>
              <span class="note-content-value ">https://youtu.be/TQfq4YdqG3k</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SkeuexBtDr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="B1eWbxStPH" data-number="2126">
      <h4>
        <a href="/forum?id=B1eWbxStPH">
          Directional Message Passing for Molecular Graphs
        </a>


        <a href="/pdf?id=B1eWbxStPH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=klicpera%40in.tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="klicpera@in.tum.de">Johannes Klicpera</a>, <a href="/profile?email=grossja%40in.tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="grossja@in.tum.de">Janek Groß</a>, <a href="/profile?email=guennemann%40in.tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="guennemann@in.tum.de">Stephan Günnemann</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>8 Replies</span>


      </div>

      <a href="#B1eWbxStPH-details-579" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="B1eWbxStPH-details-579">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Directional message passing incorporates spatial directional information to improve graph neural networks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1/4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76% on MD17 and by 31% on QM9. Our implementation is available online.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://www.daml.in.tum.de/dimenet</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">GNN, Graph neural network, message passing, graphs, equivariance, molecules</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=B1eWbxStPH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="H1xFWgrFPS" data-number="2144">
      <h4>
        <a href="/forum?id=H1xFWgrFPS">
          Explanation by Progressive Exaggeration
        </a>


        <a href="/pdf?id=H1xFWgrFPS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=sumedha.singla%40pitt.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sumedha.singla@pitt.edu">Sumedha Singla</a>, <a href="/profile?email=kayhan%40pitt.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kayhan@pitt.edu">Brian Pollack</a>, <a href="/profile?email=cjx880409%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="cjx880409@gmail.com">Junxiang Chen</a>, <a href="/profile?email=kayhan%40pitt.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kayhan@pitt.edu">Kayhan Batmanghelich</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#H1xFWgrFPS-details-838" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="H1xFWgrFPS-details-838">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A method to explain a classifier, by generating visual perturbation of an image by exaggerating or diminishing the semantic features that the classifier associates with a target label.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As machine learning methods see greater adoption and implementation in high stakes applications such as medical image diagnosis, the need for model interpretability and explanation has become more critical. Classical approaches that assess feature importance (eg saliency maps) do not explain how and why a particular region of an image is relevant to the prediction. We propose a method that explains the outcome of a classification black-box by gradually exaggerating the semantic effect of a given class. Given a query input to a classifier, our method produces a progressive set of plausible variations of that query, which gradually change the posterior probability from its original class to its negation. These counter-factually generated samples preserve features unrelated to the classification decision, such that a user can employ our method as a ``tuning knob'' to traverse a data manifold while crossing the decision boundary. Our method is model agnostic and only requires the output value and gradient of the predictor with respect to its input.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Explain, deep learning, black box, GAN, counterfactual</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/batmanlab/Explanation_by_Progressive_Exaggeration.git</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=H1xFWgrFPS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="ByeGzlrKwH" data-number="2165">
      <h4>
        <a href="/forum?id=ByeGzlrKwH">
          Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network
        </a>


        <a href="/pdf?id=ByeGzlrKwH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=taiji%40mist.i.u-tokyo.ac.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="taiji@mist.i.u-tokyo.ac.jp">Taiji Suzuki</a>, <a href="/profile?email=abe%40ipride.co.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="abe@ipride.co.jp">Hiroshi Abe</a>, <a href="/profile?email=tomoaki.nishimura%40nttdata.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tomoaki.nishimura@nttdata.com">Tomoaki Nishimura</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>12 Replies</span>


      </div>

      <a href="#ByeGzlrKwH-details-264" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="ByeGzlrKwH-details-264">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size.
                The classical learning theory suggests that overparameterized models cause overfitting.
                However, practically used large deep models avoid overfitting, which is not well explained by the classical approaches.
                To resolve this issue, several attempts have been made.
                Among them, the compression based bound is one of the promising approaches.
                However, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network.
                In this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks.
                The bound gives even better rate than the one for the compressed network by improving the bias term.
                By establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.
              </span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Generalization error, compression based bound, local Rademacher complexity</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=ByeGzlrKwH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Bkeb7lHtvH" data-number="2199">
      <h4>
        <a href="/forum?id=Bkeb7lHtvH">
          At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?
        </a>


        <a href="/pdf?id=Bkeb7lHtvH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=giladiniv%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="giladiniv@gmail.com">Niv Giladi</a>, <a href="/profile?email=mor.shpigel%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mor.shpigel@gmail.com">Mor Shpigel Nacson</a>, <a href="/profile?email=elad.hoffer%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="elad.hoffer@gmail.com">Elad Hoffer</a>, <a href="/profile?email=daniel.soudry%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="daniel.soudry@gmail.com">Daniel Soudry</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#Bkeb7lHtvH-details-777" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Bkeb7lHtvH-details-777">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">implicit bias, stability, neural networks, generalization gap, asynchronous SGD</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">How to prevent stale gradients (in asynchronous SGD) from changing minima stability and degrade steady state generalization?</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods.
                Contributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability. We provide empirical experiments to validate our theoretical findings.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/paper-submissions/delay_stability</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Bkeb7lHtvH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rygeHgSFDH" data-number="2272">
      <h4>
        <a href="/forum?id=rygeHgSFDH">
          Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)
        </a>


        <a href="/pdf?id=rygeHgSFDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=peter.sorrenson%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="peter.sorrenson@gmail.com">Peter Sorrenson</a>, <a href="/profile?email=carsten.rother%40iwr.uni-heidelberg.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="carsten.rother@iwr.uni-heidelberg.de">Carsten Rother</a>, <a href="/profile?email=ullrich.koethe%40iwr.uni-heidelberg.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="ullrich.koethe@iwr.uni-heidelberg.de">Ullrich Köthe</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#rygeHgSFDH-details-213" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rygeHgSFDH-details-213">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">disentanglement, nonlinear ICA, representation learning, feature discovery, theoretical justification</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rygeHgSFDH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BkgrBgSYDS" data-number="2283">
      <h4>
        <a href="/forum?id=BkgrBgSYDS">
          Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps
        </a>


        <a href="/pdf?id=BkgrBgSYDS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=trid%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="trid@stanford.edu">Tri Dao</a>, <a href="/profile?email=nims%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="nims@stanford.edu">Nimit Sohoni</a>, <a href="/profile?email=albertgu%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="albertgu@stanford.edu">Albert Gu</a>, <a href="/profile?email=mae226%40cornell.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mae226@cornell.edu">Matthew Eichhorn</a>, <a href="/profile?email=amitblon%40buffalo.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="amitblon@buffalo.edu">Amit Blonder</a>, <a href="/profile?email=mleszczy%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mleszczy@stanford.edu">Megan Leszczynski</a>, <a href="/profile?email=atri%40buffalo.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="atri@buffalo.edu">Atri Rudra</a>, <a href="/profile?email=chrismre%40cs.stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="chrismre@cs.stanford.edu">Christopher Ré</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#BkgrBgSYDS-details-812" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BkgrBgSYDS-details-812">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a differentiable family of "kaleidoscope matrices," prove that all structured matrices can be represented in this form, and use them to replace hand-crafted linear maps in deep learning models.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in ShuffleNet improves classification accuracy on ImageNet by up to 5%. K-matrices can also simplify hand-engineered pipelines---we replace filter bank feature computation in speech data preprocessing with a learnable kaleidoscope layer, resulting in only 0.4% loss in accuracy on the TIMIT speech recognition task. In addition, K-matrices can capture latent structure in models: for a challenging permuted image classification task, adding a K-matrix to a standard convolutional architecture can enable learning the latent permutation and improve accuracy by over 8 points. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36% faster end-to-end inference speed on a language translation task.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/HazyResearch/learning-circuits</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">structured matrices, efficient ML, algorithms, butterfly matrices, arithmetic circuits</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BkgrBgSYDS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1evHerYPr" data-number="2289">
      <h4>
        <a href="/forum?id=S1evHerYPr">
          Improving Generalization in Meta Reinforcement Learning using Learned Objectives
        </a>


        <a href="/pdf?id=S1evHerYPr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=louis%40idsia.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="louis@idsia.ch">Louis Kirsch</a>, <a href="/profile?email=sjoerd%40idsia.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="sjoerd@idsia.ch">Sjoerd van Steenkiste</a>, <a href="/profile?email=juergen%40idsia.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="juergen@idsia.ch">Juergen Schmidhuber</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>12 Replies</span>


      </div>

      <a href="#S1evHerYPr-details-245" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1evHerYPr-details-245">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce MetaGenRL, a novel meta reinforcement learning algorithm. Unlike prior work, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Biological evolution has distilled the experiences of many learners into the general learning algorithms of humans. Our novel meta reinforcement learning algorithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective function that decides how future individuals will learn. Unlike recent meta-RL algorithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy second-order gradients during meta-training that greatly increase its sample efficiency.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">meta reinforcement learning, meta learning, reinforcement learning</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1evHerYPr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BJxsrgStvr" data-number="2297">
      <h4>
        <a href="/forum?id=BJxsrgStvr">
          Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks
        </a>


        <a href="/pdf?id=BJxsrgStvr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=hy34%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="hy34@rice.edu">Haoran You</a>, <a href="/profile?email=cl114%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="cl114@rice.edu">Chaojian Li</a>, <a href="/profile?email=px5%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="px5@rice.edu">Pengfei Xu</a>, <a href="/profile?email=yf22%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yf22@rice.edu">Yonggan Fu</a>, <a href="/profile?email=yw68%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yw68@rice.edu">Yue Wang</a>, <a href="/profile?email=chernxh%40tamu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="chernxh@tamu.edu">Xiaohan Chen</a>, <a href="/profile?email=richb%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="richb@rice.edu">Richard G. Baraniuk</a>, <a href="/profile?email=atlaswang%40tamu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="atlaswang@tamu.edu">Zhangyang Wang</a>, <a href="/profile?email=yingyan.lin%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yingyan.lin@rice.edu">Yingyan Lin</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 07 Aug 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#BJxsrgStvr-details-805" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BJxsrgStvr-details-805">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">(Frankle &amp; Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as Early-Bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 5.8x ~ 10.7x energy savings while maintaining comparable or even better accuracy as compared to the most competitive state-of-the-art training methods, demonstrating a promising and easily adopted method for tackling cost-prohibitive deep network training.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/RICE-EIC/Early-Bird-Tickets</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BJxsrgStvr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="HyxyIgHFvr" data-number="2307">
      <h4>
        <a href="/forum?id=HyxyIgHFvr">
          Truth or backpropaganda? An empirical investigation of deep learning theory
        </a>


        <a href="/pdf?id=HyxyIgHFvr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=goldblumcello%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="goldblumcello@gmail.com">Micah Goldblum</a>, <a href="/profile?email=jonas.geiping%40uni-siegen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="jonas.geiping@uni-siegen.de">Jonas Geiping</a>, <a href="/profile?email=avi1%40umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="avi1@umd.edu">Avi Schwarzschild</a>, <a href="/profile?email=michael.moeller%40uni-siegen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="michael.moeller@uni-siegen.de">Michael Moeller</a>, <a href="/profile?email=tomg%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tomg@cs.umd.edu">Tom Goldstein</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 28 Apr 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>15 Replies</span>


      </div>

      <a href="#HyxyIgHFvr-details-107" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="HyxyIgHFvr-details-107">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep learning, generalization, loss landscape, robustness</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/goldblum/TruthOrBackpropaganda</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=HyxyIgHFvr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="H1gNOeHKPS" data-number="2395">
      <h4>
        <a href="/forum?id=H1gNOeHKPS">
          Neural Arithmetic Units
        </a>


        <a href="/pdf?id=H1gNOeHKPS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=amwebdk%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="amwebdk@gmail.com">Andreas Madsen</a>, <a href="/profile?email=alexander%40herhjemme.dk" class="profile-link" data-toggle="tooltip" data-placement="top" title="alexander@herhjemme.dk">Alexander Rosenberg Johansen</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>16 Replies</span>


      </div>

      <a href="#H1gNOeHKPS-details-561" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="H1gNOeHKPS-details-561">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural networks can approximate complex functions, but they struggle to perform exact arithmetic operations over real numbers. The lack of inductive bias for arithmetic operations leaves neural networks without the underlying logic necessary to extrapolate on tasks such as addition, subtraction, and multiplication. We present two new neural network components: the Neural Addition Unit (NAU), which can learn exact addition and subtraction; and the Neural Multiplication Unit (NMU) that can multiply subsets of a vector. The NMU is, to our knowledge, the first arithmetic neural network component that can learn to multiply elements from a vector, when the hidden size is large. The two new components draw inspiration from a theoretical analysis of recently proposed arithmetic components. We find that careful initialization, restricting parameter space, and regularizing for sparsity is important when optimizing the NAU and NMU. Our proposed units NAU and NMU, compared with previous neural units, converge more consistently, have fewer parameters, learn faster, can converge for larger hidden sizes, obtain sparse and meaningful weights, and can extrapolate to negative and small values.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/AndreasMadsen/stable-nalu</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=H1gNOeHKPS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="B1e3OlStPB" data-number="2413">
      <h4>
        <a href="/forum?id=B1e3OlStPB">
          DeepSphere: a graph-based spherical CNN
        </a>


        <a href="/pdf?id=B1e3OlStPB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=michael.defferrard%40epfl.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="michael.defferrard@epfl.ch">Michaël Defferrard</a>, <a href="/profile?email=martino.milani%40epfl.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="martino.milani@epfl.ch">Martino Milani</a>, <a href="/profile?email=frederick.gusset%40epfl.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="frederick.gusset@epfl.ch">Frédérick Gusset</a>, <a href="/profile?email=nathanael.perraudin%40sdsc.ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="nathanael.perraudin@sdsc.ethz.ch">Nathanaël Perraudin</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#B1e3OlStPB-details-213" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="B1e3OlStPB-details-213">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A graph-based spherical CNN that strikes an interesting balance of trade-offs for a wide variety of applications.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the discretized sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of pixels and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation. Perhaps surprisingly, comparison with previous work suggests that anisotropic filters might be an unnecessary price to pay. Our code is available at https://github.com/deepsphere.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">spherical cnns, graph neural networks, geometric deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/deepsphere</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=B1e3OlStPB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SylkYeHtwr" data-number="2419">
      <h4>
        <a href="/forum?id=SylkYeHtwr">
          SUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable Models
        </a>


        <a href="/pdf?id=SylkYeHtwr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=luoyc15%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="luoyc15@mails.tsinghua.edu.cn">Yucen Luo</a>, <a href="/profile?email=abeatson%40cs.princeton.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="abeatson@cs.princeton.edu">Alex Beatson</a>, <a href="/profile?email=mnorouzi%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mnorouzi@google.com">Mohammad Norouzi</a>, <a href="/profile?email=dcszj%40mail.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="dcszj@mail.tsinghua.edu.cn">Jun Zhu</a>, <a href="/profile?email=duvenaud%40cs.toronto.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="duvenaud@cs.toronto.edu">David Duvenaud</a>, <a href="/profile?email=rpa%40princeton.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="rpa@princeton.edu">Ryan P. Adams</a>, <a href="/profile?email=rtqichen%40cs.toronto.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="rtqichen@cs.toronto.edu">Ricky T. Q. Chen</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#SylkYeHtwr-details-713" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SylkYeHtwr-details-713">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We create an unbiased estimator for the log probability of latent variable models, extending such models to a larger scope of applications.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Standard variational lower bounds used to train latent variable models produce biased estimates of most quantities of interest. We introduce an unbiased estimator of the log marginal likelihood and its gradients for latent variable models based on randomized truncation of infinite series. If parameterized by an encoder-decoder architecture, the parameters of the encoder can be optimized to minimize its variance of this estimator. We show that models trained using our estimator give better test-set likelihoods than a standard importance-sampling based approach for the same average computational cost. This estimator also allows use of latent variable models for tasks where unbiased estimators, rather than marginal likelihood lower bounds, are preferred, such as minimizing reverse KL divergences and estimating score functions.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SylkYeHtwr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1eZYeHFDS" data-number="2425">
      <h4>
        <a href="/forum?id=S1eZYeHFDS">
          Deep Learning For Symbolic Mathematics
        </a>


        <a href="/pdf?id=S1eZYeHFDS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=guillaume.lample%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="guillaume.lample@gmail.com">Guillaume Lample</a>, <a href="/profile?email=fcharton%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="fcharton@fb.com">François Charton</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>29 Replies</span>


      </div>

      <a href="#S1eZYeHFDS-details-83" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1eZYeHFDS-details-83">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We train a neural network to compute function integrals, and to solve complex differential equations.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing these mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">symbolic, math, deep learning, transformers</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1eZYeHFDS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1xitgHtvS" data-number="2446">
      <h4>
        <a href="/forum?id=S1xitgHtvS">
          Making Sense of Reinforcement Learning and Probabilistic Inference
        </a>


        <a href="/pdf?id=S1xitgHtvS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=bodonoghue85%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="bodonoghue85@gmail.com">Brendan O'Donoghue</a>, <a href="/profile?email=iosband%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="iosband@google.com">Ian Osband</a>, <a href="/profile?email=cdi%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="cdi@google.com">Catalin Ionescu</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>11 Replies</span>


      </div>

      <a href="#S1xitgHtvS-details-289" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1xitgHtvS-details-289">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Popular algorithms that cast "RL as Inference" ignore the role of uncertainty and exploration. We highlight the importance of these issues and present a coherent framework for RL and inference that handles them gracefully.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts ‘RL as inference’ and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We demonstrate that the popular ‘RL as inference’ approximation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling.
              </span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement learning, Bayesian inference, Exploration</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1xitgHtvS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="r1eyceSYPr" data-number="2455">
      <h4>
        <a href="/forum?id=r1eyceSYPr">
          Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models
        </a>


        <a href="/pdf?id=r1eyceSYPr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=yixuanq%40andrew.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yixuanq@andrew.cmu.edu">Yixuan Qiu</a>, <a href="/profile?email=lingsong%40purdue.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="lingsong@purdue.edu">Lingsong Zhang</a>, <a href="/profile?email=wangxiao%40purdue.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangxiao@purdue.edu">Xiao Wang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>13 Replies</span>


      </div>

      <a href="#r1eyceSYPr-details-343" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="r1eyceSYPr-details-343">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We have developed a new training algorithm for energy-based latent variable models that completely removes the bias of contrastive divergence.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The contrastive divergence algorithm is a popular approach to training energy-based latent variable models, which has been widely used in many machine learning models such as the restricted Boltzmann machines and deep belief nets. Despite its empirical success, the contrastive divergence algorithm is also known to have biases that severely affect its convergence. In this article we propose an unbiased version of the contrastive divergence algorithm that completely removes its bias in stochastic gradient methods, based on recent advances on unbiased Markov chain Monte Carlo methods. Rigorous theoretical analysis is developed to justify the proposed algorithm, and numerical experiments show that it significantly improves the existing method. Our findings suggest that the unbiased contrastive divergence algorithm is a promising approach to training general energy-based latent variable models.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">energy model, restricted Boltzmann machine, contrastive divergence, unbiased Markov chain Monte Carlo, distribution coupling</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=r1eyceSYPr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="Syx79eBKwr" data-number="2466">
      <h4>
        <a href="/forum?id=Syx79eBKwr">
          A Mutual Information Maximization Perspective of Language Representation Learning
        </a>


        <a href="/pdf?id=Syx79eBKwr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=lingpenk%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lingpenk@google.com">Lingpeng Kong</a>, <a href="/profile?email=cyprien%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="cyprien@google.com">Cyprien de Masson d'Autume</a>, <a href="/profile?email=leiyu%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="leiyu@google.com">Lei Yu</a>, <a href="/profile?email=lingwang%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lingwang@google.com">Wang Ling</a>, <a href="/profile?email=zihangd%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zihangd@google.com">Zihang Dai</a>, <a href="/profile?email=dyogatama%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dyogatama@google.com">Dani Yogatama</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>12 Replies</span>


      </div>

      <a href="#Syx79eBKwr-details-558" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="Syx79eBKwr-details-558">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=Syx79eBKwr&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1e_9xrFvS" data-number="2477">
      <h4>
        <a href="/forum?id=S1e_9xrFvS">
          Energy-based models for atomic-resolution protein conformations
        </a>


        <a href="/pdf?id=S1e_9xrFvS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=yilundu%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yilundu@mit.edu">Yilun Du</a>, <a href="/profile?email=jmeier%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jmeier@fb.com">Joshua Meier</a>, <a href="/profile?email=maj%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="maj@fb.com">Jerry Ma</a>, <a href="/profile?email=robfergus%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="robfergus@fb.com">Rob Fergus</a>, <a href="/profile?email=arives%40cs.nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="arives@cs.nyu.edu">Alexander Rives</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#S1e_9xrFvS-details-874" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1e_9xrFvS-details-874">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Energy-based models trained on crystallized protein structures predict native side chain configurations and automatically discover molecular energy features.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose an energy-based model (EBM) of protein conformations that operates at atomic scale. The model is trained solely on crystallized protein data. By contrast, existing approaches for scoring conformations use energy functions that incorporate knowledge of physical principles and features that are the complex product of several decades of research and tuning. To evaluate the model, we benchmark on the rotamer recovery task, the problem of predicting the conformation of a side chain from its context within a protein structure, which has been used to evaluate energy functions for protein design. The model achieves performance close to that of the Rosetta energy function, a state-of-the-art method widely used in protein structure prediction and design. An investigation of the model’s outputs and hidden representations finds that it captures physicochemical properties relevant to protein energy.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">energy-based model, transformer, energy function, protein conformation</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/facebookresearch/protein-ebm</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1e_9xrFvS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="BJe55gBtvH" data-number="2482">
      <h4>
        <a href="/forum?id=BJe55gBtvH">
          Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem
        </a>


        <a href="/pdf?id=BJe55gBtvH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=vaggos%40cs.stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="vaggos@cs.stanford.edu">Vaggos Chatziafratis</a>, <a href="/profile?email=sai_nagarajan%40mymail.sutd.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="top" title="sai_nagarajan@mymail.sutd.edu.sg">Sai Ganesh Nagarajan</a>, <a href="/profile?email=ioannis%40sutd.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="top" title="ioannis@sutd.edu.sg">Ioannis Panageas</a>, <a href="/profile?email=xiao_wang%40sutd.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="top" title="xiao_wang@sutd.edu.sg">Xiao Wang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>6 Replies</span>


      </div>

      <a href="#BJe55gBtvH-details-408" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="BJe55gBtvH-details-408">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In this work, we point to a new connection between DNNs expressivity and Sharkovsky’s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky’s work reveals the limitations of shallow neural networks, it doesn’t inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths.
                In this work, we point to a new connection between DNNs expressivity and Sharkovsky’s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky’s work contain points of period 3 – a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke – we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions.</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://docs.google.com/document/d/1qr-sROZ7q93OhigF6CoPde5NQ901wI17wmnBvbZRT9s/edit?fbclid=IwAR1HwkNZ1g2QgMmTGRZ0ktCYNgeKKk91tvRNLb59QJwU3dRmuGCJbTNMwj0</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Depth-Width trade-offs, ReLU networks, chaos theory, Sharkovsky Theorem, dynamical systems</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=BJe55gBtvH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="H1gBsgBYwH" data-number="2506">
      <h4>
        <a href="/forum?id=H1gBsgBYwH">
          Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint
        </a>


        <a href="/pdf?id=H1gBsgBYwH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=jba%40cs.toronto.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jba@cs.toronto.edu">Jimmy Ba</a>, <a href="/profile?email=erdogdu%40cs.toronto.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="erdogdu@cs.toronto.edu">Murat Erdogdu</a>, <a href="/profile?email=taiji%40mist.i.u-tokyo.ac.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="taiji@mist.i.u-tokyo.ac.jp">Taiji Suzuki</a>, <a href="/profile?email=dennywu%40cs.toronto.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dennywu@cs.toronto.edu">Denny Wu</a>, <a href="/profile?email=ztz16%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="ztz16@mails.tsinghua.edu.cn">Tianzong Zhang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>7 Replies</span>


      </div>

      <a href="#H1gBsgBYwH-details-135" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="H1gBsgBYwH-details-135">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Derived population risk of two-layer neural networks in high dimensions and examined presence / absence of "double descent".</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="181">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D45B TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>n</mi>
                    </math></mjx-assistive-mml>
                </mjx-container>, features <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="182">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D451 TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>d</mi>
                    </math></mjx-assistive-mml>
                </mjx-container>, and neurons <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="183">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c210E TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>h</mi>
                    </math></mjx-assistive-mml>
                </mjx-container> tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups. When only the second layer coefficients are optimized, we recover the \textit{double descent} phenomenon: a cusp in the population risk appears at <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="184">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c210E TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-mo class="mjx-n" space="4">
                      <mjx-c class="mjx-c2248"></mjx-c>
                    </mjx-mo>
                    <mjx-mi class="mjx-i" space="4">
                      <mjx-c class="mjx-c1D45B TEX-I"></mjx-c>
                    </mjx-mi>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <mi>h</mi>
                      <mo>≈</mo>
                      <mi>n</mi>
                    </math></mjx-assistive-mml>
                </mjx-container> and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is \textit{independent} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to \textit{double descent} might not translate to optimizing two-layer neural networks.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Networks, Generalization, High-dimensional Statistics</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=H1gBsgBYwH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SJxUjlBtwB" data-number="2509">
      <h4>
        <a href="/forum?id=SJxUjlBtwB">
          Reconstructing continuous distributions of 3D protein structure from cryo-EM images
        </a>


        <a href="/pdf?id=SJxUjlBtwB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=zhonge%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhonge@mit.edu">Ellen D. Zhong</a>, <a href="/profile?email=tbepler%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tbepler@mit.edu">Tristan Bepler</a>, <a href="/profile?email=jhdavis%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jhdavis@mit.edu">Joseph H. Davis</a>, <a href="/profile?email=bab%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="bab@mit.edu">Bonnie Berger</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>9 Replies</span>


      </div>

      <a href="#SJxUjlBtwB-details-761" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SJxUjlBtwB-details-761">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a deep generative model of volumes for 3D cryo-EM reconstruction from unlabelled 2D images and show that it can learn can learn continuous deformations in protein structure.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the 3D structure of a macromolecule from <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 113.1%; position: relative;" role="presentation" tabindex="0" ctxtmenu_counter="185">
                  <mjx-math class="MJX-TEX" aria-hidden="true">
                    <mjx-msup>
                      <mjx-mn class="mjx-n">
                        <mjx-c class="mjx-c31"></mjx-c>
                        <mjx-c class="mjx-c30"></mjx-c>
                      </mjx-mn>
                      <mjx-script style="vertical-align: 0.393em;">
                        <mjx-texatom size="s" texclass="ORD">
                          <mjx-mn class="mjx-n">
                            <mjx-c class="mjx-c34"></mjx-c>
                          </mjx-mn>
                          <mjx-mo class="mjx-n">
                            <mjx-c class="mjx-c2212"></mjx-c>
                          </mjx-mo>
                          <mjx-mn class="mjx-n">
                            <mjx-c class="mjx-c37"></mjx-c>
                          </mjx-mn>
                        </mjx-texatom>
                      </mjx-script>
                    </mjx-msup>
                  </mjx-math>
                  <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML">
                      <msup>
                        <mn>10</mn>
                        <mrow>
                          <mn>4</mn>
                          <mo>−</mo>
                          <mn>7</mn>
                        </mrow>
                      </msup>
                    </math></mjx-assistive-mml>
                </mjx-container> noisy and randomly oriented 2D projection images. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes structures in Fourier space using coordinate-based deep neural networks, and trains these networks from unlabeled 2D cryo-EM images by combining exact inference over image orientation with variational inference for structural heterogeneity. We demonstrate that the proposed method, termed cryoDRGN, can perform ab-initio reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image data. To our knowledge, cryoDRGN is the first neural network-based approach for cryo-EM reconstruction and the first end-to-end method for directly reconstructing continuous ensembles of protein structures from cryo-EM images.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generative models, proteins, 3D reconstruction, cryo-EM</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SJxUjlBtwB&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="SJxpsxrYPS" data-number="2518">
      <h4>
        <a href="/forum?id=SJxpsxrYPS">
          PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS
        </a>


        <a href="/pdf?id=SJxpsxrYPS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=zl7904%40rit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="zl7904@rit.edu">Zhiyuan Li</a>, <a href="/profile?email=jvm6526%40rit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jvm6526@rit.edu">Jaideep Vitthal Murkute</a>, <a href="/profile?email=pkg2182%40rit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="pkg2182@rit.edu">Prashnna Kumar Gyawali</a>, <a href="/profile?email=linwei.wang%40rit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="linwei.wang@rit.edu">Linwei Wang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#SJxpsxrYPS-details-838" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="SJxpsxrYPS-details-838">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We proposed a progressive learning method to improve learning and disentangling latent representations at different levels of abstraction.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning rich representation from data is an important task for deep generative models such as variational auto-encoder (VAE). However, by extracting high-level abstractions in the bottom-up inference process, the goal of preserving all factors of variations for top-down generation is compromised. Motivated by the concept of “starting small”, we present a strategy to progressively learn independent hierarchical representations from high- to low-levels of abstractions. The model starts with learning the most abstract representation, and then progressively grow the network architecture to introduce new representations at different levels of abstraction. We quantitatively demonstrate the ability of the presented model to improve disentanglement in comparison to existing works on two benchmark datasets using three disentanglement metrics, including a new metric we proposed to complement the previously-presented metric of mutual information gap. We further present both qualitative and quantitative evidence on how the progression of learning improves disentangling of hierarchical representations. By drawing on the respective advantage of hierarchical representation learning and progressive learning, this is to our knowledge the first attempt to improve disentanglement by progressively growing the capacity of VAE to learn hierarchical representations.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generative model, disentanglement, progressive learning, VAE</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=SJxpsxrYPS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="rJg8TeSFDH" data-number="2575">
      <h4>
        <a href="/forum?id=rJg8TeSFDH">
          An Exponential Learning Rate Schedule for Deep Learning
        </a>


        <a href="/pdf?id=rJg8TeSFDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=zhiyuanli%40cs.princeton.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhiyuanli@cs.princeton.edu">Zhiyuan Li</a>, <a href="/profile?email=arora%40cs.princeton.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="arora@cs.princeton.edu">Sanjeev Arora</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>10 Replies</span>


      </div>

      <a href="#rJg8TeSFDH-details-87" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="rJg8TeSFDH-details-87">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose an exponentially growing learning rate schedule for networks with BatchNorm, which surprisingly performs well in practice and is provably equivalent to popular LR schedules like Step Decay.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe &amp; Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe &amp; Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)
                • Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + α) factor in every epoch for some α &gt; 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.
                • Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu &amp; He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.
                • A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">batch normalization, weight decay, learning rate, deep learning theory</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=rJg8TeSFDH&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
    <li class="note " data-id="S1e2agrFvS" data-number="2589">
      <h4>
        <a href="/forum?id=S1e2agrFvS">
          Geom-GCN: Geometric Graph Convolutional Networks
        </a>


        <a href="/pdf?id=S1e2agrFvS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


      </h4>



      <div class="note-authors">
        <a href="/profile?email=gspeihongbing%40163.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gspeihongbing@163.com">Hongbin Pei</a>, <a href="/profile?email=bwei6%40illinois.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="bwei6@illinois.edu">Bingzhe Wei</a>, <a href="/profile?email=kcchang%40illinois.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kcchang@illinois.edu">Kevin Chen-Chuan Chang</a>, <a href="/profile?email=csylei%40comp.polyu.edu.hk" class="profile-link" data-toggle="tooltip" data-placement="top" title="csylei@comp.polyu.edu.hk">Yu Lei</a>, <a href="/profile?email=ybo%40jlu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="ybo@jlu.edu.cn">Bo Yang</a>
      </div>

      <div class="note-meta-info">
        <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
        <span class="item">ICLR 2020 Conference Blind Submission</span>
        <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

        <span>20 Replies</span>


      </div>

      <a href="#S1e2agrFvS-details-143" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a>
      <div class="collapse" id="S1e2agrFvS-details-143">
        <div class="note-contents-collapse">
          <ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Message-passing neural networks (MPNNs) have been successfully applied in a wide variety of applications in the real world. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN, to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs.</span>
            </li>
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Learning, Graph Convolutional Network, Network Geometry</span>
            </li>
            <li>
              <strong class="note-content-field">Code:</strong>
              <span class="note-content-value ">https://github.com/graphdml-uiuc-jlu/geom-gcn</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">For graph neural networks, the aggregation on a graph can benefit from a continuous space underlying the graph.</span>
            </li>
            <li>
              <strong class="note-content-field">Original Pdf:</strong>
              <span class="note-content-value "><a href="/attachment?id=S1e2agrFvS&amp;name=original_pdf" class="attachment-download-link" title="Download Original Pdf" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
          </ul>
        </div>
      </div>




    </li>
  </ul>
</div>