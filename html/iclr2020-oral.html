<ul class="list-unstyled submissions-list">
  <li class="note " data-id="HJgzt2VKPB" data-number="69">
    <h4>
      <a href="/forum?id=HJgzt2VKPB">
        CATER: A diagnostic dataset for Compositional Actions &amp; TEmporal Reasoning
      </a>


      <a href="/pdf?id=HJgzt2VKPB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=rgirdhar%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="rgirdhar@cs.cmu.edu">Rohit Girdhar</a>, <a href="/profile?email=deva%40cs.cmu.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="deva@cs.cmu.edu">Deva Ramanan</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>9 Replies</span>


    </div>

    <a href="#HJgzt2VKPB-details-850" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="HJgzt2VKPB-details-850">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Video Understanding, Temporal Reasoning</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We propose a new video understanding benchmark, with tasks that by-design
              require temporal reasoning to be solved, unlike most existing video datasets.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Computer vision has undergone a dramatic revolution in performance, driven
              in large part through deep features trained on large-scale supervised datasets. However, much of these
              improvements have focused on static image analysis; video understanding has seen rather modest
              improvements. Even though new datasets and spatiotemporal models have been proposed, simple frame-by-frame
              classification methods often still remain competitive. We posit that current video datasets are plagued
              with implicit biases over scene and object structure that can dwarf variations in temporal structure. In
              this work, we build a video dataset with fully observable and controllable object and scene bias, and
              which truly requires spatiotemporal understanding in order to be solved. Our dataset, named CATER, is
              rendered synthetically using a library of standard 3D objects, and tests the ability to recognize
              compositions of object movements that require long-term reasoning. In addition to being a challenging
              dataset, CATER also provides a plethora of diagnostic tools to analyze modern spatiotemporal video
              architectures by being completely observable and controllable. Using CATER, we provide insights into some
              of the most recent state of the art deep video architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">http://rohitgirdhar.github.io/CATER</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=HJgzt2VKPB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="BJlrF24twB" data-number="78">
    <h4>
      <a href="/forum?id=BJlrF24twB">
        BackPACK: Packing more into Backprop
      </a>


      <a href="/pdf?id=BJlrF24twB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=felix.dangel%40tuebingen.mpg.de" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="felix.dangel@tuebingen.mpg.de">Felix Dangel</a>, <a
        href="/profile?email=kunstner%40cs.ubc.ca" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="kunstner@cs.ubc.ca">Frederik Kunstner</a>, <a href="/profile?email=philipp.hennig%40uni-tuebingen.de"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="philipp.hennig@uni-tuebingen.de">Philipp
        Hennig</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>7 Replies</span>


    </div>

    <a href="#BJlrF24twB-details-553" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="BJlrF24twB-details-553">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Automatic differentiation frameworks are optimized for exactly one thing:
              computing the average mini-batch gradient. Yet, other quantities such as the variance of the mini-batch
              gradients or many approximations to the Hessian can, in theory, be computed efficiently, and at the same
              time as the gradient. While these quantities are of great interest to researchers and practitioners,
              current deep learning software does not support their automatic calculation. Manually implementing them is
              burdensome, inefficient if done naively, and the resulting code is rarely shared. This hampers progress in
              deep learning, and unnecessarily narrows research to focus on gradient descent and its variants; it also
              complicates replication studies and comparisons between newly developed methods that require those
              quantities, to the point of impossibility. To address this problem, we introduce BackPACK, an efficient
              framework built on top of PyTorch, that extends the backpropagation algorithm to extract additional
              information from first-and second-order derivatives. Its capabilities are illustrated by benchmark reports
              for computing additional quantities on deep neural networks, and an example application by testing several
              recent curvature approximations for optimization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://toiaydcdyywlhzvlob.github.io/backpack/</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=BJlrF24twB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="HkxlcnVFwB" data-number="102">
    <h4>
      <a href="/forum?id=HkxlcnVFwB">
        GenDICE: Generalized Offline Estimation of Stationary Values
      </a>


      <a href="/pdf?id=HkxlcnVFwB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=ryzhang%40cs.duke.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="ryzhang@cs.duke.edu">Ruiyi Zhang*</a>, <a href="/profile?email=bodai%40google.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="bodai@google.com">Bo Dai*</a>, <a
        href="/profile?email=lihongli.cs%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="lihongli.cs@gmail.com">Lihong Li</a>, <a href="/profile?email=schuurmans%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="schuurmans@google.com">Dale
        Schuurmans</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>10 Replies</span>


    </div>

    <a href="#HkxlcnVFwB-details-850" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="HkxlcnVFwB-details-850">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">In this paper, we proposed a novel algorithm, GenDICE, for general
              stationary distribution correction estimation, which can handle both discounted and average off-policy
              evaluation on multiple behavior-agnostic samples.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">An important problem that arises in reinforcement learning and Monte Carlo
              methods is estimating quantities defined by the stationary distribution of a Markov chain. In many
              real-world applications, access to the underlying transition operator is limited to a fixed set of data
              that has already been collected, without additional interaction with the environment being available. We
              show that consistent estimation remains possible in this scenario, and that effective estimation can still
              be achieved in important applications. Our approach is based on estimating a ratio that corrects for the
              discrepancy between the stationary and empirical distributions, derived from fundamental properties of the
              stationary distribution, and exploiting constraint reformulations based on variational divergence
              minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency
              of the method under general conditions, provide a detailed error analysis, and demonstrate strong
              empirical performance on benchmark tasks, including off-line PageRank and off-policy policy
              evaluation.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Off-policy Policy Evaluation, Reinforcement Learning, Stationary
              Distribution Correction Estimation, Fenchel Dual</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=HkxlcnVFwB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="H1lma24tPB" data-number="221">
    <h4>
      <a href="/forum?id=H1lma24tPB">
        Principled Weight Initialization for Hypernetworks
      </a>


      <a href="/pdf?id=H1lma24tPB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=oscar.chang%40columbia.edu" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="oscar.chang@columbia.edu">Oscar Chang</a>, <a
        href="/profile?email=lamflokas%40cs.columbia.edu" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="lamflokas@cs.columbia.edu">Lampros Flokas</a>, <a
        href="/profile?email=hod.lipson%40columbia.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="hod.lipson@columbia.edu">Hod Lipson</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>11 Replies</span>


    </div>

    <a href="#H1lma24tPB-details-630" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="H1lma24tPB-details-630">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Hypernetworks are meta neural networks that generate weights for a main
              neural network in an end-to-end differentiable manner. Despite extensive applications ranging from
              multi-task learning to Bayesian deep learning, the problem of optimizing hypernetworks has not been
              studied to date. We observe that classical weight initialization methods like Glorot &amp; Bengio (2010)
              and He et al. (2015), when applied directly on a hypernet, fail to produce weights for the mainnet in the
              correct scale. We develop principled techniques for weight initialization in hypernets, and show that they
              lead to more stable mainnet weights, lower training loss, and faster convergence.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">hypernetworks, initialization, optimization, meta-learning</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">The first principled weight initialization method for hypernetworks</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=H1lma24tPB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="HJxNAnVtDS" data-number="261">
    <h4>
      <a href="/forum?id=HJxNAnVtDS">
        On the Convergence of FedAvg on Non-IID Data
      </a>


      <a href="/pdf?id=HJxNAnVtDS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=smslixiang%40pku.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="smslixiang@pku.edu.cn">Xiang Li</a>, <a href="/profile?email=hackyhuang%40pku.edu.cn"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="hackyhuang@pku.edu.cn">Kaixuan Huang</a>,
      <a href="/profile?email=yangwhsms%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="yangwhsms@gmail.com">Wenhao Yang</a>, <a href="/profile?email=shusen.wang%40stevens.edu"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="shusen.wang@stevens.edu">Shusen Wang</a>,
      <a href="/profile?email=zhzhang%40math.pku.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="zhzhang@math.pku.edu.cn">Zhihua Zhang</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>7 Replies</span>


    </div>

    <a href="#HJxNAnVtDS-details-751" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="HJxNAnVtDS-details-751">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Federated Learning, stochastic optimization, Federated Averaging</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Federated learning enables a large amount of edge computing devices to
              jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging
              (\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total
              devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical
              guarantees under realistic settings. In this paper, we analyze the convergence of \texttt{FedAvg} on
              non-iid data and establish a convergence rate of <mjx-container class="MathJax CtxtMenu_Attached_0"
                jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="372"
                style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-texatom texclass="ORD">
                    <mjx-mi class="mjx-cal mjx-i">
                      <mjx-c class="mjx-c4F TEX-C"></mjx-c>
                    </mjx-mi>
                  </mjx-texatom>
                  <mjx-mo class="mjx-n">
                    <mjx-c class="mjx-c28"></mjx-c>
                  </mjx-mo>
                  <mjx-mfrac>
                    <mjx-frac>
                      <mjx-num>
                        <mjx-nstrut></mjx-nstrut>
                        <mjx-mn class="mjx-n" size="s">
                          <mjx-c class="mjx-c31"></mjx-c>
                        </mjx-mn>
                      </mjx-num>
                      <mjx-dbox>
                        <mjx-dtable>
                          <mjx-line></mjx-line>
                          <mjx-row>
                            <mjx-den>
                              <mjx-dstrut></mjx-dstrut>
                              <mjx-mi class="mjx-i" size="s">
                                <mjx-c class="mjx-c1D447 TEX-I"></mjx-c>
                              </mjx-mi>
                            </mjx-den>
                          </mjx-row>
                        </mjx-dtable>
                      </mjx-dbox>
                    </mjx-frac>
                  </mjx-mfrac>
                  <mjx-mo class="mjx-n">
                    <mjx-c class="mjx-c29"></mjx-c>
                  </mjx-mo>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi>
                    </mrow>
                    <mo stretchy="false">(</mo>
                    <mfrac>
                      <mn>1</mn>
                      <mi>T</mi>
                    </mfrac>
                    <mo stretchy="false">)</mo>
                  </math></mjx-assistive-mml>
              </mjx-container> for strongly convex and smooth problems, where <mjx-container
                class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="373"
                style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-mi class="mjx-i">
                    <mjx-c class="mjx-c1D447 TEX-I"></mjx-c>
                  </mjx-mi>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>T</mi>
                  </math></mjx-assistive-mml>
              </mjx-container> is the number of SGDs. Importantly, our bound demonstrates a trade-off between
              communication-efficiency and convergence rate. As user devices may be disconnected from the server, we
              relax the assumption of full device participation to partial device participation and study different
              averaging schemes; low device participation rate can be achieved without severely slowing down the
              learning. Our results indicate that heterogeneity of data slows down the convergence, which matches
              empirical observations. Furthermore, we provide a necessary condition for \texttt{FedAvg} on non-iid data:
              the learning rate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation"
                tabindex="0" ctxtmenu_counter="374" style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-mi class="mjx-i">
                    <mjx-c class="mjx-c1D702 TEX-I"></mjx-c>
                  </mjx-mi>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>η</mi>
                  </math></mjx-assistive-mml>
              </mjx-container> must decay, even if full-gradient is used; otherwise, the solution will be <mjx-container
                class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="375"
                style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-mi class="mjx-n">
                    <mjx-c class="mjx-c3A9"></mjx-c>
                  </mjx-mi>
                  <mjx-mo class="mjx-n">
                    <mjx-c class="mjx-c28"></mjx-c>
                  </mjx-mo>
                  <mjx-mi class="mjx-i">
                    <mjx-c class="mjx-c1D702 TEX-I"></mjx-c>
                  </mjx-mi>
                  <mjx-mo class="mjx-n">
                    <mjx-c class="mjx-c29"></mjx-c>
                  </mjx-mo>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi mathvariant="normal">Ω</mi>
                    <mo stretchy="false">(</mo>
                    <mi>η</mi>
                    <mo stretchy="false">)</mo>
                  </math></mjx-assistive-mml>
              </mjx-container> away from the optimal.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/lx10077/fedavgpy</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=HJxNAnVtDS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="S1efxTVYDr" data-number="329">
    <h4>
      <a href="/forum?id=S1efxTVYDr">
        Data-dependent Gaussian Prior Objective for Language Generation
      </a>


      <a href="/pdf?id=S1efxTVYDr" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=charlee%40sjtu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="charlee@sjtu.edu.cn">Zuchao Li</a>, <a href="/profile?email=wangrui%40nict.go.jp" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="wangrui@nict.go.jp">Rui Wang</a>, <a
        href="/profile?email=khchen%40nict.go.jp" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="khchen@nict.go.jp">Kehai Chen</a>, <a href="/profile?email=mutiyama%40nict.go.jp" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="mutiyama@nict.go.jp">Masso Utiyama</a>, <a
        href="/profile?email=eiichiro.sumita%40nict.go.jp" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="eiichiro.sumita@nict.go.jp">Eiichiro Sumita</a>, <a
        href="/profile?email=zhangzs%40sjtu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="zhangzs@sjtu.edu.cn">Zhuosheng Zhang</a>, <a href="/profile?email=zhaohai%40cs.sjtu.edu.cn"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="zhaohai@cs.sjtu.edu.cn">Hai Zhao</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>12 Replies</span>


    </div>

    <a href="#S1efxTVYDr-details-705" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="S1efxTVYDr-details-705">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We introduce an extra data-dependent Gaussian prior objective to augment
              the current MLE training, which is designed to capture the prior knowledge in the ground-truth
              data.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">For typical sequence prediction problems such as language generation,
              maximum likelihood estimation (MLE) has commonly been adopted as it encourages the predicted sequence most
              consistent with the ground-truth sequence to have the highest probability of occurring. However, MLE
              focuses on once-to-all matching between the predicted sequence and gold-standard, consequently treating
              all incorrect predictions as being equally incorrect. We refer to this drawback as {\it negative diversity
              ignorance} in this paper. Treating all incorrect predictions as equal unfairly downplays the nuance of
              these sequences' detailed token-wise structure. To counteract this, we augment the MLE loss by introducing
              an extra Kullback--Leibler divergence term derived by comparing a data-dependent Gaussian prior and the
              detailed training prediction. The proposed data-dependent Gaussian prior objective (D2GPo) is defined over
              a prior topological order of tokens and is poles apart from the data-independent Gaussian prior (L2
              regularization) commonly adopted in smoothing the training of MLE. Experimental results show that the
              proposed method makes effective use of a more detailed prior in the data and has improved performance in
              typical language generation tasks, including supervised and unsupervised machine translation, text
              summarization, storytelling, and image captioning.
            </span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://drive.google.com/file/d/1q8PqhF9eOLOHOcOCGVKXtA_OlP6qq2mn</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Gaussian Prior Objective, Language Generation</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=S1efxTVYDr&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="H1gax6VtDB" data-number="356">
    <h4>
      <a href="/forum?id=H1gax6VtDB">
        Contrastive Learning of Structured World Models
      </a>


      <a href="/pdf?id=H1gax6VtDB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=t.n.kipf%40uva.nl" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="t.n.kipf@uva.nl">Thomas Kipf</a>, <a href="/profile?email=e.e.vanderpol%40uva.nl" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="e.e.vanderpol@uva.nl">Elise van der Pol</a>, <a
        href="/profile?email=m.welling%40uva.nl" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="m.welling@uva.nl">Max Welling</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>17 Replies</span>


    </div>

    <a href="#H1gax6VtDB-details-117" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="H1gax6VtDB-details-117">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">state representation learning, graph neural networks, model-based
              reinforcement learning, relational learning, object discovery</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">Contrastively-trained Structured World Models (C-SWMs) learn
              object-oriented state representations and a relational model of an environment from raw pixel
              input.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A structured understanding of our world in terms of objects, relations,
              and hierarchies is an important component of human cognition. Learning such a structured world model from
              raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained
              Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in
              environments with compositional structure. We structure each state embedding as a set of object
              representations and their relations, modeled by a graph neural network. This allows objects to be
              discovered from raw pixel observations without direct supervision as part of the learning process. We
              evaluate C-SWMs on compositional environments involving multiple interacting objects that can be
              manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our
              experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and
              outperform typical representatives of this model class in highly structured environments, while learning
              interpretable object-based representations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/tkipf/c-swm</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=H1gax6VtDB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="B1evfa4tPB" data-number="415">
    <h4>
      <a href="/forum?id=B1evfa4tPB">
        Neural Network Branching for Neural Network Verification
      </a>


      <a href="/pdf?id=B1evfa4tPB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=jingyue.lu%40spc.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="jingyue.lu@spc.ox.ac.uk">Jingyue Lu</a>, <a href="/profile?email=pawan%40robots.ox.ac.uk"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="pawan@robots.ox.ac.uk">M. Pawan Kumar</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>12 Replies</span>


    </div>

    <a href="#B1evfa4tPB-details-649" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="B1evfa4tPB-details-649">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural Network Verification, Branch and Bound, Graph Neural Network,
              Learning to branch</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We propose a novel learning to branch framework using graph neural
              networks to improve branch and bound based neural network verification methods. </span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Formal verification of neural networks is essential for their deployment
              in safety-critical areas. Many available formal verification methods have been shown to be instances of a
              unified Branch and Bound (BaB) formulation. We propose a novel framework for designing an effective
              branching strategy for BaB. Specifically, we learn a graph neural network (GNN) to imitate the strong
              branching heuristic behaviour. Our framework differs from previous methods for learning to branch in two
              main aspects. Firstly, our framework directly treats the neural network we want to verify as a graph input
              for the GNN. Secondly, we develop an intuitive forward and backward embedding update schedule.
              Empirically, our framework achieves roughly <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML"
                role="presentation" tabindex="0" ctxtmenu_counter="376" style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-mn class="mjx-n">
                    <mjx-c class="mjx-c35"></mjx-c>
                    <mjx-c class="mjx-c30"></mjx-c>
                  </mjx-mn>
                  <mjx-mi class="mjx-n">
                    <mjx-c class="mjx-c25"></mjx-c>
                  </mjx-mi>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mn>50</mn>
                    <mi mathvariant="normal">%</mi>
                  </math></mjx-assistive-mml>
              </mjx-container> reduction in both the number of branches and the time required for verification on
              various convolutional networks when compared to the best available hand-designed branching strategy. In
              addition, we show that our GNN model enjoys both horizontal and vertical transferability. Horizontally,
              the model trained on easy properties performs well on properties of increased difficulty levels.
              Vertically, the model trained on small neural networks achieves similar performance on large neural
              networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=B1evfa4tPB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="BJgnXpVYwS" data-number="464">
    <h4>
      <a href="/forum?id=BJgnXpVYwS">
        Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity
      </a>


      <a href="/pdf?id=BJgnXpVYwS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=jzhzhang%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="jzhzhang@mit.edu">Jingzhao Zhang</a>, <a href="/profile?email=tianxing%40mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="tianxing@mit.edu">Tianxing He</a>, <a
        href="/profile?email=suvrit%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="suvrit@mit.edu">Suvrit Sra</a>, <a href="/profile?email=jadbabai%40mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="jadbabai@mit.edu">Ali Jadbabaie</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>12 Replies</span>


    </div>

    <a href="#BJgnXpVYwS-details-764" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="BJgnXpVYwS-details-764">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">Gradient clipping provably accelerates gradient descent for non-smooth
              non-convex functions.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We provide a theoretical explanation for the effectiveness of gradient
              clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from
              practical neural network training examples. We observe that gradient smoothness, a concept central to the
              analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates
              significant variability along the training trajectory of deep neural networks. Further, this smoothness
              positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it
              can grow with the norm of the gradient. These empirical observations limit the applicability of existing
              theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate
              us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz
              smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient
              clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize.
              We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and
              verify our results empirically in popular neural network training settings.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Adaptive methods, optimization, deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/JingzhaoZhang/why-clipping-accelerates</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=BJgnXpVYwS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="Syg-ET4FPS" data-number="476">
    <h4>
      <a href="/forum?id=Syg-ET4FPS">
        Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information
      </a>


      <a href="/pdf?id=Syg-ET4FPS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=vofhqn%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="vofhqn@gmail.com">Yichi Zhou</a>, <a href="/profile?email=lijialia16%40mails.tsinghua.edu.cn"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="lijialia16@mails.tsinghua.edu.cn">Jialian
        Li</a>, <a href="/profile?email=dcszj%40mail.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="dcszj@mail.tsinghua.edu.cn">Jun Zhu</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>12 Replies</span>


    </div>

    <a href="#Syg-ET4FPS-details-261" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="Syg-ET4FPS-details-261">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Posterior sampling for reinforcement learning (PSRL) is a useful framework
              for making decisions in an unknown environment. PSRL maintains a posterior distribution of the environment
              and then makes planning on the environment sampled from the posterior distribution. Though PSRL works well
              on single-agent reinforcement learning problems, how to apply PSRL to multi-agent reinforcement learning
              problems is relatively unexplored. In this work, we extend PSRL to two-player zero-sum extensive-games
              with imperfect information (TEGI), which is a class of multi-agent systems. More specifically, we combine
              PSRL with counterfactual regret minimization (CFR), which is the leading algorithm for TEGI with a known
              environment. Our main contribution is a novel design of interaction strategies. With our interaction
              strategies, our algorithm provably converges to the Nash Equilibrium at a rate of <mjx-container
                class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="377"
                style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-mi class="mjx-i">
                    <mjx-c class="mjx-c1D442 TEX-I"></mjx-c>
                  </mjx-mi>
                  <mjx-mo class="mjx-n">
                    <mjx-c class="mjx-c28"></mjx-c>
                  </mjx-mo>
                  <mjx-msqrt>
                    <mjx-sqrt>
                      <mjx-surd>
                        <mjx-mo class="mjx-sop">
                          <mjx-c class="mjx-c221A TEX-S1"></mjx-c>
                        </mjx-mo>
                      </mjx-surd>
                      <mjx-box style="padding-top: 0.107em;">
                        <mjx-mi class="mjx-n">
                          <mjx-c class="mjx-c6C"></mjx-c>
                          <mjx-c class="mjx-c6F"></mjx-c>
                          <mjx-c class="mjx-c67"></mjx-c>
                        </mjx-mi>
                        <mjx-mo class="mjx-n">
                          <mjx-c class="mjx-c2061"></mjx-c>
                        </mjx-mo>
                        <mjx-mi class="mjx-i" space="2">
                          <mjx-c class="mjx-c1D447 TEX-I"></mjx-c>
                        </mjx-mi>
                        <mjx-texatom texclass="ORD">
                          <mjx-mo class="mjx-n">
                            <mjx-c class="mjx-c2F"></mjx-c>
                          </mjx-mo>
                        </mjx-texatom>
                        <mjx-mi class="mjx-i">
                          <mjx-c class="mjx-c1D447 TEX-I"></mjx-c>
                        </mjx-mi>
                      </mjx-box>
                    </mjx-sqrt>
                  </mjx-msqrt>
                  <mjx-mo class="mjx-n">
                    <mjx-c class="mjx-c29"></mjx-c>
                  </mjx-mo>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>O</mi>
                    <mo stretchy="false">(</mo>
                    <msqrt>
                      <mi>log</mi>
                      <mo data-mjx-texclass="NONE">⁡</mo>
                      <mi>T</mi>
                      <mrow>
                        <mo>/</mo>
                      </mrow>
                      <mi>T</mi>
                    </msqrt>
                    <mo stretchy="false">)</mo>
                  </math></mjx-assistive-mml>
              </mjx-container>. Empirical results show that our algorithm works well.</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=Syg-ET4FPS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="SJe5P6EYvS" data-number="606">
    <h4>
      <a href="/forum?id=SJe5P6EYvS">
        Mogrifier LSTM
      </a>


      <a href="/pdf?id=SJe5P6EYvS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=melisgl%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="melisgl@google.com">Gábor Melis</a>, <a href="/profile?email=tkocisky%40google.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="tkocisky@google.com">Tomáš Kočiský</a>, <a
        href="/profile?email=pblunsom%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="pblunsom@google.com">Phil Blunsom</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>7 Replies</span>


    </div>

    <a href="#SJe5P6EYvS-details-140" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="SJe5P6EYvS-details-140">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">lstm, language modelling</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">An LSTM extension with state-of-the-art language modelling results.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many advances in Natural Language Processing have been based upon more
              expressive models for how inputs interact with the context in which they occur. Recurrent networks, which
              have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for
              modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the
              form of mutual gating of the current input and the previous output. This mechanism affords the modelling
              of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed
              as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly
              improved generalization on language modelling in the range of 3–4 perplexity points on Penn Treebank and
              Wikitext-2, and 0.01–0.05 bpc on four character-based datasets. We establish a new state of the art on all
              datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.
            </span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=SJe5P6EYvS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="B1elCp4KwH" data-number="843">
    <h4>
      <a href="/forum?id=B1elCp4KwH">
        Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech
      </a>


      <a href="/pdf?id=B1elCp4KwH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=dharwath%40csail.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="dharwath@csail.mit.edu">David Harwath*</a>, <a href="/profile?email=wnhsu%40mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="wnhsu@mit.edu">Wei-Ning Hsu*</a>, <a
        href="/profile?email=glass%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="glass@mit.edu">James Glass</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 08 Jun 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>10 Replies</span>


    </div>

    <a href="#B1elCp4KwH-details-386" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="B1elCp4KwH-details-386">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">Vector quantization layers incorporated into a self-supervised neural
              model of speech audio learn hierarchical and discrete linguistic units (phone-like, word-like) when
              trained with a visual-grounding objective. </span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we present a method for learning discrete linguistic units
              by incorporating vector quantization layers into neural models of visually grounded speech. We show that
              our method is capable of capturing both word-level and sub-word units, depending on how it is configured.
              What differentiates this paper from prior work on speech unit learning is the choice of training
              objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding
              objective which forces the learned units to be useful for semantic image retrieval. We evaluate the
              sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3% reduction in ABX error rate over the
              top-performing submission, while keeping the bitrate approximately the same. We also present experiments
              demonstrating the noise robustness of these units. Finally, we show that a model with multiple quantizers
              can simultaneously learn phone-like detectors at a lower layer and word-like detectors at a higher layer.
              We show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than
              0.5.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">visually-grounded speech, self-supervised learning, discrete
              representation learning, vision and language, vision and speech, hierarchical representation
              learning</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/wnhsu/ResDAVEnet-VQ</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=B1elCp4KwH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="HkxQRTNYPH" data-number="850">
    <h4>
      <a href="/forum?id=HkxQRTNYPH">
        Mirror-Generative Neural Machine Translation
      </a>


      <a href="/pdf?id=HkxQRTNYPH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=zhengzx.142857%40gmail.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="zhengzx.142857@gmail.com">Zaixiang Zheng</a>, <a
        href="/profile?email=zhouhao.nlp%40bytedance.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="zhouhao.nlp@bytedance.com">Hao Zhou</a>, <a
        href="/profile?email=huangsj%40nju.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="huangsj@nju.edu.cn">Shujian Huang</a>, <a href="/profile?email=lilei.02%40bytedance.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="lilei.02@bytedance.com">Lei Li</a>, <a
        href="/profile?email=daixinyu%40nju.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="daixinyu@nju.edu.cn">Xin-Yu Dai</a>, <a href="/profile?email=chenjj%40nju.edu.cn" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="chenjj@nju.edu.cn">Jiajun Chen</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 14 May 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>13 Replies</span>


    </div>

    <a href="#HkxQRTNYPH-details-648" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="HkxQRTNYPH-details-648">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">
              Training neural machine translation models (NMT) requires a large amount of parallel corpus, which is
              scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing
              approaches have not exploited the full potential of non-parallel bilingual data either in training or
              decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that
              simultaneously integrates the source to target translation model, the target to source translation model,
              and two language models. Both translation models and language models share the same latent semantic space,
              therefore both translation directions can learn from non-parallel data more effectively. Besides, the
              translation models and language models can collaborate together during decoding. Our experiments show that
              the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language
              pairs, including resource-rich and low-resource languages. </span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural machine translation, generative model, mirror</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=HkxQRTNYPH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="rkeS1RVtPS" data-number="892">
    <h4>
      <a href="/forum?id=rkeS1RVtPS">
        Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning
      </a>


      <a href="/pdf?id=rkeS1RVtPS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=rz297%40cornell.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="rz297@cornell.edu">Ruqi Zhang</a>, <a href="/profile?email=chunyuan.li%40duke.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="chunyuan.li@duke.edu">Chunyuan Li</a>, <a
        href="/profile?email=jz318%40duke.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="jz318@duke.edu">Jianyi Zhang</a>, <a href="/profile?email=cchangyou%40gmail.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="cchangyou@gmail.com">Changyou Chen</a>, <a
        href="/profile?email=andrewgw%40cims.nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="andrewgw@cims.nyu.edu">Andrew Gordon Wilson</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 01 May 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>10 Replies</span>


    </div>

    <a href="#rkeS1RVtPS-details-773" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="rkeS1RVtPS-details-773">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The posteriors over neural network weights are high dimensional and
              multimodal. Each mode typically characterizes a meaningfully different representation of the data. We
              develop Cyclical Stochastic Gradient MCMC (SG-MCMC) to automatically explore such distributions. In
              particular, we propose a cyclical stepsize schedule, where larger steps discover new modes, and smaller
              steps characterize each mode. We prove non-asymptotic convergence theory of our proposed algorithm.
              Moreover, we provide extensive experimental results, including ImageNet, to demonstrate the effectiveness
              of cyclical SG-MCMC in learning complex multimodal distributions, especially for fully Bayesian inference
              with modern deep neural networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=rkeS1RVtPS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="Hkxzx0NtDB" data-number="921">
    <h4>
      <a href="/forum?id=Hkxzx0NtDB">
        Your classifier is secretly an energy based model and you should treat it like one
      </a>


      <a href="/pdf?id=Hkxzx0NtDB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=wgrathwohl%40cs.toronto.edu" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="wgrathwohl@cs.toronto.edu">Will Grathwohl</a>, <a
        href="/profile?email=wangkua1%40cs.toronto.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="wangkua1@cs.toronto.edu">Kuan-Chieh Wang</a>, <a href="/profile?email=j.jacobsen%40vectorinstitute.ai"
        class="profile-link" data-toggle="tooltip" data-placement="top"
        title="j.jacobsen@vectorinstitute.ai">Joern-Henrik Jacobsen</a>, <a
        href="/profile?email=duvenaud%40cs.toronto.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="duvenaud@cs.toronto.edu">David Duvenaud</a>, <a href="/profile?email=mnorouzi%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="mnorouzi@google.com">Mohammad
        Norouzi</a>, <a href="/profile?email=kswersky%40google.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="kswersky@google.com">Kevin Swersky</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>26 Replies</span>


    </div>

    <a href="#Hkxzx0NtDB-details-950" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="Hkxzx0NtDB-details-950">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">energy based models, adversarial robustness, generative models, out of
              distribution detection, outlier detection, hybrid models, robustness, calibration</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We show that there is a hidden generative model inside of every
              classifier. We demonstrate how to train this model and show the many benefits of doing so. </span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose to reinterpret a standard discriminative classifier of p(y|x)
              as an energy based model for the joint distribution p(x, y). In this setting, the standard class
              probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this
              framework, standard discriminative architectures may be used and the model can also be trained on
              unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration,
              robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling
              the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the
              training of energy based models and present an approach which adds little overhead compared to standard
              classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in
              both generative and discriminative learning within one hybrid model. </span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://wgrathwohl.github.io/JEM/</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=Hkxzx0NtDB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="HJgLZR4KvH" data-number="966">
    <h4>
      <a href="/forum?id=HJgLZR4KvH">
        Dynamics-Aware Unsupervised Discovery of Skills
      </a>


      <a href="/pdf?id=HJgLZR4KvH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=architsh%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="architsh@google.com">Archit Sharma</a>, <a href="/profile?email=shanegu%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="shanegu@google.com">Shixiang Gu</a>, <a
        href="/profile?email=slevine%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="slevine@google.com">Sergey Levine</a>, <a href="/profile?email=vikashplus%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="vikashplus@google.com">Vikash Kumar</a>,
      <a href="/profile?email=karolhausman%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="karolhausman@google.com">Karol Hausman</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 17 Apr 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>7 Replies</span>


    </div>

    <a href="#HJgLZR4KvH-details-679" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="HJgLZR4KvH-details-679">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, unsupervised learning, model-based learning, deep
              learning, hierarchical reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We propose an unsupervised skill discovery which enables model-based
              planning for hierarchical reinforcement learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Conventionally, model-based reinforcement learning (MBRL) aims to learn a
              global model for the dynamics of the environment. A good model can potentially enable planning algorithms
              to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for
              complex dynamical systems is difficult, and even then, the model might not generalize well outside the
              distribution of states on which it was trained. In this work, we combine model-based learning with
              model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the
              question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised
              learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable
              behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically,
              allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that
              zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free
              goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL
              methods for unsupervised skill discovery.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/google-research/dads</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=HJgLZR4KvH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="BkgzMCVtPB" data-number="995">
    <h4>
      <a href="/forum?id=BkgzMCVtPB">
        Optimal Strategies Against Generative Attacks
      </a>


      <a href="/pdf?id=BkgzMCVtPB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=roy16mor%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="roy16mor@gmail.com">Roy Mor</a>, <a href="/profile?email=erezpeter%40cs.huji.ac.il" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="erezpeter@cs.huji.ac.il">Erez Peterfreund</a>, <a
        href="/profile?email=matan.gavish%40mail.huji.ac.il" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="matan.gavish@mail.huji.ac.il">Matan Gavish</a>, <a
        href="/profile?email=amir.globerson%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="amir.globerson@gmail.com">Amir Globerson</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>9 Replies</span>


    </div>

    <a href="#BkgzMCVtPB-details-649" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="BkgzMCVtPB-details-649">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Generative neural models have improved dramatically recently. With this
              progress comes the risk that such models will be used to attack systems that rely on sensor data for
              authentication and anomaly detection. Many such learning systems are installed worldwide, protecting
              critical infrastructure or private data against malfunction and cyber attacks. We formulate the scenario
              of such an authentication system facing generative impersonation attacks, characterize it from a
              theoretical perspective and explore its practical implications. In particular, we ask fundamental
              theoretical questions in learning, statistics and information theory: How hard is it to detect a "fake
              reality"? How much data does the attacker need to collect before it can reliably generate
              nominally-looking artificial data? Are there optimal strategies for the attacker or the authenticator? We
              cast the problem as a maximin game, characterize the optimal strategy for both attacker and authenticator
              in the general case, and provide the optimal strategies in closed form for the case of Gaussian source
              distributions. Our analysis reveals the structure of the optimal attack and the relative importance of
              data collection for both authenticator and attacker. Based on these insights we design practical learning
              approaches and show that they result in models that are more robust to various attacks on real-world
              data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span
              class="note-content-value ">https://github.com/roymor1/OptimalStrategiesAgainstGenerativeAttacks</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=BkgzMCVtPB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="r1lGO0EKDH" data-number="1203">
    <h4>
      <a href="/forum?id=r1lGO0EKDH">
        GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding
      </a>


      <a href="/pdf?id=r1lGO0EKDH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=cd574%40cornell.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="cd574@cornell.edu">Chenhui Deng</a>, <a href="/profile?email=qzzhao%40mtu.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="qzzhao@mtu.edu">Zhiqiang Zhao</a>, <a
        href="/profile?email=yongyuw%40mtu.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="yongyuw@mtu.edu">Yongyu Wang</a>, <a href="/profile?email=zhiruz%40cornell.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="zhiruz@cornell.edu">Zhiru Zhang</a>, <a
        href="/profile?email=zfeng12%40stevens.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="zfeng12@stevens.edu">Zhuo Feng</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>11 Replies</span>


    </div>

    <a href="#r1lGO0EKDH-details-588" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="r1lGO0EKDH-details-588">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">A multi-level spectral approach to improving the quality and scalability
              of unsupervised graph embedding.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graph embedding techniques have been increasingly deployed in a multitude
              of different applications that involve learning on non-Euclidean data. However, existing graph embedding
              models either fail to incorporate node attribute information during training or suffer from node attribute
              noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high
              computational complexity and memory usage. In this paper we propose GraphZoom, a multi-level framework for
              improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first
              performs graph fusion to generate a new graph that effectively encodes the topology of the original graph
              and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs
              by merging nodes with high spectral similarities. GraphZoom allows any existing embedding methods to be
              applied to the coarsened graph, before it progressively refine the embeddings obtained at the coarsest
              level to increasingly finer graphs. We have evaluated our approach on a number of popular graph datasets
              for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase
              the classification accuracy and significantly accelerate the entire graph embedding process by up to
              <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0"
                ctxtmenu_counter="378" style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-mn class="mjx-n">
                    <mjx-c class="mjx-c34"></mjx-c>
                    <mjx-c class="mjx-c30"></mjx-c>
                    <mjx-c class="mjx-c2E"></mjx-c>
                    <mjx-c class="mjx-c38"></mjx-c>
                  </mjx-mn>
                  <mjx-mo class="mjx-n">
                    <mjx-c class="mjx-cD7"></mjx-c>
                  </mjx-mo>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mn>40.8</mn>
                    <mo>×</mo>
                  </math></mjx-assistive-mml>
              </mjx-container>, when compared to the state-of-the-art unsupervised embedding methods. </span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/cornell-zhang/GraphZoom</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">graph embedding, unsupervised learning, multi-level optimization, spectral
              graph theory</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=r1lGO0EKDH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="rklHqRVKvH" data-number="1281">
    <h4>
      <a href="/forum?id=rklHqRVKvH">
        Harnessing Structures for Value-Based Planning and Reinforcement Learning
      </a>


      <a href="/pdf?id=rklHqRVKvH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=yuzhe%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="yuzhe@mit.edu">Yuzhe Yang</a>, <a href="/profile?email=guozhang%40mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="guozhang@mit.edu">Guo Zhang</a>, <a
        href="/profile?email=zhixu%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="zhixu@mit.edu">Zhi Xu</a>, <a href="/profile?email=dina%40csail.mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="dina@csail.mit.edu">Dina Katabi</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>12 Replies</span>


    </div>

    <a href="#rklHqRVKvH-details-287" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="rklHqRVKvH-details-287">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep reinforcement learning, value-based reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We propose a generic framework that allows for exploiting the low-rank
              structure in both planning and deep reinforcement learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Value-based methods constitute a fundamental methodology in planning and
              deep reinforcement learning (RL). In this paper, we propose to exploit the underlying structures of the
              state-action value function, i.e., Q function, for both planning and deep RL. In particular, if the
              underlying system dynamics lead to some global structures of the Q function, one should be capable of
              inferring the function better by leveraging such structures. Specifically, we investigate the low-rank
              structure, which widely exists for big data matrices. We verify empirically the existence of low-rank Q
              functions in the context of control and deep RL tasks. As our key contribution, by leveraging Matrix
              Estimation (ME) techniques, we propose a general framework to exploit the underlying low-rank structure in
              Q functions. This leads to a more efficient planning procedure for classical control, and additionally, a
              simple scheme that can be applied to value-based RL techniques to consistently achieve better performance
              on "low-rank" tasks. Extensive experiments on control tasks and Atari games confirm the efficacy of our
              approach.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/YyzHarry/SV-RL</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=rklHqRVKvH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="S1gSj0NKvB" data-number="1317">
    <h4>
      <a href="/forum?id=S1gSj0NKvB">
        Comparing Rewinding and Fine-tuning in Neural Network Pruning
      </a>


      <a href="/pdf?id=S1gSj0NKvB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=renda%40csail.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="renda@csail.mit.edu">Alex Renda</a>, <a href="/profile?email=jfrankle%40csail.mit.edu"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="jfrankle@csail.mit.edu">Jonathan
        Frankle</a>, <a href="/profile?email=mcarbin%40csail.mit.edu" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="mcarbin@csail.mit.edu">Michael Carbin</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>9 Replies</span>


    </div>

    <a href="#S1gSj0NKvB-details-764" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="S1gSj0NKvB-details-764">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">Instead of fine-tuning after pruning, rewind weights or learning rate
              schedule to their values earlier in training and retrain from there to achieve higher accuracy when
              pruning neural networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many neural network pruning algorithms proceed in three steps: train the
              network to completion, remove unwanted structure to compress the network, and retrain the remaining
              structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned
              weights from their final trained values using a small fixed learning rate. In this paper, we compare
              fine-tuning to alternative retraining techniques. Weight rewinding (as proposed by Frankle et al.,
              (2019)), rewinds unpruned weights to their values from earlier in training and retrains them from there
              using the original training schedule. Learning rate rewinding (which we propose) trains the unpruned
              weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding
              techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches
              the accuracy and compression ratios of several more network-specific state-of-the-art techniques.
            </span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/lottery-ticket/rewinding-iclr20-public</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">pruning, sparsity, fine-tuning, lottery ticket</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=S1gSj0NKvB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="SJeD3CEFPH" data-number="1359">
    <h4>
      <a href="/forum?id=SJeD3CEFPH">
        Meta-Q-Learning
      </a>


      <a href="/pdf?id=SJeD3CEFPH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=rasool.fakoor%40mavs.uta.edu" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="rasool.fakoor@mavs.uta.edu">Rasool Fakoor</a>, <a
        href="/profile?email=pratikac%40seas.upenn.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="pratikac@seas.upenn.edu">Pratik Chaudhari</a>, <a href="/profile?email=soatto%40cs.ucla.edu"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="soatto@cs.ucla.edu">Stefano Soatto</a>,
      <a href="/profile?email=alex%40smola.org" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="alex@smola.org">Alexander J. Smola</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>14 Replies</span>


    </div>

    <a href="#SJeD3CEFPH-details-869" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="SJeD3CEFPH-details-869">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">meta reinforcement learning, propensity estimation, off-policy</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">MQL is a simple off-policy meta-RL algorithm that recycles data from the
              meta-training replay buffer to adapt to new tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm
              for meta-Reinforcement Learning (meta-RL). MQL builds upon three simple ideas. First, we show that
              Q-learning is competitive with state-of-the-art meta-RL algorithms if given access to a context variable
              that is a representation of the past trajectory. Second, a multi-task objective to maximize the average
              reward across the training tasks is an effective method to meta-train RL policies. Third, past data from
              the meta-training replay buffer can be recycled to adapt the policy on a new task using off-policy
              updates. MQL draws upon ideas in propensity estimation to do so and thereby amplifies the amount of
              available data for adaptation. Experiments on standard continuous-control benchmarks suggest that MQL
              compares favorably with the state of the art in meta-RL.</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=SJeD3CEFPH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="Ske31kBtPr" data-number="1483">
    <h4>
      <a href="/forum?id=Ske31kBtPr">
        Mathematical Reasoning in Latent Space
      </a>


      <a href="/pdf?id=Ske31kBtPr" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=ldennis%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="ldennis@google.com">Dennis Lee</a>, <a href="/profile?email=szegedy%40google.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="szegedy@google.com">Christian Szegedy</a>, <a
        href="/profile?email=mrabe%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="mrabe@google.com">Markus Rabe</a>, <a href="/profile?email=smoos%40google.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="smoos@google.com">Sarah Loos</a>, <a
        href="/profile?email=kbk%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="kbk@google.com">Kshitij Bansal</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>11 Replies</span>


    </div>

    <a href="#Ske31kBtPr-details-79" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="Ske31kBtPr-details-79">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">machine learning, formal reasoning</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">Learning to reason about higher order logic formulas in the latent
              space.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We design and conduct a simple experiment to study whether neural networks
              can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of
              rewrites (i.e. transformations) that can be successfully performed on a statement represents essential
              semantic features of the statement. We can compress this information by embedding the formula in a vector
              space, such that the vector associated with a statement can be used to predict whether a statement can be
              rewritten by other theorems. Predicting the embedding of a formula generated by some rewrite rule is
              naturally viewed as approximate reasoning in the latent space. In order to measure the effectiveness of
              this reasoning, we perform approximate deduction sequences in the latent space and use the resulting
              embedding to inform the semantic features of the corresponding formal statement (which is obtained by
              performing the corresponding rewrite sequence using real formulas). Our experiments show that graph neural
              networks can make non-trivial predictions about the rewrite-success of statements, even when they
              propagate predicted latent representations for several steps. Since our corpus of mathematical formulas
              includes a wide variety of mathematical disciplines, this experiment is a strong indicator for the
              feasibility of deduction in latent space in general.</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=Ske31kBtPr&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="r1eBeyHFDH" data-number="1504">
    <h4>
      <a href="/forum?id=r1eBeyHFDH">
        A Theory of Usable Information under Computational Constraints
      </a>


      <a href="/pdf?id=r1eBeyHFDH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=xuyilun%40pku.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="xuyilun@pku.edu.cn">Yilun Xu</a>, <a href="/profile?email=sjzhao%40stanford.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="sjzhao@stanford.edu">Shengjia Zhao</a>, <a
        href="/profile?email=tsong%40cs.stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="tsong@cs.stanford.edu">Jiaming Song</a>, <a href="/profile?email=russell.sb.nebel%40gmail.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="russell.sb.nebel@gmail.com">Russell
        Stewart</a>, <a href="/profile?email=ermon%40cs.stanford.edu" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="ermon@cs.stanford.edu">Stefano Ermon</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 08 Jul 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>6 Replies</span>


    </div>

    <a href="#r1eBeyHFDH-details-630" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="r1eBeyHFDH-details-630">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a new framework for reasoning about information in complex
              systems. Our foundation is based on a variational extension of Shannon’s information theory that takes
              into account the modeling power and computational constraints of the observer. The resulting predictive
              V-information encompasses mutual information and other notions of informativeness such as the coefficient
              of determination. Unlike Shannon’s mutual information and in violation of the data processing inequality,
              V-information can be created through computation. This is consistent with deep neural networks extracting
              hierarchies of progressively more informative features in representation learning. Additionally, we show
              that by incorporating computational constraints, V-information can be reliably estimated from data even in
              high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive V-information is more
              effective than mutual information for structure learning and fair representation learning. Codes are
              available at https://github.com/Newbeeer/V-information .</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/Newbeeer/V-information</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=r1eBeyHFDH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="rygixkHKDH" data-number="1519">
    <h4>
      <a href="/forum?id=rygixkHKDH">
        Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning
      </a>


      <a href="/pdf?id=rygixkHKDH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=qingqu1006%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="qingqu1006@gmail.com">Qing Qu</a>, <a href="/profile?email=ysz%40berkeley.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="ysz@berkeley.edu">Yuexiang Zhai</a>, <a
        href="/profile?email=xli%40ee.cuhk.edu.hk" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="xli@ee.cuhk.edu.hk">Xiao Li</a>, <a href="/profile?email=yqz.zhang%40gmail.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="yqz.zhang@gmail.com">Yuqian Zhang</a>, <a
        href="/profile?email=zzhu29%40jhu.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="zzhu29@jhu.edu">Zhihui Zhu</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>9 Replies</span>


    </div>

    <a href="#rygixkHKDH-details-936" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="rygixkHKDH-details-936">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning overcomplete representations finds many applications in machine
              learning and data analytics. In the past decade, despite the empirical success of heuristic methods,
              theoretical understandings and explanations of these algorithms are still far from satisfactory. In this
              work, we provide new theoretical insights for several important representation learning problems: learning
              (i) sparsely used overcomplete dictionaries and (ii) convolutional dictionaries. We formulate these
              problems as <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation"
                tabindex="0" ctxtmenu_counter="379" style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-msup>
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c2113"></mjx-c>
                    </mjx-mi>
                    <mjx-script style="vertical-align: 0.363em;">
                      <mjx-mn class="mjx-n" size="s">
                        <mjx-c class="mjx-c34"></mjx-c>
                      </mjx-mn>
                    </mjx-script>
                  </mjx-msup>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <msup>
                      <mi>ℓ</mi>
                      <mn>4</mn>
                    </msup>
                  </math></mjx-assistive-mml>
              </mjx-container>-norm optimization problems over the sphere and study the geometric properties of their
              nonconvex optimization landscapes. For both problems, we show the nonconvex objective has benign (global)
              geometric structures, which enable the development of efficient optimization methods finding the target
              solutions. Finally, our theoretical results are justified by numerical simulations.
            </span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">dictionary learning, sparse representations, nonconvex optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=rygixkHKDH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="ryghZJBKPS" data-number="1557">
    <h4>
      <a href="/forum?id=ryghZJBKPS">
        Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds
      </a>


      <a href="/pdf?id=ryghZJBKPS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=jordanta%40cs.princeton.edu" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="jordanta@cs.princeton.edu">Jordan T. Ash</a>, <a
        href="/profile?email=chichengz%40cs.arizona.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="chichengz@cs.arizona.edu">Chicheng Zhang</a>, <a
        href="/profile?email=akshay.krishnamurthy%40microsoft.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="akshay.krishnamurthy@microsoft.com">Akshay Krishnamurthy</a>, <a
        href="/profile?email=jcl%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="jcl@microsoft.com">John Langford</a>, <a href="/profile?email=alekha%40microsoft.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="alekha@microsoft.com">Alekh Agarwal</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>8 Replies</span>


    </div>

    <a href="#ryghZJBKPS-details-612" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="ryghZJBKPS-details-612">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We introduce a new batch active learning algorithm that's robust to model
              architecture, batch size, and dataset.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We design a new algorithm for batch active learning with deep neural
              network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples
              groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space,
              a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected
              batch. Crucially, BADGE trades off between diversity and uncertainty without requiring any hand-tuned
              hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures,
              BADGE consistently performs as well or better, making it a useful option for real world active learning
              problems.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, active learning, batch active learning</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=ryghZJBKPS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="H1gDNyrKDS" data-number="1657">
    <h4>
      <a href="/forum?id=H1gDNyrKDS">
        Understanding and Robustifying Differentiable Architecture Search
      </a>


      <a href="/pdf?id=H1gDNyrKDS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=zelaa%40cs.uni-freiburg.de" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="zelaa@cs.uni-freiburg.de">Arber Zela</a>, <a
        href="/profile?email=thomas.elsken%40de.bosch.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="thomas.elsken@de.bosch.com">Thomas Elsken</a>, <a
        href="/profile?email=saikiat%40cs.uni-freiburg.de" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="saikiat@cs.uni-freiburg.de">Tonmoy Saikia</a>, <a
        href="/profile?email=marrakch%40cs.uni-freiburg.de" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="marrakch@cs.uni-freiburg.de">Yassine Marrakchi</a>, <a
        href="/profile?email=brox%40cs.uni-freiburg.de" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="brox@cs.uni-freiburg.de">Thomas Brox</a>, <a href="/profile?email=fh%40cs.uni-freiburg.de"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="fh@cs.uni-freiburg.de">Frank Hutter</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>11 Replies</span>


    </div>

    <a href="#H1gDNyrKDS-details-429" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="H1gDNyrKDS-details-429">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural Architecture Search, AutoML, AutoDL, Deep Learning, Computer
              Vision</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We study the failure modes of DARTS (Differentiable Architecture Search)
              by looking at the eigenvalues of the Hessian of validation loss w.r.t. the architecture and propose
              robustifications based on our analysis.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Differentiable Architecture Search (DARTS) has attracted a lot of
              attention due to its simplicity and small search costs achieved by a continuous relaxation and an
              approximation of the resulting bi-level optimization problem. However, DARTS does not work robustly for
              new problems: we identify a wide range of search spaces for which DARTS yields degenerate architectures
              with very poor test performance. We study this failure mode and show that, while DARTS successfully
              minimizes validation loss, the found solutions generalize poorly when they coincide with high validation
              loss curvature in the architecture space. We show that by adding one of various types of regularization we
              can robustify DARTS to find solutions with less curvature and better generalization properties. Based on
              these observations, we propose several simple variations of DARTS that perform substantially more robustly
              in practice. Our observations are robust across five search spaces on three image classification tasks and
              also hold for the very different domains of disparity estimation (a dense regression task) and language
              modelling.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/automl/RobustDARTS</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=H1gDNyrKDS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="ryxdEkHtPS" data-number="1660">
    <h4>
      <a href="/forum?id=ryxdEkHtPS">
        A Closer Look at Deep Policy Gradients
      </a>


      <a href="/pdf?id=ryxdEkHtPS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=ailyas%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="ailyas@mit.edu">Andrew Ilyas</a>, <a href="/profile?email=engstrom%40mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="engstrom@mit.edu">Logan Engstrom</a>, <a
        href="/profile?email=shibani%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="shibani@mit.edu">Shibani Santurkar</a>, <a href="/profile?email=tsipras%40mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="tsipras@mit.edu">Dimitris Tsipras</a>, <a
        href="/profile?email=firdaus.janoos%40twosigma.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="firdaus.janoos@twosigma.com">Firdaus Janoos</a>, <a
        href="/profile?email=rudolph%40csail.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="rudolph@csail.mit.edu">Larry Rudolph</a>, <a href="/profile?email=madry%40mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="madry@mit.edu">Aleksander Madry</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 03 Apr 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>8 Replies</span>


    </div>

    <a href="#ryxdEkHtPS-details-379" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="ryxdEkHtPS-details-379">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep policy gradient methods, deep reinforcement learning, trpo,
              ppo</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value "> We study how the behavior of deep policy gradient algorithms reflects the
              conceptual framework motivating their development. To this end, we propose a fine-grained analysis of
              state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction,
              and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often
              deviates from what their motivating framework would predict: surrogate rewards do not match the true
              reward landscape, learned value estimators fail to fit the true value function, and gradient estimates
              poorly correlate with the "true" gradient. The mismatch between predicted and empirical behavior we
              uncover highlights our poor understanding of current methods, and indicates the need to move beyond
              current benchmark-centric evaluation methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=ryxdEkHtPS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="r1etN1rtPB" data-number="1661">
    <h4>
      <a href="/forum?id=r1etN1rtPB">
        Implementation Matters in Deep RL: A Case Study on PPO and TRPO
      </a>


      <a href="/pdf?id=r1etN1rtPB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=ailyas%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="ailyas@mit.edu">Logan Engstrom</a>, <a href="/profile?email=engstrom%40mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="engstrom@mit.edu">Andrew Ilyas</a>, <a
        href="/profile?email=shibani%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="shibani@mit.edu">Shibani Santurkar</a>, <a href="/profile?email=tsipras%40mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="tsipras@mit.edu">Dimitris Tsipras</a>, <a
        href="/profile?email=firdaus.janoos%40twosigma.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="firdaus.janoos@twosigma.com">Firdaus Janoos</a>, <a
        href="/profile?email=rudolph%40csail.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="rudolph@csail.mit.edu">Larry Rudolph</a>, <a href="/profile?email=madry%40mit.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="madry@mit.edu">Aleksander Madry</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 14 Apr 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>16 Replies</span>


    </div>

    <a href="#r1etN1rtPB-details-506" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="r1etN1rtPB-details-506">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep policy gradient methods, deep reinforcement learning, trpo,
              ppo</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the roots of algorithmic progress in deep policy gradient
              algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust
              Region Policy Optimization (TRPO). Specifically, we investigate the consequences of "code-level
              optimizations:" algorithm augmentations found only in implementations or described as auxiliary details to
              the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact
              on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative
              reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the
              difficulty, and importance, of attributing performance gains in deep reinforcement learning.
            </span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/implementation-matters/code-for-paper</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=r1etN1rtPB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="BJeAHkrYDS" data-number="1711">
    <h4>
      <a href="/forum?id=BJeAHkrYDS">
        Fast Task Inference with Variational Intrinsic Successor Features
      </a>


      <a href="/pdf?id=BJeAHkrYDS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=stevenhansen%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="stevenhansen@google.com">Steven Hansen</a>, <a href="/profile?email=wdabney%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="wdabney@google.com">Will Dabney</a>, <a
        href="/profile?email=andrebarreto%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="andrebarreto@google.com">Andre Barreto</a>, <a href="/profile?email=dwf%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="dwf@google.com">David Warde-Farley</a>,
      <a href="/profile?email=tvdwiele%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="tvdwiele@gmail.com">Tom Van de Wiele</a>, <a href="/profile?email=vmnih%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="vmnih@google.com">Volodymyr Mnih</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>8 Replies</span>


    </div>

    <a href="#BJeAHkrYDS-details-677" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="BJeAHkrYDS-details-677">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We introduce Variational Intrinsic Successor FeatuRes (VISR), a novel
              algorithm which learns controllable features that can be leveraged to provide fast task inference through
              the successor features framework.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">It has been established that diverse behaviors spanning the controllable
              subspace of a Markov decision process can be trained by rewarding a policy for being distinguishable from
              other policies. However, one limitation of this formulation is the difficulty to generalize beyond the
              finite set of behaviors being explicitly learned, as may be needed in subsequent tasks. Successor features
              provide an appealing solution to this generalization problem, but require defining the reward function as
              linear in some grounded feature space. In this paper, we show that these two techniques can be combined,
              and that each method solves the other's primary limitation. To do so we introduce Variational Intrinsic
              Successor FeatuRes (VISR), a novel algorithm which learns controllable features that can be leveraged to
              provide enhanced generalization and fast task inference through the successor features framework. We
              empirically validate VISR on the full Atari suite, in a novel setup wherein the rewards are only exposed
              briefly after a long unsupervised phase. Achieving human-level performance on 12 games and beating all
              baselines, we believe VISR represents a step towards agents that rapidly learn from limited
              feedback.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Variational Intrinsic Control, Successor
              Features</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=BJeAHkrYDS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="rkeZIJBYvr" data-number="1719">
    <h4>
      <a href="/forum?id=rkeZIJBYvr">
        Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks
      </a>


      <a href="/pdf?id=rkeZIJBYvr" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=haebeom.lee%40kaist.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="haebeom.lee@kaist.ac.kr">Hae Beom Lee</a>, <a href="/profile?email=hayeon926%40kaist.ac.kr"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="hayeon926@kaist.ac.kr">Hayeon Lee</a>, <a
        href="/profile?email=donghyun.na%40kaist.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="donghyun.na@kaist.ac.kr">Donghyun Na</a>, <a href="/profile?email=shkim%40aitrics.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="shkim@aitrics.com">Saehoon Kim</a>, <a
        href="/profile?email=mike_seop%40aitrics.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="mike_seop@aitrics.com">Minseop Park</a>, <a href="/profile?email=eunhoy%40kaist.ac.kr"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="eunhoy@kaist.ac.kr">Eunho Yang</a>, <a
        href="/profile?email=sjhwang82%40kaist.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="sjhwang82@kaist.ac.kr">Sung Ju Hwang</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>8 Replies</span>


    </div>

    <a href="#rkeZIJBYvr-details-587" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="rkeZIJBYvr-details-587">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">meta-learning, few-shot learning, Bayesian neural network, variational
              inference, learning to learn, imbalanced and out-of-distribution tasks for few-shot learning</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">A novel meta-learning model that adaptively balances the effect of the
              meta-learning and task-specific learning, and also class-specific learning within each task.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While tasks could come with varying the number of instances and classes in
              realistic settings, the existing meta-learning approaches for few-shot classification assume that number
              of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the
              meta-knowledge across all the tasks, even when the number of instances per task and class largely varies.
              Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may
              have less usefulness depending on the task relatedness. To overcome these limitations, we propose a novel
              meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning
              within each task. Through the learning of the balancing variables, we can decide whether to obtain a
              solution by relying on the meta-knowledge or task-specific learning. We formulate this objective into a
              Bayesian inference framework and tackle it using variational inference. We validate our Bayesian
              Task-Adaptive Meta-Learning (Bayesian TAML) on two realistic task- and class-imbalanced datasets, on which
              it significantly outperforms existing meta-learning approaches. Further ablation study confirms the
              effectiveness of each balancing component and the Bayesian learning framework. </span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/haebeom-lee/l2b</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=rkeZIJBYvr&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="S1eALyrYDH" data-number="1749">
    <h4>
      <a href="/forum?id=S1eALyrYDH">
        RNA Secondary Structure Prediction By Learning Unrolled Algorithms
      </a>


      <a href="/pdf?id=S1eALyrYDH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=xinshi.chen%40gatech.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="xinshi.chen@gatech.edu">Xinshi Chen</a>, <a href="/profile?email=yu.li%40kaust.edu.sa"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="yu.li@kaust.edu.sa">Yu Li</a>, <a
        href="/profile?email=ramzan.umarov%40kaust.edu.sa" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="ramzan.umarov@kaust.edu.sa">Ramzan Umarov</a>, <a
        href="/profile?email=xin.gao%40kaust.edu.sa" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="xin.gao@kaust.edu.sa">Xin Gao</a>, <a href="/profile?email=lsong%40cc.gatech.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="lsong@cc.gatech.edu">Le Song</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>19 Replies</span>


    </div>

    <a href="#S1eALyrYDH-details-289" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="S1eALyrYDH-details-289">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">A DL model for RNA secondary structure prediction, which uses an unrolled
              algorithm in the architecture to enforce constraints.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we propose an end-to-end deep learning model, called
              E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent
              constraints in the problem. The key idea of E2Efold is to directly predict the RNA base-pairing matrix,
              and use an unrolled algorithm for constrained programming as the template for deep architectures to
              enforce constraints. With comprehensive experiments on benchmark datasets, we demonstrate the superior
              performance of E2Efold: it predicts significantly better structures compared to previous SOTA (especially
              for pseudoknotted structures), while being as efficient as the fastest algorithms in terms of inference
              time.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">RNA secondary structure prediction, learning algorithm, deep architecture
              design, computational biology</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/ml4bio/e2efold</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=S1eALyrYDH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="BJlQtJSKDB" data-number="1835">
    <h4>
      <a href="/forum?id=BJlQtJSKDB">
        Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search
      </a>


      <a href="/pdf?id=BJlQtJSKDB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=anjiliu219%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="anjiliu219@gmail.com">Anji Liu</a>, <a href="/profile?email=chenjianshu%40gmail.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="chenjianshu@gmail.com">Jianshu Chen</a>, <a
        href="/profile?email=yumingze%40kuaishou.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="yumingze@kuaishou.com">Mingze Yu</a>, <a href="/profile?email=zhaiyu%40kuaishou.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="zhaiyu@kuaishou.com">Yu Zhai</a>, <a
        href="/profile?email=zhouxuewen%40kuaishou.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="zhouxuewen@kuaishou.com">Xuewen Zhou</a>, <a href="/profile?email=ji.liu.uwisc%40gmail.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="ji.liu.uwisc@gmail.com">Ji Liu</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>13 Replies</span>


    </div>

    <a href="#BJlQtJSKDB-details-634" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="BJlQtJSKDB-details-634">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We developed an effective parallel UCT algorithm that achieves linear
              speedup and suffers negligible performance loss.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Monte Carlo Tree Search (MCTS) algorithms have achieved great success on
              many challenging benchmarks (e.g., Computer Go). However, they generally require a large number of
              rollouts, making their applications costly. Furthermore, it is also extremely challenging to parallelize
              MCTS due to its inherent sequential nature: each rollout heavily relies on the statistics (e.g., node
              visitation counts) estimated from previous simulations to achieve an effective exploration-exploitation
              tradeoff. In spite of these difficulties, we develop an algorithm, WU-UCT, to effectively parallelize
              MCTS, which achieves linear speedup and exhibits only limited performance loss with an increasing number
              of workers. The key idea in WU-UCT is a set of statistics that we introduce to track the number of
              on-going yet incomplete simulation queries (named as unobserved samples). These statistics are used to
              modify the UCT tree policy in the selection steps in a principled manner to retain effective
              exploration-exploitation tradeoff when we parallelize the most time-consuming expansion and simulation
              steps. Experiments on a proprietary benchmark and the Atari Game benchmark demonstrate the linear speedup
              and the superior performance of WU-UCT comparing to existing techniques.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">parallel Monte Carlo Tree Search (MCTS), Upper Confidence bound for Trees
              (UCT), Reinforcement Learning (RL)</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=BJlQtJSKDB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="BygXFkSYDH" data-number="1836">
    <h4>
      <a href="/forum?id=BygXFkSYDH">
        Target-Embedding Autoencoders for Supervised Representation Learning
      </a>


      <a href="/pdf?id=BygXFkSYDH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=daniel.jarrett%40eng.ox.ac.uk" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="daniel.jarrett@eng.ox.ac.uk">Daniel Jarrett</a>, <a
        href="/profile?email=mv472%40damtp.cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="mv472@damtp.cam.ac.uk">Mihaela van der Schaar</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>24 Replies</span>


    </div>

    <a href="#BygXFkSYDH-details-772" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="BygXFkSYDH-details-772">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Autoencoder-based learning has emerged as a staple for disciplining
              representations in unsupervised and semi-supervised settings. This paper analyzes a framework for
              improving generalization in a purely supervised setting, where the target space is high-dimensional. We
              motivate and formalize the general framework of target-embedding autoencoders (TEA) for supervised
              prediction, learning intermediate latent representations jointly optimized to be both predictable from
              features as well as predictive of targets---encoding the prior that variations in targets are driven by a
              compact set of underlying factors. As our theoretical contribution, we provide a guarantee of
              generalization for linear TEAs by demonstrating uniform stability, interpreting the benefit of the
              auxiliary reconstruction task as a form of regularization. As our empirical contribution, we extend
              validation of this approach beyond existing static classification applications to multivariate sequence
              forecasting, verifying their advantage on both linear and nonlinear recurrent architectures---thereby
              underscoring the further generality of this framework beyond feedforward instantiations.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">autoencoders, supervised learning, representation learning,
              target-embedding, label-embedding</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=BygXFkSYDH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="rkgNKkHtvB" data-number="1838">
    <h4>
      <a href="/forum?id=rkgNKkHtvB">
        Reformer: The Efficient Transformer
      </a>


      <a href="/pdf?id=rkgNKkHtvB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=kitaev%40cs.berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="kitaev@cs.berkeley.edu">Nikita Kitaev</a>, <a href="/profile?email=lukaszkaiser%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="lukaszkaiser@google.com">Lukasz
        Kaiser</a>, <a href="/profile?email=levskaya%40google.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="levskaya@google.com">Anselm Levskaya</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>19 Replies</span>


    </div>

    <a href="#rkgNKkHtvB-details-150" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="rkgNKkHtvB-details-150">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">attention, locality sensitive hashing, reversible layers</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">Efficient Transformer with locality-sensitive hashing and reversible
              layers</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Large Transformer models routinely achieve state-of-the-art results on
              a number of tasks but training these models can be prohibitively costly,
              especially on long sequences. We introduce two techniques to improve
              the efficiency of Transformers. For one, we replace dot-product attention
              by one that uses locality-sensitive hashing, changing its complexity
              from O(<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0"
                ctxtmenu_counter="380" style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-msup>
                    <mjx-mi class="mjx-i">
                      <mjx-c class="mjx-c1D43F TEX-I"></mjx-c>
                    </mjx-mi>
                    <mjx-script style="vertical-align: 0.363em;">
                      <mjx-mn class="mjx-n" size="s">
                        <mjx-c class="mjx-c32"></mjx-c>
                      </mjx-mn>
                    </mjx-script>
                  </mjx-msup>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <msup>
                      <mi>L</mi>
                      <mn>2</mn>
                    </msup>
                  </math></mjx-assistive-mml>
              </mjx-container>) to O(<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation"
                tabindex="0" ctxtmenu_counter="381" style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-mi class="mjx-i">
                    <mjx-c class="mjx-c1D43F TEX-I"></mjx-c>
                  </mjx-mi>
                  <mjx-mi class="mjx-n" space="2">
                    <mjx-c class="mjx-c6C"></mjx-c>
                    <mjx-c class="mjx-c6F"></mjx-c>
                    <mjx-c class="mjx-c67"></mjx-c>
                  </mjx-mi>
                  <mjx-mo class="mjx-n">
                    <mjx-c class="mjx-c2061"></mjx-c>
                  </mjx-mo>
                  <mjx-mi class="mjx-i" space="2">
                    <mjx-c class="mjx-c1D43F TEX-I"></mjx-c>
                  </mjx-mi>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>L</mi>
                    <mi>log</mi>
                    <mo data-mjx-texclass="NONE">⁡</mo>
                    <mi>L</mi>
                  </math></mjx-assistive-mml>
              </mjx-container>), where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML"
                role="presentation" tabindex="0" ctxtmenu_counter="382" style="font-size: 113.1%; position: relative;">
                <mjx-math class="MJX-TEX" aria-hidden="true">
                  <mjx-mi class="mjx-i">
                    <mjx-c class="mjx-c1D43F TEX-I"></mjx-c>
                  </mjx-mi>
                </mjx-math>
                <mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>L</mi>
                  </math></mjx-assistive-mml>
              </mjx-container> is the length of the sequence.
              Furthermore, we use reversible residual layers instead of the standard
              residuals, which allows storing activations only once in the training
              process instead of N times, where N is the number of layers.
              The resulting model, the Reformer, performs on par with Transformer models
              while being much more memory-efficient and much faster on long sequences.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/google/trax/tree/master/trax/models/reformer</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=rkgNKkHtvB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="rklr9kHFDB" data-number="1876">
    <h4>
      <a href="/forum?id=rklr9kHFDB">
        Rotation-invariant clustering of neuronal responses in primary visual cortex
      </a>


      <a href="/pdf?id=rklr9kHFDB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=ivan.ustyuzhaninov%40bethgelab.org" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="ivan.ustyuzhaninov@bethgelab.org">Ivan Ustyuzhaninov</a>, <a
        href="/profile?email=santiago.cadena%40bethgelab.org" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="santiago.cadena@bethgelab.org">Santiago A. Cadena</a>, <a
        href="/profile?email=froudara%40bcm.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="froudara@bcm.edu">Emmanouil Froudarakis</a>, <a href="/profile?email=paul.fahey%40bcm.edu"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="paul.fahey@bcm.edu">Paul G. Fahey</a>, <a
        href="/profile?email=eywalker%40bcm.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="eywalker@bcm.edu">Edgar Y. Walker</a>, <a href="/profile?email=ecobos%40bcm.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="ecobos@bcm.edu">Erick Cobos</a>, <a
        href="/profile?email=reimer%40bcm.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="reimer@bcm.edu">Jacob Reimer</a>, <a href="/profile?email=fabian.sinz%40bcm.edu" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="fabian.sinz@bcm.edu">Fabian H. Sinz</a>, <a
        href="/profile?email=astolias%40bcm.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="astolias@bcm.edu">Andreas S. Tolias</a>, <a href="/profile?email=matthias%40bethgelab.org"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="matthias@bethgelab.org">Matthias
        Bethge</a>, <a href="/profile?email=alexander.ecker%40uni-tuebingen.de" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="alexander.ecker@uni-tuebingen.de">Alexander S. Ecker</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>11 Replies</span>


    </div>

    <a href="#rklr9kHFDB-details-630" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="rklr9kHFDB-details-630">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We classify mouse V1 neurons into putative functional cell types based on
              their representations in a CNN predicting neural responses</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Similar to a convolutional neural network (CNN), the mammalian retina
              encodes visual information into several dozen nonlinear feature maps, each formed by one ganglion cell
              type that tiles the visual space in an approximately shift-equivariant manner. Whether such organization
              into distinct cell types is maintained at the level of cortical image processing is an open question.
              Predictive models building upon convolutional features have been shown to provide state-of-the-art
              performance, and have recently been extended to include rotation equivariance in order to account for the
              orientation selectivity of V1 neurons. However, generally no direct correspondence between CNN feature
              maps and groups of individual neurons emerges in these models, thus rendering it an open question whether
              V1 neurons form distinct functional clusters. Here we build upon the rotation-equivariant representation
              of a CNN-based V1 model and propose a methodology for clustering the representations of neurons in this
              model to find functional cell types independent of preferred orientations of the neurons. We apply this
              method to a dataset of 6000 neurons and visualize the preferred stimuli of the resulting clusters. Our
              results highlight the range of non-linear computations in mouse V1.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">computational neuroscience, neural system identification, functional cell
              types, deep learning, rotational equivariance</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=rklr9kHFDB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="S1g2skStPB" data-number="1929">
    <h4>
      <a href="/forum?id=S1g2skStPB">
        Causal Discovery with Reinforcement Learning
      </a>


      <a href="/pdf?id=S1g2skStPB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=zhushengyu%40huawei.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="zhushengyu@huawei.com">Shengyu Zhu</a>, <a href="/profile?email=ignavierng%40cs.toronto.edu"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="ignavierng@cs.toronto.edu">Ignavier
        Ng</a>, <a href="/profile?email=chenzhitang2%40huawei.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="chenzhitang2@huawei.com">Zhitang Chen</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>13 Replies</span>


    </div>

    <a href="#S1g2skStPB-details-675" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="S1g2skStPB-details-675">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">causal discovery, structure learning, reinforcement learning, directed
              acyclic graph</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We apply reinforcement learning to score-based causal discovery and
              achieve promising results on both synthetic and real datasets</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Discovering causal structure among a set of variables is a fundamental
              problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local
              heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While
              these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and
              certain model assumptions, they are less satisfactory in practice due to finite data and possible
              violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to
              use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model
              takes observable data as input and generates graph adjacency matrices that are used to compute rewards.
              The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity.
              In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search
              strategy and our final output would be the graph, among all graphs generated during training, that
              achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the
              proposed approach not only has an improved search ability but also allows for a flexible score function
              under the acyclicity constraint. </span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=S1g2skStPB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="rkg6sJHYDr" data-number="1931">
    <h4>
      <a href="/forum?id=rkg6sJHYDr">
        Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems
      </a>


      <a href="/pdf?id=rkg6sJHYDr" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=chris.reinke%40inria.fr" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="chris.reinke@inria.fr">Chris Reinke</a>, <a href="/profile?email=mayalen.etcheverry%40inria.fr"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="mayalen.etcheverry@inria.fr">Mayalen
        Etcheverry</a>, <a href="/profile?email=chris.reinke%40inria.fr" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="chris.reinke@inria.fr">Pierre-Yves Oudeyer</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>9 Replies</span>


    </div>

    <a href="#rkg6sJHYDr-details-621" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="rkg6sJHYDr-details-621">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We study how an unsupervised exploration and feature learning approach
              addresses efficiently a new problem: automatic discovery of diverse self-organized patterns in high-dim
              complex systems such as the game of life.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In many complex dynamical systems, artificial or natural, one can observe
              self-organization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL),
              have been widely used as abstract models enabling the study of various aspects of self-organization and
              morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized
              patterns in such models have so far relied on manual tuning of parameters and initial states, and on the
              human eye to identify interesting patterns. In this paper, we formulate the problem of automated discovery
              of diverse self-organized patterns in such high-dimensional complex dynamical systems, as well as a
              framework for experimentation and evaluation. Using a continuous GOL as a testbed, we show that recent
              intrinsically-motivated machine learning algorithms (POP-IMGEPs), initially developed for learning of
              inverse models in robotics, can be transposed and used in this novel application area. These algorithms
              combine intrinsically-motivated goal exploration and unsupervised learning of goal space representations.
              Goal space representations describe the interesting features of patterns for which diverse variations
              should be discovered. In particular, we compare various approaches to define and learn goal space
              representations from the perspective of discovering diverse spatially localized patterns. Moreover, we
              introduce an extension of a state-of-the-art POP-IMGEP algorithm which incrementally learns a goal
              representation using a deep auto-encoder, and the use of CPPN primitives for generating initialization
              parameters. We show that it is more efficient than several baselines and equally efficient as a system
              pre-trained on a hand-made database of patterns identified by human experts.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://automated-discovery.github.io/</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, unsupervised Learning, self-organization,
              game-of-life</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=rkg6sJHYDr&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="S1xWh1rYwB" data-number="1940">
    <h4>
      <a href="/forum?id=S1xWh1rYwB">
        Restricting the Flow: Information Bottlenecks for Attribution
      </a>


      <a href="/pdf?id=S1xWh1rYwB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=karl.schulz%40tum.de" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="karl.schulz@tum.de">Karl Schulz</a>, <a href="/profile?email=leon.sixt%40fu-berlin.de"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="leon.sixt@fu-berlin.de">Leon Sixt</a>, <a
        href="/profile?email=tombari%40in.tum.de" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="tombari@in.tum.de">Federico Tombari</a>, <a href="/profile?email=tim.landgraf%40fu-berlin.de"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="tim.landgraf@fu-berlin.de">Tim
        Landgraf</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 08 May 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>13 Replies</span>


    </div>

    <a href="#S1xWh1rYwB-details-159" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="S1xWh1rYwB-details-159">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We apply the informational bottleneck concept to attribution.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Attribution methods provide insights into the decision-making of machine
              learning models like artificial neural networks. For a given input sample, they assign a relevance score
              to each individual input variable, such as the pixels of an image. In this work, we adopt the information
              bottleneck concept for attribution. By adding noise to intermediate feature maps, we restrict the flow of
              information and can quantify (in bits) how much information image regions provide. We compare our method
              against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods
              outperform all baselines in five out of six settings. The method’s information-theoretic foundation
              provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored
              close to zero are not necessary for the network's decision. </span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/BioroboticsLab/IBA-paper-code</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Attribution, Informational Bottleneck, Interpretable Machine Learning,
              Explainable AI</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=S1xWh1rYwB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="BJgNJgSFPS" data-number="2058">
    <h4>
      <a href="/forum?id=BJgNJgSFPS">
        Building Deep Equivariant Capsule Networks
      </a>


      <a href="/pdf?id=BJgNJgSFPS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=vsairaam%40sssihl.edu.in" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="vsairaam@sssihl.edu.in">Sai Raam Venkataraman</a>, <a
        href="/profile?email=sbalasubramanian%40sssihl.edu.in" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="sbalasubramanian@sssihl.edu.in">S. Balasubramanian</a>, <a
        href="/profile?email=rraghunathasarma%40sssihl.edu.in" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="rraghunathasarma@sssihl.edu.in">R. Raghunatha Sarma</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>13 Replies</span>


    </div>

    <a href="#BJgNJgSFPS-details-676" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="BJgNJgSFPS-details-676">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Capsule networks, equivariance</span>
          </li>
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">A new scalable, group-equivariant model for capsule networks that
              preserves compositionality under transformations, and is empirically more transformation-robust to older
              capsule network models.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Capsule networks are constrained by the parameter-expensive nature of
              their layers, and the general lack of provable equivariance guarantees. We present a variation of capsule
              networks that aims to remedy this. We identify that learning all pair-wise part-whole relationships
              between capsules of successive layers is inefficient. Further, we also realise that the choice of
              prediction networks and the routing mechanism are both key to equivariance. Based on these, we propose an
              alternative framework for capsule networks that learns to projectively encode the manifold of
              pose-variations, termed the space-of-variation (SOV), for every capsule-type of each layer. This is done
              using a trainable, equivariant function defined over a grid of group-transformations. Thus, the
              prediction-phase of routing involves projection into the SOV of a deeper capsule using the corresponding
              function. As a specific instantiation of this idea, and also in order to reap the benefits of increased
              parameter-sharing, we use type-homogeneous group-equivariant convolutions of shallower capsules in this
              phase. We also introduce an equivariant routing mechanism based on degree-centrality. We show that this
              particular instance of our general model is equivariant, and hence preserves the compositional
              representation of an input under transformations. We conduct several experiments on standard
              object-classification datasets that showcase the increased transformation-robustness, as well as general
              performance, of our model to several capsule baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/AnonymousCapsuleSOVNET/SOVNET</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=BJgNJgSFPS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="Bkl5kxrKDr" data-number="2072">
    <h4>
      <a href="/forum?id=Bkl5kxrKDr">
        A Generalized Training Approach for Multiagent Learning
      </a>


      <a href="/pdf?id=Bkl5kxrKDr" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=pmuller%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="pmuller@google.com">Paul Muller</a>, <a href="/profile?email=somidshafiei%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="somidshafiei@google.com">Shayegan
        Omidshafiei</a>, <a href="/profile?email=markrowland%40google.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="markrowland@google.com">Mark Rowland</a>, <a
        href="/profile?email=karltuyls%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="karltuyls@google.com">Karl Tuyls</a>, <a href="/profile?email=perolat%40google.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="perolat@google.com">Julien Perolat</a>, <a
        href="/profile?email=liusiqi%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="liusiqi@google.com">Siqi Liu</a>, <a href="/profile?email=hennes%40google.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="hennes@google.com">Daniel Hennes</a>, <a
        href="/profile?email=marris%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="marris@google.com">Luke Marris</a>, <a href="/profile?email=lanctot%40google.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="lanctot@google.com">Marc Lanctot</a>, <a
        href="/profile?email=edwardhughes%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="edwardhughes@google.com">Edward Hughes</a>, <a href="/profile?email=zhewang%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="zhewang@google.com">Zhe Wang</a>, <a
        href="/profile?email=guylever%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="guylever@google.com">Guy Lever</a>, <a href="/profile?email=heess%40google.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="heess@google.com">Nicolas Heess</a>, <a
        href="/profile?email=thore%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="thore@google.com">Thore Graepel</a>, <a href="/profile?email=munos%40google.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="munos@google.com">Remi Munos</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>14 Replies</span>


    </div>

    <a href="#Bkl5kxrKDr-details-564" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="Bkl5kxrKDr-details-564">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">multiagent learning, game theory, training, games</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper investigates a population-based training regime based on
              game-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is general in the sense that
              it (1) encompasses well-known algorithms such as fictitious play and double oracle as special cases, and
              (2) in principle applies to general-sum, many-player games. Despite this, prior studies of PSRO have been
              focused on two-player zero-sum games, a regime where in Nash equilibria are tractably computable. In
              moving from two-player zero-sum games to more general settings, computation of Nash equilibria quickly
              becomes infeasible. Here, we extend the theoretical underpinnings of PSRO by considering an alternative
              solution concept, α-Rank, which is unique (thus faces no equilibrium selection issues, unlike Nash) and
              applies readily to general-sum, many-player settings. We establish convergence guarantees in several games
              classes, and identify links between Nash equilibria and α-Rank. We demonstrate the competitive performance
              of α-Rank-based PSRO against an exact Nash solver-based PSRO in 2-player Kuhn and Leduc Poker. We then go
              beyond the reach of prior PSRO applications by considering 3- to 5-player poker games, yielding instances
              where α-Rank achieves faster convergence than approximate Nash solvers, thus establishing it as a
              favorable general games solver. We also carry out an initial empirical validation in MuJoCo soccer,
              illustrating the feasibility of the proposed approach in another complex domain.</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=Bkl5kxrKDr&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="r1gfQgSFDr" data-number="2203">
    <h4>
      <a href="/forum?id=r1gfQgSFDr">
        High Fidelity Speech Synthesis with Adversarial Networks
      </a>


      <a href="/pdf?id=r1gfQgSFDr" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=mikbinkowski%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="mikbinkowski@gmail.com">Mikołaj Bińkowski</a>, <a href="/profile?email=jeffdonahue%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="jeffdonahue@google.com">Jeff Donahue</a>,
      <a href="/profile?email=sedielem%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="sedielem@google.com">Sander Dieleman</a>, <a href="/profile?email=aidanclark%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="aidanclark@google.com">Aidan Clark</a>,
      <a href="/profile?email=eriche%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="eriche@google.com">Erich Elsen</a>, <a href="/profile?email=ncasagrande%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="ncasagrande@google.com">Norman
        Casagrande</a>, <a href="/profile?email=luisca%40google.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="luisca@google.com">Luis C. Cobo</a>, <a href="/profile?email=simonyan%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="simonyan@google.com">Karen Simonyan</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>11 Replies</span>


    </div>

    <a href="#r1gfQgSFDr-details-321" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="r1gfQgSFDr-details-321">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech,
              which achieves Mean Opinion Score (MOS) 4.2.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Generative adversarial networks have seen rapid development in recent
              years and have led to remarkable improvements in generative modelling of images. However, their
              application in the audio domain has received limited attention,
              and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio
              signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial
              Network for Text-to-Speech.
              Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an
              ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse
              the audio both in terms of general realism, as well as how well the audio corresponds to the utterance
              that should be pronounced. To measure the performance of GAN-TTS, we employ both subjective human
              evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fréchet DeepSpeech Distance
              and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is
              capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and
              unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator.
              Listen to GAN-TTS reading this abstract at
              https://storage.googleapis.com/deepmind-media/research/abstract.wav</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">texttospeech, speechsynthesis, audiosynthesis, gans,
              generativeadversarialnetworks, implicitgenerativemodels</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/mbinkowski/DeepSpeechDistances</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=r1gfQgSFDr&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="rkgvXlrKwH" data-number="2213">
    <h4>
      <a href="/forum?id=rkgvXlrKwH">
        SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference
      </a>


      <a href="/pdf?id=rkgvXlrKwH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=lespeholt%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="lespeholt@google.com">Lasse Espeholt</a>, <a href="/profile?email=raphaelm%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="raphaelm@google.com">Raphaël
        Marinier</a>, <a href="/profile?email=stanczyk%40google.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="stanczyk@google.com">Piotr Stanczyk</a>, <a href="/profile?email=kewa%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="kewa@google.com">Ke Wang</a>, <a
        href="/profile?email=michalski%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="michalski@google.com">Marcin Michalski‎</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>8 Replies</span>


    </div>

    <a href="#rkgvXlrKwH-details-116" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="rkgvXlrKwH-details-116">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">SEED RL, a scalable and efficient deep reinforcement learning agent with
              accelerated central inference. State of the art results, reduces cost and can process millions of frames
              per second. </span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present a modern scalable reinforcement learning agent called SEED
              (Scalable, Efficient Deep-RL). By effectively utilizing modern accelerators, we show that it is not only
              possible to train on millions of frames per second but also to lower the cost. of experiments compared to
              current methods. We achieve this with a simple architecture that features centralized inference and an
              optimized communication layer. SEED adopts two state-of-the-art distributed algorithms, IMPALA/V-trace
              (policy gradients) and R2D2 (Q-learning), and is evaluated on Atari-57, DeepMind Lab and Google Research
              Football. We improve the state of the art on Football and are able to reach state of the art on Atari-57
              twice as fast in wall-time. For the scenarios we consider, a 40% to 80% cost reduction for running
              experiments is achieved. The implementation along with experiments is open-sourced so results can be
              reproduced and novel ideas tried out.</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span
              class="note-content-value ">https://drive.google.com/file/d/144yp7PQf486dmctE2oS2md_qmNBTFbez/view?usp=sharing</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">machine learning, reinforcement learning, scalability, distributed,
              DeepMind Lab, ALE, Atari-57, Google Research Football</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=rkgvXlrKwH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="rkeiQlBFPB" data-number="2223">
    <h4>
      <a href="/forum?id=rkeiQlBFPB">
        Meta-Learning with Warped Gradient Descent
      </a>


      <a href="/pdf?id=rkeiQlBFPB" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=flennerhag%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="flennerhag@google.com">Sebastian Flennerhag</a>, <a href="/profile?email=andreirusu%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="andreirusu@google.com">Andrei A.
        Rusu</a>, <a href="/profile?email=razp%40google.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="razp@google.com">Razvan Pascanu</a>, <a href="/profile?email=visin%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="visin@google.com">Francesco Visin</a>, <a
        href="/profile?email=hujun.yin%40manchester.ac.uk" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="hujun.yin@manchester.ac.uk">Hujun Yin</a>, <a
        href="/profile?email=raia%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="raia@google.com">Raia Hadsell</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>11 Replies</span>


    </div>

    <a href="#rkeiQlBFPB-details-65" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="rkeiQlBFPB-details-65">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We propose a novel framework for meta-learning a gradient-based update
              rule that scales to beyond few-shot learning and is applicable to any form of learning, including
              continual learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning an efficient update rule from data that promotes rapid learning
              of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous
              works have approached this issue either by attempting to train a neural network that directly produces
              updates or by attempting to learn better initialisations or scaling factors for a gradient-based update
              rule. Both of these approaches pose challenges. On one hand, directly producing an update forgoes a useful
              inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to
              control a gradient-based update rule typically resort to computing gradients through the learning process
              to obtain their meta-gradients, leading to methods that can not scale beyond few-shot task adaptation. In
              this work, we propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to
              mitigate their limitations. WarpGrad meta-learns an efficiently parameterised preconditioning matrix that
              facilitates gradient descent across the task distribution. Preconditioning arises by interleaving
              non-linear layers, referred to as warp-layers, between the layers of a task-learner. Warp-layers are
              meta-learned without backpropagating through the task training process in a manner similar to methods that
              learn to directly produce updates. WarpGrad is computationally efficient, easy to implement, and can scale
              to arbitrarily large meta-learning problems. We provide a geometrical interpretation of the approach and
              evaluate its effectiveness in a variety of settings, including few-shot, standard supervised, continual
              and reinforcement learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">meta-learning, transfer learning</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/flennerhag/warpgrad</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=rkeiQlBFPB&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="Skey4eBYPS" data-number="2232">
    <h4>
      <a href="/forum?id=Skey4eBYPS">
        Convolutional Conditional Neural Processes
      </a>


      <a href="/pdf?id=Skey4eBYPS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=jg801%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="jg801@cam.ac.uk">Jonathan Gordon</a>, <a href="/profile?email=wpb23%40cam.ac.uk" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="wpb23@cam.ac.uk">Wessel P. Bruinsma</a>, <a
        href="/profile?email=ykf21%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="ykf21@cam.ac.uk">Andrew Y. K. Foong</a>, <a href="/profile?email=jrr41%40cam.ac.uk" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="jrr41@cam.ac.uk">James Requeima</a>, <a
        href="/profile?email=yanndubois96%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="yanndubois96@gmail.com">Yann Dubois</a>, <a href="/profile?email=ret26%40cam.ac.uk" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="ret26@cam.ac.uk">Richard E. Turner</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 25 Jun 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>11 Replies</span>


    </div>

    <a href="#Skey4eBYPS-details-166" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="Skey4eBYPS-details-166">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We extend deep sets to functional embeddings and Neural Processes to
              include translation equivariant members</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce the Convolutional Conditional Neural Process (ConvCNP), a new
              member of the Neural Process family that models translation equivariance in the data. Translation
              equivariance is an important inductive bias for many learning problems including time series modelling,
              spatial data, and images. The model embeds data sets into an infinite-dimensional function space, as
              opposed to finite-dimensional vector spaces. To formalize this notion, we extend the theory of neural
              representations of sets to include functional representations, and demonstrate that any
              translation-equivariant embedding can be represented using a convolutional deep-set. We evaluate ConvCNPs
              in several settings, demonstrating that they achieve state-of-the-art performance compared to existing
              NPs. We demonstrate that building in translation equivariance enables zero-shot generalization to
              challenging, out-of-domain tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural Processes, Deep Sets, Translation Equivariance</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/cambridge-mlg/convcnp</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=Skey4eBYPS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="SJeLIgBKPS" data-number="2324">
    <h4>
      <a href="/forum?id=SJeLIgBKPS">
        Gradient Descent Maximizes the Margin of Homogeneous Neural Networks
      </a>


      <a href="/pdf?id=SJeLIgBKPS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=vfleaking%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="vfleaking@gmail.com">Kaifeng Lyu</a>, <a href="/profile?email=lijian83%40mail.tsinghua.edu.cn"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="lijian83@mail.tsinghua.edu.cn">Jian
        Li</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>7 Replies</span>


    </div>

    <a href="#SJeLIgBKPS-details-309" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="SJeLIgBKPS-details-309">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We study the implicit bias of gradient descent and prove under a minimal
              set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural
              margin maximization problem.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we study the implicit regularization of the gradient
              descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural
              networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow
              (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss
              of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a
              certain threshold, then we can define a smoothed version of the normalized margin which increases over
              time. We also formulate a natural constrained optimization problem related to margin maximization, and
              prove that both the normalized margin and its smoothed version converge to the objective value at a KKT
              point of the optimization problem. Our results generalize the previous results for logistic regression
              with one-layer or multi-layer linear networks, and provide more quantitative convergence results with
              weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several
              experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is
              closely related to robustness, we discuss potential benefits of training longer for improving the
              robustness of the model.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">margin, homogeneous, gradient descent</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/vfleaking/max-margin</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=SJeLIgBKPS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="SJxSDxrKDr" data-number="2360">
    <h4>
      <a href="/forum?id=SJxSDxrKDr">
        Adversarial Training and Provable Defenses: Bridging the Gap
      </a>


      <a href="/pdf?id=SJxSDxrKDr" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=bmislav%40student.ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="bmislav@student.ethz.ch">Mislav Balunovic</a>, <a href="/profile?email=martin.vechev%40inf.ethz.ch"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="martin.vechev@inf.ethz.ch">Martin
        Vechev</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>17 Replies</span>


    </div>

    <a href="#SJxSDxrKDr-details-414" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="SJxSDxrKDr-details-414">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">We propose a novel combination of adversarial training and provable
              defenses which produces a model with state-of-the-art accuracy and certified robustness on CIFAR-10.
            </span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present COLT, a new method to train neural networks based on a novel
              combination of adversarial training and provable defenses. The key idea is to model neural network
              training as a procedure which includes both, the verifier and the adversary. In every iteration, the
              verifier aims to certify the network using convex relaxation while the adversary tries to find inputs
              inside that convex relaxation which cause verification to fail. We experimentally show that this training
              method, named convex layerwise adversarial training (COLT), is promising and achieves the best of both
              worlds -- it produces a state-of-the-art neural network with certified robustness of 60.5% and accuracy of
              78.4% on the challenging CIFAR-10 dataset with a 2/255 L-infinity perturbation. This significantly
              improves over the best concurrent results of 54.0% certified robustness and 71.5% accuracy.

            </span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">adversarial examples, adversarial training, provable defense, convex
              relaxations, deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=SJxSDxrKDr&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="SJxstlHFPH" data-number="2447">
    <h4>
      <a href="/forum?id=SJxstlHFPH">
        Differentiable Reasoning over a Virtual Knowledge Base
      </a>


      <a href="/pdf?id=SJxstlHFPH" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=bdhingra%40andrew.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="bdhingra@andrew.cmu.edu">Bhuwan Dhingra</a>, <a href="/profile?email=manzilzaheer%40google.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="manzilzaheer@google.com">Manzil
        Zaheer</a>, <a href="/profile?email=vbalacha%40andrew.cmu.edu" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="vbalacha@andrew.cmu.edu">Vidhisha Balachandran</a>, <a
        href="/profile?email=gneubig%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="gneubig@cs.cmu.edu">Graham Neubig</a>, <a href="/profile?email=rsalakhu%40cs.cmu.edu"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="rsalakhu@cs.cmu.edu">Ruslan
        Salakhutdinov</a>, <a href="/profile?email=wcohen%40google.com" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="wcohen@google.com">William W. Cohen</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>10 Replies</span>


    </div>

    <a href="#SJxstlHFPH-details-498" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="SJxstlHFPH-details-498">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">Differentiable multi-hop access to a textual knowledge base of indexed
              contextual representations</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider the task of answering complex multi-hop questions using a
              corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses
              textual data like a KB, softly following paths of relations between mentions of entities in the corpus. At
              each step the module uses a combination of sparse-matrix TFIDF indices and a maximum inner product search
              (MIPS) on a special index of contextual representations of the mentions. This module is differentiable, so
              the full system can be trained end-to-end using gradient based methods, starting from natural language
              inputs. We also describe a pretraining scheme for the contextual representation encoder by generating hard
              negative examples using existing knowledge bases. We show that DrKIT improves accuracy by 9 points on
              3-hop questions in the MetaQA dataset, cutting the gap between text-based and KB-based state-of-the-art by
              70%. On HotpotQA, DrKIT leads to a 10% improvement over a BERT-based re-ranking approach to retrieving the
              relevant passages required to answer a question. DrKIT is also very efficient, processing up to 10-100x
              more queries per second than existing multi-hop systems.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Question Answering, Multi-Hop QA, Deep Learning, Knowledge Bases,
              Information Extraction, Data Structures for QA</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">http://www.cs.cmu.edu/~bdhingra/pages/drkit.html</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=SJxstlHFPH&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
  <li class="note " data-id="BkluqlSFDS" data-number="2476">
    <h4>
      <a href="/forum?id=BkluqlSFDS">
        Federated Learning with Matched Averaging
      </a>


      <a href="/pdf?id=BkluqlSFDS" class="pdf-link" title="Download PDF" target="_blank"><img
          src="/images/pdf_icon_blue.svg"></a>


    </h4>



    <div class="note-authors">
      <a href="/profile?email=hongyiwang%40cs.wisc.edu" class="profile-link" data-toggle="tooltip" data-placement="top"
        title="hongyiwang@cs.wisc.edu">Hongyi Wang</a>, <a href="/profile?email=mikhail.yurochkin%40ibm.com"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="mikhail.yurochkin@ibm.com">Mikhail
        Yurochkin</a>, <a href="/profile?email=yuekai%40umich.edu" class="profile-link" data-toggle="tooltip"
        data-placement="top" title="yuekai@umich.edu">Yuekai Sun</a>, <a href="/profile?email=dimitris%40papail.io"
        class="profile-link" data-toggle="tooltip" data-placement="top" title="dimitris@papail.io">Dimitris
        Papailiopoulos</a>, <a href="/profile?email=yasaman.khazaeni%40us.ibm.com" class="profile-link"
        data-toggle="tooltip" data-placement="top" title="yasaman.khazaeni@us.ibm.com">Yasaman Khazaeni</a>
    </div>

    <div class="note-meta-info">
      <span class="date">26 Sep 2019 (modified: 11 Mar 2020)</span>
      <span class="item">ICLR 2020 Conference Blind Submission</span>
      <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>

      <span>13 Replies</span>


    </div>

    <a href="#BkluqlSFDS-details-219" class="note-contents-toggle" role="button" data-toggle="collapse"
      aria-expanded="false">Show details</a>
    <div class="collapse" id="BkluqlSFDS-details-219">
      <div class="note-contents-collapse">
        <ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">TL;DR:</strong>
            <span class="note-content-value ">Communication efficient federated learning with layer-wise matching</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Federated learning allows edge devices to collaboratively learn a shared
              model while keeping the training data on device, decoupling the ability to do model training from the need
              to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for
              federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and
              LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden
              elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected
              layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only
              outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures
              trained on real world datasets, but also reduces the overall communication burden.</span>
          </li>
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">federated learning</span>
          </li>
          <li>
            <strong class="note-content-field">Code:</strong>
            <span class="note-content-value ">https://github.com/IBM/FedMA</span>
          </li>
          <li>
            <strong class="note-content-field">Original Pdf:</strong>
            <span class="note-content-value "><a href="/attachment?id=BkluqlSFDS&amp;name=original_pdf"
                class="attachment-download-link" title="Download Original Pdf" target="_blank"><span
                  class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
          </li>
        </ul>
      </div>
    </div>




  </li>
</ul>